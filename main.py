# coding=utf-8

import json
import os
import random
import re
import time
import webbrowser
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union

import pytz
import requests
import yaml


VERSION = "2.0.2"


# === é…ç½®ç®¡ç† ===
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.environ.get("CONFIG_PATH", "config/config.yaml")

    if not Path(config_path).exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨")

    with open(config_path, "r", encoding="utf-8") as f:
        config_data = yaml.safe_load(f)

    print(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")

    # æ„å»ºé…ç½®
    config = {
        "VERSION_CHECK_URL": config_data["app"]["version_check_url"],
        "SHOW_VERSION_UPDATE": config_data["app"]["show_version_update"],
        "REQUEST_INTERVAL": config_data["crawler"]["request_interval"],
        "REPORT_MODE": config_data["report"]["mode"],
        "RANK_THRESHOLD": config_data["report"]["rank_threshold"],
        "USE_PROXY": config_data["crawler"]["use_proxy"],
        "DEFAULT_PROXY": config_data["crawler"]["default_proxy"],
        "ENABLE_CRAWLER": config_data["crawler"]["enable_crawler"],
        "ENABLE_NOTIFICATION": config_data["notification"]["enable_notification"],
        "MESSAGE_BATCH_SIZE": config_data["notification"]["message_batch_size"],
        "BATCH_SEND_INTERVAL": config_data["notification"]["batch_send_interval"],
        "FEISHU_MESSAGE_SEPARATOR": config_data["notification"][
            "feishu_message_separator"
        ],
        "WEIGHT_CONFIG": {
            "RANK_WEIGHT": config_data["weight"]["rank_weight"],
            "FREQUENCY_WEIGHT": config_data["weight"]["frequency_weight"],
            "HOTNESS_WEIGHT": config_data["weight"]["hotness_weight"],
        },
        "PLATFORMS": config_data["platforms"],
    }

    # Webhooké…ç½®ï¼ˆç¯å¢ƒå˜é‡ä¼˜å…ˆï¼‰
    notification = config_data.get("notification", {})
    webhooks = notification.get("webhooks", {})

    config["FEISHU_WEBHOOK_URL"] = os.environ.get(
        "FEISHU_WEBHOOK_URL", ""
    ).strip() or webhooks.get("feishu_url", "")
    config["DINGTALK_WEBHOOK_URL"] = os.environ.get(
        "DINGTALK_WEBHOOK_URL", ""
    ).strip() or webhooks.get("dingtalk_url", "")
    config["WEWORK_WEBHOOK_URL"] = os.environ.get(
        "WEWORK_WEBHOOK_URL", ""
    ).strip() or webhooks.get("wework_url", "")
    config["TELEGRAM_BOT_TOKEN"] = os.environ.get(
        "TELEGRAM_BOT_TOKEN", ""
    ).strip() or webhooks.get("telegram_bot_token", "")
    config["TELEGRAM_CHAT_ID"] = os.environ.get(
        "TELEGRAM_CHAT_ID", ""
    ).strip() or webhooks.get("telegram_chat_id", "")

    # è¾“å‡ºé…ç½®æ¥æºä¿¡æ¯
    webhook_sources = []
    if config["FEISHU_WEBHOOK_URL"]:
        source = "ç¯å¢ƒå˜é‡" if os.environ.get("FEISHU_WEBHOOK_URL") else "é…ç½®æ–‡ä»¶"
        webhook_sources.append(f"é£ä¹¦({source})")
    if config["DINGTALK_WEBHOOK_URL"]:
        source = "ç¯å¢ƒå˜é‡" if os.environ.get("DINGTALK_WEBHOOK_URL") else "é…ç½®æ–‡ä»¶"
        webhook_sources.append(f"é’‰é’‰({source})")
    if config["WEWORK_WEBHOOK_URL"]:
        source = "ç¯å¢ƒå˜é‡" if os.environ.get("WEWORK_WEBHOOK_URL") else "é…ç½®æ–‡ä»¶"
        webhook_sources.append(f"ä¼ä¸šå¾®ä¿¡({source})")
    if config["TELEGRAM_BOT_TOKEN"] and config["TELEGRAM_CHAT_ID"]:
        token_source = (
            "ç¯å¢ƒå˜é‡" if os.environ.get("TELEGRAM_BOT_TOKEN") else "é…ç½®æ–‡ä»¶"
        )
        chat_source = "ç¯å¢ƒå˜é‡" if os.environ.get("TELEGRAM_CHAT_ID") else "é…ç½®æ–‡ä»¶"
        webhook_sources.append(f"Telegram({token_source}/{chat_source})")

    if webhook_sources:
        print(f"Webhook é…ç½®æ¥æº: {', '.join(webhook_sources)}")
    else:
        print("æœªé…ç½®ä»»ä½• Webhook")

    return config


print("æ­£åœ¨åŠ è½½é…ç½®...")
CONFIG = load_config()
print(f"TrendRadar v{VERSION} é…ç½®åŠ è½½å®Œæˆ")
print(f"ç›‘æ§å¹³å°æ•°é‡: {len(CONFIG['PLATFORMS'])}")


# === å·¥å…·å‡½æ•° ===
def get_beijing_time():
    """è·å–åŒ—äº¬æ—¶é—´"""
    return datetime.now(pytz.timezone("Asia/Shanghai"))


def format_date_folder():
    """æ ¼å¼åŒ–æ—¥æœŸæ–‡ä»¶å¤¹"""
    return get_beijing_time().strftime("%Yå¹´%mæœˆ%dæ—¥")


def format_time_filename():
    """æ ¼å¼åŒ–æ—¶é—´æ–‡ä»¶å"""
    return get_beijing_time().strftime("%Hæ—¶%Måˆ†")


def clean_title(title: str) -> str:
    """æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
    if not isinstance(title, str):
        title = str(title)
    cleaned_title = title.replace("\n", " ").replace("\r", " ")
    cleaned_title = re.sub(r"\s+", " ", cleaned_title)
    cleaned_title = cleaned_title.strip()
    return cleaned_title


def ensure_directory_exists(directory: str):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    Path(directory).mkdir(parents=True, exist_ok=True)


def get_output_path(subfolder: str, filename: str) -> str:
    """è·å–è¾“å‡ºè·¯å¾„"""
    date_folder = format_date_folder()
    output_dir = Path("output") / date_folder / subfolder
    ensure_directory_exists(str(output_dir))
    return str(output_dir / filename)


def check_version_update(
    current_version: str, version_url: str, proxy_url: Optional[str] = None
) -> Tuple[bool, Optional[str]]:
    """æ£€æŸ¥ç‰ˆæœ¬æ›´æ–°"""
    try:
        proxies = None
        if proxy_url:
            proxies = {"http": proxy_url, "https": proxy_url}

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/plain, */*",
            "Cache-Control": "no-cache",
        }

        response = requests.get(
            version_url, proxies=proxies, headers=headers, timeout=10
        )
        response.raise_for_status()

        remote_version = response.text.strip()
        print(f"å½“å‰ç‰ˆæœ¬: {current_version}, è¿œç¨‹ç‰ˆæœ¬: {remote_version}")

        # æ¯”è¾ƒç‰ˆæœ¬
        def parse_version(version_str):
            try:
                parts = version_str.strip().split(".")
                if len(parts) != 3:
                    raise ValueError("ç‰ˆæœ¬å·æ ¼å¼ä¸æ­£ç¡®")
                return int(parts[0]), int(parts[1]), int(parts[2])
            except:
                return 0, 0, 0

        current_tuple = parse_version(current_version)
        remote_tuple = parse_version(remote_version)

        need_update = current_tuple < remote_tuple
        return need_update, remote_version if need_update else None

    except Exception as e:
        print(f"ç‰ˆæœ¬æ£€æŸ¥å¤±è´¥: {e}")
        return False, None


def is_first_crawl_today() -> bool:
    """æ£€æµ‹æ˜¯å¦æ˜¯å½“å¤©ç¬¬ä¸€æ¬¡çˆ¬å–"""
    date_folder = format_date_folder()
    txt_dir = Path("output") / date_folder / "txt"

    if not txt_dir.exists():
        return True

    files = sorted([f for f in txt_dir.iterdir() if f.suffix == ".txt"])
    return len(files) <= 1


def html_escape(text: str) -> str:
    """HTMLè½¬ä¹‰"""
    if not isinstance(text, str):
        text = str(text)

    return (
        text.replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
        .replace('"', "&quot;")
        .replace("'", "&#x27;")
    )


# === æ•°æ®è·å– ===
class DataFetcher:
    """æ•°æ®è·å–å™¨"""

    def __init__(self, proxy_url: Optional[str] = None):
        self.proxy_url = proxy_url

    def fetch_data(
        self,
        id_info: Union[str, Tuple[str, str]],
        max_retries: int = 2,
        min_retry_wait: int = 3,
        max_retry_wait: int = 5,
    ) -> Tuple[Optional[str], str, str]:
        """è·å–æŒ‡å®šIDæ•°æ®ï¼Œæ”¯æŒé‡è¯•"""
        if isinstance(id_info, tuple):
            id_value, alias = id_info
        else:
            id_value = id_info
            alias = id_value

        url = f"https://newsnow.busiyi.world/api/s?id={id_value}&latest"

        proxies = None
        if self.proxy_url:
            proxies = {"http": self.proxy_url, "https": self.proxy_url}

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
            "Cache-Control": "no-cache",
        }

        retries = 0
        while retries <= max_retries:
            try:
                response = requests.get(
                    url, proxies=proxies, headers=headers, timeout=10
                )
                response.raise_for_status()

                data_text = response.text
                data_json = json.loads(data_text)

                status = data_json.get("status", "æœªçŸ¥")
                if status not in ["success", "cache"]:
                    raise ValueError(f"å“åº”çŠ¶æ€å¼‚å¸¸: {status}")

                status_info = "æœ€æ–°æ•°æ®" if status == "success" else "ç¼“å­˜æ•°æ®"
                print(f"è·å– {id_value} æˆåŠŸï¼ˆ{status_info}ï¼‰")
                return data_text, id_value, alias

            except Exception as e:
                retries += 1
                if retries <= max_retries:
                    base_wait = random.uniform(min_retry_wait, max_retry_wait)
                    additional_wait = (retries - 1) * random.uniform(1, 2)
                    wait_time = base_wait + additional_wait
                    print(f"è¯·æ±‚ {id_value} å¤±è´¥: {e}. {wait_time:.2f}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"è¯·æ±‚ {id_value} å¤±è´¥: {e}")
                    return None, id_value, alias
        return None, id_value, alias

    def crawl_websites(
        self,
        ids_list: List[Union[str, Tuple[str, str]]],
        request_interval: int = CONFIG["REQUEST_INTERVAL"],
    ) -> Tuple[Dict, Dict, List]:
        """çˆ¬å–å¤šä¸ªç½‘ç«™æ•°æ®"""
        results = {}
        id_to_name = {}
        failed_ids = []

        for i, id_info in enumerate(ids_list):
            if isinstance(id_info, tuple):
                id_value, name = id_info
            else:
                id_value = id_info
                name = id_value

            id_to_name[id_value] = name
            response, _, _ = self.fetch_data(id_info)

            if response:
                try:
                    data = json.loads(response)
                    results[id_value] = {}
                    for index, item in enumerate(data.get("items", []), 1):
                        title = item["title"]
                        url = item.get("url", "")
                        mobile_url = item.get("mobileUrl", "")

                        if title in results[id_value]:
                            results[id_value][title]["ranks"].append(index)
                        else:
                            results[id_value][title] = {
                                "ranks": [index],
                                "url": url,
                                "mobileUrl": mobile_url,
                            }
                except json.JSONDecodeError:
                    print(f"è§£æ {id_value} å“åº”å¤±è´¥")
                    failed_ids.append(id_value)
                except Exception as e:
                    print(f"å¤„ç† {id_value} æ•°æ®å‡ºé”™: {e}")
                    failed_ids.append(id_value)
            else:
                failed_ids.append(id_value)

            if i < len(ids_list) - 1:
                actual_interval = request_interval + random.randint(-10, 20)
                actual_interval = max(50, actual_interval)
                time.sleep(actual_interval / 1000)

        print(f"æˆåŠŸ: {list(results.keys())}, å¤±è´¥: {failed_ids}")
        return results, id_to_name, failed_ids


# === æ•°æ®å¤„ç† ===
def save_titles_to_file(results: Dict, id_to_name: Dict, failed_ids: List) -> str:
    """ä¿å­˜æ ‡é¢˜åˆ°æ–‡ä»¶"""
    file_path = get_output_path("txt", f"{format_time_filename()}.txt")

    with open(file_path, "w", encoding="utf-8") as f:
        for id_value, title_data in results.items():
            # id | name æˆ– id
            name = id_to_name.get(id_value)
            if name and name != id_value:
                f.write(f"{id_value} | {name}\n")
            else:
                f.write(f"{id_value}\n")

            # æŒ‰æ’åæ’åºæ ‡é¢˜
            sorted_titles = []
            for title, info in title_data.items():
                cleaned_title = clean_title(title)
                if isinstance(info, dict):
                    ranks = info.get("ranks", [])
                    url = info.get("url", "")
                    mobile_url = info.get("mobileUrl", "")
                else:
                    ranks = info if isinstance(info, list) else []
                    url = ""
                    mobile_url = ""

                rank = ranks[0] if ranks else 1
                sorted_titles.append((rank, cleaned_title, url, mobile_url))

            sorted_titles.sort(key=lambda x: x[0])

            for rank, cleaned_title, url, mobile_url in sorted_titles:
                line = f"{rank}. {cleaned_title}"

                if url:
                    line += f" [URL:{url}]"
                if mobile_url:
                    line += f" [MOBILE:{mobile_url}]"
                f.write(line + "\n")

            f.write("\n")

        if failed_ids:
            f.write("==== ä»¥ä¸‹IDè¯·æ±‚å¤±è´¥ ====\n")
            for id_value in failed_ids:
                f.write(f"{id_value}\n")

    return file_path


def load_frequency_words(
    frequency_file: Optional[str] = None,
) -> Tuple[List[Dict], List[str]]:
    """åŠ è½½é¢‘ç‡è¯é…ç½®"""
    if frequency_file is None:
        frequency_file = os.environ.get(
            "FREQUENCY_WORDS_PATH", "config/frequency_words.txt"
        )

    frequency_path = Path(frequency_file)
    if not frequency_path.exists():
        raise FileNotFoundError(f"é¢‘ç‡è¯æ–‡ä»¶ {frequency_file} ä¸å­˜åœ¨")

    with open(frequency_path, "r", encoding="utf-8") as f:
        content = f.read()

    word_groups = [group.strip() for group in content.split("\n\n") if group.strip()]

    processed_groups = []
    filter_words = []

    for group in word_groups:
        words = [word.strip() for word in group.split("\n") if word.strip()]

        group_required_words = []
        group_normal_words = []
        group_filter_words = []

        for word in words:
            if word.startswith("!"):
                filter_words.append(word[1:])
                group_filter_words.append(word[1:])
            elif word.startswith("+"):
                group_required_words.append(word[1:])
            else:
                group_normal_words.append(word)

        if group_required_words or group_normal_words:
            if group_normal_words:
                group_key = " ".join(group_normal_words)
            else:
                group_key = " ".join(group_required_words)

            processed_groups.append(
                {
                    "required": group_required_words,
                    "normal": group_normal_words,
                    "group_key": group_key,
                }
            )

    return processed_groups, filter_words


def parse_file_titles(file_path: Path) -> Tuple[Dict, Dict]:
    """è§£æå•ä¸ªtxtæ–‡ä»¶çš„æ ‡é¢˜æ•°æ®ï¼Œè¿”å›(titles_by_id, id_to_name)"""
    titles_by_id = {}
    id_to_name = {}

    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
        sections = content.split("\n\n")

        for section in sections:
            if not section.strip() or "==== ä»¥ä¸‹IDè¯·æ±‚å¤±è´¥ ====" in section:
                continue

            lines = section.strip().split("\n")
            if len(lines) < 2:
                continue

            # id | name æˆ– id
            header_line = lines[0].strip()
            if " | " in header_line:
                parts = header_line.split(" | ", 1)
                source_id = parts[0].strip()
                name = parts[1].strip()
                id_to_name[source_id] = name
            else:
                source_id = header_line
                id_to_name[source_id] = source_id

            titles_by_id[source_id] = {}

            for line in lines[1:]:
                if line.strip():
                    try:
                        title_part = line.strip()
                        rank = None

                        # æå–æ’å
                        if ". " in title_part and title_part.split(". ")[0].isdigit():
                            rank_str, title_part = title_part.split(". ", 1)
                            rank = int(rank_str)

                        # æå– MOBILE URL
                        mobile_url = ""
                        if " [MOBILE:" in title_part:
                            title_part, mobile_part = title_part.rsplit(" [MOBILE:", 1)
                            if mobile_part.endswith("]"):
                                mobile_url = mobile_part[:-1]

                        # æå– URL
                        url = ""
                        if " [URL:" in title_part:
                            title_part, url_part = title_part.rsplit(" [URL:", 1)
                            if url_part.endswith("]"):
                                url = url_part[:-1]

                        title = clean_title(title_part.strip())
                        ranks = [rank] if rank is not None else [1]

                        titles_by_id[source_id][title] = {
                            "ranks": ranks,
                            "url": url,
                            "mobileUrl": mobile_url,
                        }

                    except Exception as e:
                        print(f"è§£ææ ‡é¢˜è¡Œå‡ºé”™: {line}, é”™è¯¯: {e}")

    return titles_by_id, id_to_name


def read_all_today_titles(
    current_platform_ids: Optional[List[str]] = None,
) -> Tuple[Dict, Dict, Dict]:
    """è¯»å–å½“å¤©æ‰€æœ‰æ ‡é¢˜æ–‡ä»¶ï¼Œæ”¯æŒæŒ‰å½“å‰ç›‘æ§å¹³å°è¿‡æ»¤"""
    date_folder = format_date_folder()
    txt_dir = Path("output") / date_folder / "txt"

    if not txt_dir.exists():
        return {}, {}, {}

    all_results = {}
    final_id_to_name = {}
    title_info = {}

    files = sorted([f for f in txt_dir.iterdir() if f.suffix == ".txt"])

    for file_path in files:
        time_info = file_path.stem

        titles_by_id, file_id_to_name = parse_file_titles(file_path)

        if current_platform_ids is not None:
            filtered_titles_by_id = {}
            filtered_id_to_name = {}

            for source_id, title_data in titles_by_id.items():
                if source_id in current_platform_ids:
                    filtered_titles_by_id[source_id] = title_data
                    if source_id in file_id_to_name:
                        filtered_id_to_name[source_id] = file_id_to_name[source_id]

            titles_by_id = filtered_titles_by_id
            file_id_to_name = filtered_id_to_name

        final_id_to_name.update(file_id_to_name)

        for source_id, title_data in titles_by_id.items():
            process_source_data(
                source_id, title_data, time_info, all_results, title_info
            )

    return all_results, final_id_to_name, title_info


def process_source_data(
    source_id: str,
    title_data: Dict,
    time_info: str,
    all_results: Dict,
    title_info: Dict,
) -> None:
    """å¤„ç†æ¥æºæ•°æ®ï¼Œåˆå¹¶é‡å¤æ ‡é¢˜"""
    if source_id not in all_results:
        all_results[source_id] = title_data

        if source_id not in title_info:
            title_info[source_id] = {}

        for title, data in title_data.items():
            ranks = data.get("ranks", [])
            url = data.get("url", "")
            mobile_url = data.get("mobileUrl", "")

            title_info[source_id][title] = {
                "first_time": time_info,
                "last_time": time_info,
                "count": 1,
                "ranks": ranks,
                "url": url,
                "mobileUrl": mobile_url,
            }
    else:
        for title, data in title_data.items():
            ranks = data.get("ranks", [])
            url = data.get("url", "")
            mobile_url = data.get("mobileUrl", "")

            if title not in all_results[source_id]:
                all_results[source_id][title] = {
                    "ranks": ranks,
                    "url": url,
                    "mobileUrl": mobile_url,
                }
                title_info[source_id][title] = {
                    "first_time": time_info,
                    "last_time": time_info,
                    "count": 1,
                    "ranks": ranks,
                    "url": url,
                    "mobileUrl": mobile_url,
                }
            else:
                existing_data = all_results[source_id][title]
                existing_ranks = existing_data.get("ranks", [])
                existing_url = existing_data.get("url", "")
                existing_mobile_url = existing_data.get("mobileUrl", "")

                merged_ranks = existing_ranks.copy()
                for rank in ranks:
                    if rank not in merged_ranks:
                        merged_ranks.append(rank)

                all_results[source_id][title] = {
                    "ranks": merged_ranks,
                    "url": existing_url or url,
                    "mobileUrl": existing_mobile_url or mobile_url,
                }

                title_info[source_id][title]["last_time"] = time_info
                title_info[source_id][title]["ranks"] = merged_ranks
                title_info[source_id][title]["count"] += 1
                if not title_info[source_id][title].get("url"):
                    title_info[source_id][title]["url"] = url
                if not title_info[source_id][title].get("mobileUrl"):
                    title_info[source_id][title]["mobileUrl"] = mobile_url


def detect_latest_new_titles(current_platform_ids: Optional[List[str]] = None) -> Dict:
    """æ£€æµ‹å½“æ—¥æœ€æ–°æ‰¹æ¬¡çš„æ–°å¢æ ‡é¢˜ï¼Œæ”¯æŒæŒ‰å½“å‰ç›‘æ§å¹³å°è¿‡æ»¤"""
    date_folder = format_date_folder()
    txt_dir = Path("output") / date_folder / "txt"

    if not txt_dir.exists():
        return {}

    files = sorted([f for f in txt_dir.iterdir() if f.suffix == ".txt"])
    if len(files) < 2:
        return {}

    # è§£ææœ€æ–°æ–‡ä»¶
    latest_file = files[-1]
    latest_titles, _ = parse_file_titles(latest_file)

    # å¦‚æœæŒ‡å®šäº†å½“å‰å¹³å°åˆ—è¡¨ï¼Œè¿‡æ»¤æœ€æ–°æ–‡ä»¶æ•°æ®
    if current_platform_ids is not None:
        filtered_latest_titles = {}
        for source_id, title_data in latest_titles.items():
            if source_id in current_platform_ids:
                filtered_latest_titles[source_id] = title_data
        latest_titles = filtered_latest_titles

    # æ±‡æ€»å†å²æ ‡é¢˜ï¼ˆæŒ‰å¹³å°è¿‡æ»¤ï¼‰
    historical_titles = {}
    for file_path in files[:-1]:
        historical_data, _ = parse_file_titles(file_path)

        # è¿‡æ»¤å†å²æ•°æ®
        if current_platform_ids is not None:
            filtered_historical_data = {}
            for source_id, title_data in historical_data.items():
                if source_id in current_platform_ids:
                    filtered_historical_data[source_id] = title_data
            historical_data = filtered_historical_data

        for source_id, titles_data in historical_data.items():
            if source_id not in historical_titles:
                historical_titles[source_id] = set()
            for title in titles_data.keys():
                historical_titles[source_id].add(title)

    # æ‰¾å‡ºæ–°å¢æ ‡é¢˜
    new_titles = {}
    for source_id, latest_source_titles in latest_titles.items():
        historical_set = historical_titles.get(source_id, set())
        source_new_titles = {}

        for title, title_data in latest_source_titles.items():
            if title not in historical_set:
                source_new_titles[title] = title_data

        if source_new_titles:
            new_titles[source_id] = source_new_titles

    return new_titles


# === ç»Ÿè®¡å’Œåˆ†æ ===
def calculate_news_weight(
    title_data: Dict, rank_threshold: int = CONFIG["RANK_THRESHOLD"]
) -> float:
    """è®¡ç®—æ–°é—»æƒé‡ï¼Œç”¨äºæ’åº"""
    ranks = title_data.get("ranks", [])
    if not ranks:
        return 0.0

    count = title_data.get("count", len(ranks))
    weight_config = CONFIG["WEIGHT_CONFIG"]

    # æ’åæƒé‡ï¼šÎ£(11 - min(rank, 10)) / å‡ºç°æ¬¡æ•°
    rank_scores = []
    for rank in ranks:
        score = 11 - min(rank, 10)
        rank_scores.append(score)

    rank_weight = sum(rank_scores) / len(ranks) if ranks else 0

    # é¢‘æ¬¡æƒé‡ï¼šmin(å‡ºç°æ¬¡æ•°, 10) Ã— 10
    frequency_weight = min(count, 10) * 10

    # çƒ­åº¦åŠ æˆï¼šé«˜æ’åæ¬¡æ•° / æ€»å‡ºç°æ¬¡æ•° Ã— 100
    high_rank_count = sum(1 for rank in ranks if rank <= rank_threshold)
    hotness_ratio = high_rank_count / len(ranks) if ranks else 0
    hotness_weight = hotness_ratio * 100

    total_weight = (
        rank_weight * weight_config["RANK_WEIGHT"]
        + frequency_weight * weight_config["FREQUENCY_WEIGHT"]
        + hotness_weight * weight_config["HOTNESS_WEIGHT"]
    )

    return total_weight


def matches_word_groups(
    title: str, word_groups: List[Dict], filter_words: List[str]
) -> bool:
    """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ¹é…è¯ç»„è§„åˆ™"""
    # å¦‚æœæ²¡æœ‰é…ç½®è¯ç»„ï¼Œåˆ™åŒ¹é…æ‰€æœ‰æ ‡é¢˜ï¼ˆæ”¯æŒæ˜¾ç¤ºå…¨éƒ¨æ–°é—»ï¼‰
    if not word_groups:
        return True

    title_lower = title.lower()

    # è¿‡æ»¤è¯æ£€æŸ¥
    if any(filter_word.lower() in title_lower for filter_word in filter_words):
        return False

    # è¯ç»„åŒ¹é…æ£€æŸ¥
    for group in word_groups:
        required_words = group["required"]
        normal_words = group["normal"]

        # å¿…é¡»è¯æ£€æŸ¥
        if required_words:
            all_required_present = all(
                req_word.lower() in title_lower for req_word in required_words
            )
            if not all_required_present:
                continue

        # æ™®é€šè¯æ£€æŸ¥
        if normal_words:
            any_normal_present = any(
                normal_word.lower() in title_lower for normal_word in normal_words
            )
            if not any_normal_present:
                continue

        return True

    return False


def format_time_display(first_time: str, last_time: str) -> str:
    """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
    if not first_time:
        return ""
    if first_time == last_time or not last_time:
        return first_time
    else:
        return f"[{first_time} ~ {last_time}]"


def format_rank_display(ranks: List[int], rank_threshold: int, format_type: str) -> str:
    """ç»Ÿä¸€çš„æ’åæ ¼å¼åŒ–æ–¹æ³•"""
    if not ranks:
        return ""

    unique_ranks = sorted(set(ranks))
    min_rank = unique_ranks[0]
    max_rank = unique_ranks[-1]

    if format_type == "html":
        highlight_start = "<font color='red'><strong>"
        highlight_end = "</strong></font>"
    elif format_type == "feishu":
        highlight_start = "<font color='red'>**"
        highlight_end = "**</font>"
    elif format_type == "dingtalk":
        highlight_start = "**"
        highlight_end = "**"
    elif format_type == "wework":
        highlight_start = "**"
        highlight_end = "**"
    elif format_type == "telegram":
        highlight_start = "<b>"
        highlight_end = "</b>"
    else:
        highlight_start = "**"
        highlight_end = "**"

    if min_rank <= rank_threshold:
        if min_rank == max_rank:
            return f"{highlight_start}[{min_rank}]{highlight_end}"
        else:
            return f"{highlight_start}[{min_rank} - {max_rank}]{highlight_end}"
    else:
        if min_rank == max_rank:
            return f"[{min_rank}]"
        else:
            return f"[{min_rank} - {max_rank}]"


def count_word_frequency(
    results: Dict,
    word_groups: List[Dict],
    filter_words: List[str],
    id_to_name: Dict,
    title_info: Optional[Dict] = None,
    rank_threshold: int = CONFIG["RANK_THRESHOLD"],
    new_titles: Optional[Dict] = None,
    mode: str = "daily",
) -> Tuple[List[Dict], int]:
    """ç»Ÿè®¡è¯é¢‘ï¼Œæ”¯æŒå¿…é¡»è¯ã€é¢‘ç‡è¯ã€è¿‡æ»¤è¯ï¼Œå¹¶æ ‡è®°æ–°å¢æ ‡é¢˜"""

    # å¦‚æœæ²¡æœ‰é…ç½®è¯ç»„ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰æ–°é—»çš„è™šæ‹Ÿè¯ç»„
    if not word_groups:
        print("é¢‘ç‡è¯é…ç½®ä¸ºç©ºï¼Œå°†æ˜¾ç¤ºæ‰€æœ‰æ–°é—»")
        word_groups = [{"required": [], "normal": [], "group_key": "å…¨éƒ¨æ–°é—»"}]
        filter_words = []  # æ¸…ç©ºè¿‡æ»¤è¯ï¼Œæ˜¾ç¤ºæ‰€æœ‰æ–°é—»

    is_first_today = is_first_crawl_today()

    # ç¡®å®šå¤„ç†çš„æ•°æ®æºå’Œæ–°å¢æ ‡è®°é€»è¾‘
    if mode == "incremental":
        if is_first_today:
            # å¢é‡æ¨¡å¼ + å½“å¤©ç¬¬ä¸€æ¬¡ï¼šå¤„ç†æ‰€æœ‰æ–°é—»ï¼Œéƒ½æ ‡è®°ä¸ºæ–°å¢
            results_to_process = results
            all_news_are_new = True
        else:
            # å¢é‡æ¨¡å¼ + å½“å¤©éç¬¬ä¸€æ¬¡ï¼šåªå¤„ç†æ–°å¢çš„æ–°é—»
            results_to_process = new_titles if new_titles else {}
            all_news_are_new = True
    elif mode == "current":
        # current æ¨¡å¼ï¼šåªå¤„ç†å½“å‰æ—¶é—´æ‰¹æ¬¡çš„æ–°é—»ï¼Œä½†ç»Ÿè®¡ä¿¡æ¯æ¥è‡ªå…¨éƒ¨å†å²
        if title_info:
            latest_time = None
            for source_titles in title_info.values():
                for title_data in source_titles.values():
                    last_time = title_data.get("last_time", "")
                    if last_time:
                        if latest_time is None or last_time > latest_time:
                            latest_time = last_time

            # åªå¤„ç† last_time ç­‰äºæœ€æ–°æ—¶é—´çš„æ–°é—»
            if latest_time:
                results_to_process = {}
                for source_id, source_titles in results.items():
                    if source_id in title_info:
                        filtered_titles = {}
                        for title, title_data in source_titles.items():
                            if title in title_info[source_id]:
                                info = title_info[source_id][title]
                                if info.get("last_time") == latest_time:
                                    filtered_titles[title] = title_data
                        if filtered_titles:
                            results_to_process[source_id] = filtered_titles

                print(
                    f"å½“å‰æ¦œå•æ¨¡å¼ï¼šæœ€æ–°æ—¶é—´ {latest_time}ï¼Œç­›é€‰å‡º {sum(len(titles) for titles in results_to_process.values())} æ¡å½“å‰æ¦œå•æ–°é—»"
                )
            else:
                results_to_process = results
        else:
            results_to_process = results
        all_news_are_new = False
    else:
        # å½“æ—¥æ±‡æ€»æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰æ–°é—»
        results_to_process = results
        all_news_are_new = False
        total_input_news = sum(len(titles) for titles in results.values())
        filter_status = (
            "å…¨éƒ¨æ˜¾ç¤º"
            if len(word_groups) == 1 and word_groups[0]["group_key"] == "å…¨éƒ¨æ–°é—»"
            else "é¢‘ç‡è¯è¿‡æ»¤"
        )
        print(f"å½“æ—¥æ±‡æ€»æ¨¡å¼ï¼šå¤„ç† {total_input_news} æ¡æ–°é—»ï¼Œæ¨¡å¼ï¼š{filter_status}")

    word_stats = {}
    total_titles = 0
    processed_titles = {}
    matched_new_count = 0

    if title_info is None:
        title_info = {}
    if new_titles is None:
        new_titles = {}

    for group in word_groups:
        group_key = group["group_key"]
        word_stats[group_key] = {"count": 0, "titles": {}}

    for source_id, titles_data in results_to_process.items():
        total_titles += len(titles_data)

        if source_id not in processed_titles:
            processed_titles[source_id] = {}

        for title, title_data in titles_data.items():
            if title in processed_titles.get(source_id, {}):
                continue

            # ä½¿ç”¨ç»Ÿä¸€çš„åŒ¹é…é€»è¾‘
            matches_frequency_words = matches_word_groups(
                title, word_groups, filter_words
            )

            if not matches_frequency_words:
                continue

            # å¦‚æœæ˜¯å¢é‡æ¨¡å¼æˆ– current æ¨¡å¼ç¬¬ä¸€æ¬¡ï¼Œç»Ÿè®¡åŒ¹é…çš„æ–°å¢æ–°é—»æ•°é‡
            if (mode == "incremental" and all_news_are_new) or (
                mode == "current" and is_first_today
            ):
                matched_new_count += 1

            source_ranks = title_data.get("ranks", [])
            source_url = title_data.get("url", "")
            source_mobile_url = title_data.get("mobileUrl", "")

            # æ‰¾åˆ°åŒ¹é…çš„è¯ç»„
            title_lower = title.lower()
            for group in word_groups:
                required_words = group["required"]
                normal_words = group["normal"]

                # å¦‚æœæ˜¯"å…¨éƒ¨æ–°é—»"æ¨¡å¼ï¼Œæ‰€æœ‰æ ‡é¢˜éƒ½åŒ¹é…ç¬¬ä¸€ä¸ªï¼ˆå”¯ä¸€çš„ï¼‰è¯ç»„
                if len(word_groups) == 1 and word_groups[0]["group_key"] == "å…¨éƒ¨æ–°é—»":
                    group_key = group["group_key"]
                    word_stats[group_key]["count"] += 1
                    if source_id not in word_stats[group_key]["titles"]:
                        word_stats[group_key]["titles"][source_id] = []
                else:
                    # åŸæœ‰çš„åŒ¹é…é€»è¾‘
                    if required_words:
                        all_required_present = all(
                            req_word.lower() in title_lower
                            for req_word in required_words
                        )
                        if not all_required_present:
                            continue

                    if normal_words:
                        any_normal_present = any(
                            normal_word.lower() in title_lower
                            for normal_word in normal_words
                        )
                        if not any_normal_present:
                            continue

                    group_key = group["group_key"]
                    word_stats[group_key]["count"] += 1
                    if source_id not in word_stats[group_key]["titles"]:
                        word_stats[group_key]["titles"][source_id] = []

                first_time = ""
                last_time = ""
                count_info = 1
                ranks = source_ranks if source_ranks else []
                url = source_url
                mobile_url = source_mobile_url

                # å¯¹äº current æ¨¡å¼ï¼Œä»å†å²ç»Ÿè®¡ä¿¡æ¯ä¸­è·å–å®Œæ•´æ•°æ®
                if (
                    mode == "current"
                    and title_info
                    and source_id in title_info
                    and title in title_info[source_id]
                ):
                    info = title_info[source_id][title]
                    first_time = info.get("first_time", "")
                    last_time = info.get("last_time", "")
                    count_info = info.get("count", 1)
                    if "ranks" in info and info["ranks"]:
                        ranks = info["ranks"]
                    url = info.get("url", source_url)
                    mobile_url = info.get("mobileUrl", source_mobile_url)
                elif (
                    title_info
                    and source_id in title_info
                    and title in title_info[source_id]
                ):
                    info = title_info[source_id][title]
                    first_time = info.get("first_time", "")
                    last_time = info.get("last_time", "")
                    count_info = info.get("count", 1)
                    if "ranks" in info and info["ranks"]:
                        ranks = info["ranks"]
                    url = info.get("url", source_url)
                    mobile_url = info.get("mobileUrl", source_mobile_url)

                if not ranks:
                    ranks = [99]

                time_display = format_time_display(first_time, last_time)

                source_name = id_to_name.get(source_id, source_id)

                # åˆ¤æ–­æ˜¯å¦ä¸ºæ–°å¢
                is_new = False
                if all_news_are_new:
                    # å¢é‡æ¨¡å¼ä¸‹æ‰€æœ‰å¤„ç†çš„æ–°é—»éƒ½æ˜¯æ–°å¢ï¼Œæˆ–è€…å½“å¤©ç¬¬ä¸€æ¬¡çš„æ‰€æœ‰æ–°é—»éƒ½æ˜¯æ–°å¢
                    is_new = True
                elif new_titles and source_id in new_titles:
                    # æ£€æŸ¥æ˜¯å¦åœ¨æ–°å¢åˆ—è¡¨ä¸­
                    new_titles_for_source = new_titles[source_id]
                    is_new = title in new_titles_for_source

                word_stats[group_key]["titles"][source_id].append(
                    {
                        "title": title,
                        "source_name": source_name,
                        "first_time": first_time,
                        "last_time": last_time,
                        "time_display": time_display,
                        "count": count_info,
                        "ranks": ranks,
                        "rank_threshold": rank_threshold,
                        "url": url,
                        "mobileUrl": mobile_url,
                        "is_new": is_new,
                    }
                )

                if source_id not in processed_titles:
                    processed_titles[source_id] = {}
                processed_titles[source_id][title] = True

                break

    # æœ€åç»Ÿä¸€æ‰“å°æ±‡æ€»ä¿¡æ¯
    if mode == "incremental":
        if is_first_today:
            total_input_news = sum(len(titles) for titles in results.values())
            filter_status = (
                "å…¨éƒ¨æ˜¾ç¤º"
                if len(word_groups) == 1 and word_groups[0]["group_key"] == "å…¨éƒ¨æ–°é—»"
                else "é¢‘ç‡è¯åŒ¹é…"
            )
            print(
                f"å¢é‡æ¨¡å¼ï¼šå½“å¤©ç¬¬ä¸€æ¬¡çˆ¬å–ï¼Œ{total_input_news} æ¡æ–°é—»ä¸­æœ‰ {matched_new_count} æ¡{filter_status}"
            )
        else:
            if new_titles:
                total_new_count = sum(len(titles) for titles in new_titles.values())
                filter_status = (
                    "å…¨éƒ¨æ˜¾ç¤º"
                    if len(word_groups) == 1
                    and word_groups[0]["group_key"] == "å…¨éƒ¨æ–°é—»"
                    else "åŒ¹é…é¢‘ç‡è¯"
                )
                print(
                    f"å¢é‡æ¨¡å¼ï¼š{total_new_count} æ¡æ–°å¢æ–°é—»ä¸­ï¼Œæœ‰ {matched_new_count} æ¡{filter_status}"
                )
                if matched_new_count == 0 and len(word_groups) > 1:
                    print("å¢é‡æ¨¡å¼ï¼šæ²¡æœ‰æ–°å¢æ–°é—»åŒ¹é…é¢‘ç‡è¯ï¼Œå°†ä¸ä¼šå‘é€é€šçŸ¥")
            else:
                print("å¢é‡æ¨¡å¼ï¼šæœªæ£€æµ‹åˆ°æ–°å¢æ–°é—»")
    elif mode == "current":
        total_input_news = sum(len(titles) for titles in results_to_process.values())
        if is_first_today:
            filter_status = (
                "å…¨éƒ¨æ˜¾ç¤º"
                if len(word_groups) == 1 and word_groups[0]["group_key"] == "å…¨éƒ¨æ–°é—»"
                else "é¢‘ç‡è¯åŒ¹é…"
            )
            print(
                f"å½“å‰æ¦œå•æ¨¡å¼ï¼šå½“å¤©ç¬¬ä¸€æ¬¡çˆ¬å–ï¼Œ{total_input_news} æ¡å½“å‰æ¦œå•æ–°é—»ä¸­æœ‰ {matched_new_count} æ¡{filter_status}"
            )
        else:
            matched_count = sum(stat["count"] for stat in word_stats.values())
            filter_status = (
                "å…¨éƒ¨æ˜¾ç¤º"
                if len(word_groups) == 1 and word_groups[0]["group_key"] == "å…¨éƒ¨æ–°é—»"
                else "é¢‘ç‡è¯åŒ¹é…"
            )
            print(
                f"å½“å‰æ¦œå•æ¨¡å¼ï¼š{total_input_news} æ¡å½“å‰æ¦œå•æ–°é—»ä¸­æœ‰ {matched_count} æ¡{filter_status}"
            )

    stats = []
    for group_key, data in word_stats.items():
        all_titles = []
        for source_id, title_list in data["titles"].items():
            all_titles.extend(title_list)

        # æŒ‰æƒé‡æ’åº
        sorted_titles = sorted(
            all_titles,
            key=lambda x: (
                -calculate_news_weight(x, rank_threshold),
                min(x["ranks"]) if x["ranks"] else 999,
                -x["count"],
            ),
        )

        stats.append(
            {
                "word": group_key,
                "count": data["count"],
                "titles": sorted_titles,
                "percentage": (
                    round(data["count"] / total_titles * 100, 2)
                    if total_titles > 0
                    else 0
                ),
            }
        )

    stats.sort(key=lambda x: x["count"], reverse=True)
    return stats, total_titles


# === æŠ¥å‘Šç”Ÿæˆ ===
def prepare_report_data(
    stats: List[Dict],
    failed_ids: Optional[List] = None,
    new_titles: Optional[Dict] = None,
    id_to_name: Optional[Dict] = None,
    mode: str = "daily",
) -> Dict:
    """å‡†å¤‡æŠ¥å‘Šæ•°æ®"""
    processed_new_titles = []

    # åœ¨å¢é‡æ¨¡å¼ä¸‹éšè—æ–°å¢æ–°é—»åŒºåŸŸ
    hide_new_section = mode == "incremental"

    # åªæœ‰åœ¨ééšè—æ¨¡å¼ä¸‹æ‰å¤„ç†æ–°å¢æ–°é—»éƒ¨åˆ†
    if not hide_new_section:
        filtered_new_titles = {}
        if new_titles and id_to_name:
            word_groups, filter_words = load_frequency_words()
            for source_id, titles_data in new_titles.items():
                filtered_titles = {}
                for title, title_data in titles_data.items():
                    if matches_word_groups(title, word_groups, filter_words):
                        filtered_titles[title] = title_data
                if filtered_titles:
                    filtered_new_titles[source_id] = filtered_titles

        if filtered_new_titles and id_to_name:
            for source_id, titles_data in filtered_new_titles.items():
                source_name = id_to_name.get(source_id, source_id)
                source_titles = []

                for title, title_data in titles_data.items():
                    url = title_data.get("url", "")
                    mobile_url = title_data.get("mobileUrl", "")
                    ranks = title_data.get("ranks", [])

                    processed_title = {
                        "title": title,
                        "source_name": source_name,
                        "time_display": "",
                        "count": 1,
                        "ranks": ranks,
                        "rank_threshold": CONFIG["RANK_THRESHOLD"],
                        "url": url,
                        "mobile_url": mobile_url,
                        "is_new": True,
                    }
                    source_titles.append(processed_title)

                if source_titles:
                    processed_new_titles.append(
                        {
                            "source_id": source_id,
                            "source_name": source_name,
                            "titles": source_titles,
                        }
                    )

    processed_stats = []
    for stat in stats:
        if stat["count"] <= 0:
            continue

        processed_titles = []
        for title_data in stat["titles"]:
            processed_title = {
                "title": title_data["title"],
                "source_name": title_data["source_name"],
                "time_display": title_data["time_display"],
                "count": title_data["count"],
                "ranks": title_data["ranks"],
                "rank_threshold": title_data["rank_threshold"],
                "url": title_data.get("url", ""),
                "mobile_url": title_data.get("mobileUrl", ""),
                "is_new": title_data.get("is_new", False),
            }
            processed_titles.append(processed_title)

        processed_stats.append(
            {
                "word": stat["word"],
                "count": stat["count"],
                "percentage": stat.get("percentage", 0),
                "titles": processed_titles,
            }
        )

    return {
        "stats": processed_stats,
        "new_titles": processed_new_titles,
        "failed_ids": failed_ids or [],
        "total_new_count": sum(
            len(source["titles"]) for source in processed_new_titles
        ),
    }


def format_title_for_platform(
    platform: str, title_data: Dict, show_source: bool = True
) -> str:
    """ç»Ÿä¸€çš„æ ‡é¢˜æ ¼å¼åŒ–æ–¹æ³•"""
    rank_display = format_rank_display(
        title_data["ranks"], title_data["rank_threshold"], platform
    )

    link_url = title_data["mobile_url"] or title_data["url"]

    cleaned_title = clean_title(title_data["title"])

    if platform == "feishu":
        if link_url:
            formatted_title = f"[{cleaned_title}]({link_url})"
        else:
            formatted_title = cleaned_title

        title_prefix = "ğŸ†• " if title_data.get("is_new") else ""

        if show_source:
            result = f"<font color='grey'>[{title_data['source_name']}]</font> {title_prefix}{formatted_title}"
        else:
            result = f"{title_prefix}{formatted_title}"

        if rank_display:
            result += f" {rank_display}"
        if title_data["time_display"]:
            result += f" <font color='grey'>- {title_data['time_display']}</font>"
        if title_data["count"] > 1:
            result += f" <font color='green'>({title_data['count']}æ¬¡)</font>"

        return result

    elif platform == "dingtalk":
        if link_url:
            formatted_title = f"[{cleaned_title}]({link_url})"
        else:
            formatted_title = cleaned_title

        title_prefix = "ğŸ†• " if title_data.get("is_new") else ""

        if show_source:
            result = f"[{title_data['source_name']}] {title_prefix}{formatted_title}"
        else:
            result = f"{title_prefix}{formatted_title}"

        if rank_display:
            result += f" {rank_display}"
        if title_data["time_display"]:
            result += f" - {title_data['time_display']}"
        if title_data["count"] > 1:
            result += f" ({title_data['count']}æ¬¡)"

        return result

    elif platform == "wework":
        if link_url:
            formatted_title = f"[{cleaned_title}]({link_url})"
        else:
            formatted_title = cleaned_title

        title_prefix = "ğŸ†• " if title_data.get("is_new") else ""

        if show_source:
            result = f"[{title_data['source_name']}] {title_prefix}{formatted_title}"
        else:
            result = f"{title_prefix}{formatted_title}"

        if rank_display:
            result += f" {rank_display}"
        if title_data["time_display"]:
            result += f" - {title_data['time_display']}"
        if title_data["count"] > 1:
            result += f" ({title_data['count']}æ¬¡)"

        return result

    elif platform == "telegram":
        if link_url:
            formatted_title = f'<a href="{link_url}">{html_escape(cleaned_title)}</a>'
        else:
            formatted_title = cleaned_title

        title_prefix = "ğŸ†• " if title_data.get("is_new") else ""

        if show_source:
            result = f"[{title_data['source_name']}] {title_prefix}{formatted_title}"
        else:
            result = f"{title_prefix}{formatted_title}"

        if rank_display:
            result += f" {rank_display}"
        if title_data["time_display"]:
            result += f" <code>- {title_data['time_display']}</code>"
        if title_data["count"] > 1:
            result += f" <code>({title_data['count']}æ¬¡)</code>"

        return result

    elif platform == "html":
        rank_display = format_rank_display(
            title_data["ranks"], title_data["rank_threshold"], "html"
        )

        link_url = title_data["mobile_url"] or title_data["url"]

        escaped_title = html_escape(cleaned_title)
        escaped_source_name = html_escape(title_data["source_name"])

        if link_url:
            escaped_url = html_escape(link_url)
            formatted_title = f'[{escaped_source_name}] <a href="{escaped_url}" target="_blank" class="news-link">{escaped_title}</a>'
        else:
            formatted_title = (
                f'[{escaped_source_name}] <span class="no-link">{escaped_title}</span>'
            )

        if rank_display:
            formatted_title += f" {rank_display}"
        if title_data["time_display"]:
            escaped_time = html_escape(title_data["time_display"])
            formatted_title += f" <font color='grey'>- {escaped_time}</font>"
        if title_data["count"] > 1:
            formatted_title += f" <font color='green'>({title_data['count']}æ¬¡)</font>"

        if title_data.get("is_new"):
            formatted_title = f"<div class='new-title'>ğŸ†• {formatted_title}</div>"

        return formatted_title

    else:
        return cleaned_title


def generate_html_report(
    stats: List[Dict],
    total_titles: int,
    failed_ids: Optional[List] = None,
    new_titles: Optional[Dict] = None,
    id_to_name: Optional[Dict] = None,
    mode: str = "daily",
    is_daily_summary: bool = False,
) -> str:
    """ç”ŸæˆHTMLæŠ¥å‘Š"""
    if is_daily_summary:
        if mode == "current":
            filename = "å½“å‰æ¦œå•æ±‡æ€».html"
        elif mode == "incremental":
            filename = "å½“æ—¥å¢é‡.html"
        else:
            filename = "å½“æ—¥æ±‡æ€».html"
    else:
        filename = f"{format_time_filename()}.html"

    file_path = get_output_path("html", filename)

    report_data = prepare_report_data(stats, failed_ids, new_titles, id_to_name, mode)

    html_content = render_html_content(
        report_data, total_titles, is_daily_summary, mode
    )

    with open(file_path, "w", encoding="utf-8") as f:
        f.write(html_content)

    if is_daily_summary:
        root_file_path = Path("index.html")
        with open(root_file_path, "w", encoding="utf-8") as f:
            f.write(html_content)

    return file_path


def render_html_content(
    report_data: Dict,
    total_titles: int,
    is_daily_summary: bool = False,
    mode: str = "daily",
) -> str:
    """æ¸²æŸ“HTMLå†…å®¹"""
    html = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>é¢‘ç‡è¯ç»Ÿè®¡æŠ¥å‘Š</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; }
            h1, h2 { color: #333; }
            table { border-collapse: collapse; width: 100%; margin-top: 20px; }
            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
            th { background-color: #f2f2f2; }
            tr:nth-child(even) { background-color: #f9f9f9; }
            .word { font-weight: bold; }
            .count { text-align: center; }
            .percentage { text-align: center; }
            .titles { max-width: 500px; }
            .source { color: #666; font-style: italic; }
            .error { color: #d9534f; }
            .news-link { 
                color: #007bff; 
                text-decoration: none; 
                border-bottom: 1px dotted #007bff;
            }
            .news-link:hover { 
                color: #0056b3; 
                text-decoration: underline; 
            }
            .news-link:visited { 
                color: #6f42c1; 
            }
            .no-link { 
                color: #333; 
            }
            .new-title {
                background-color: #fff3cd;
                border: 1px solid #ffc107;
                border-radius: 3px;
                padding: 2px 6px;
                margin: 2px 0;
            }
            .new-section {
                background-color: #d1ecf1;
                border: 1px solid #bee5eb;
                border-radius: 5px;
                padding: 10px;
                margin-top: 10px;
            }
            .new-section h3 {
                color: #0c5460;
                margin-top: 0;
            }
        </style>
    </head>
    <body>
        <h1>é¢‘ç‡è¯ç»Ÿè®¡æŠ¥å‘Š</h1>
    """

    if is_daily_summary:
        if mode == "current":
            html += "<p>æŠ¥å‘Šç±»å‹: å½“å‰æ¦œå•æ¨¡å¼</p>"
        elif mode == "incremental":
            html += "<p>æŠ¥å‘Šç±»å‹: å¢é‡æ¨¡å¼</p>"
        else:
            html += "<p>æŠ¥å‘Šç±»å‹: å½“æ—¥æ±‡æ€»</p>"
    else:
        html += "<p>æŠ¥å‘Šç±»å‹: å®æ—¶åˆ†æ</p>"

    now = get_beijing_time()
    html += f"<p>æ€»æ ‡é¢˜æ•°: {total_titles}</p>"
    html += f"<p>ç”Ÿæˆæ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')}</p>"

    if report_data["failed_ids"]:
        html += """
        <div class="error">
            <h2>è¯·æ±‚å¤±è´¥çš„å¹³å°</h2>
            <ul>
        """
        for id_value in report_data["failed_ids"]:
            html += f"<li>{html_escape(id_value)}</li>"
        html += """
            </ul>
        </div>
        """

    html += """
        <table>
            <tr>
                <th>æ’å</th>
                <th>é¢‘ç‡è¯</th>
                <th>å‡ºç°æ¬¡æ•°</th>
                <th>å æ¯”</th>
                <th>ç›¸å…³æ ‡é¢˜</th>
            </tr>
    """

    for i, stat in enumerate(report_data["stats"], 1):
        formatted_titles = []

        for title_data in stat["titles"]:
            formatted_title = format_title_for_platform("html", title_data)
            formatted_titles.append(formatted_title)

        escaped_word = html_escape(stat["word"])
        html += f"""
            <tr>
                <td>{i}</td>
                <td class="word">{escaped_word}</td>
                <td class="count">{stat['count']}</td>
                <td class="percentage">{stat.get('percentage', 0)}%</td>
                <td class="titles">{"<br>".join(formatted_titles)}</td>
            </tr>
        """

    html += """
        </table>
    """

    if report_data["new_titles"]:
        html += f"""
        <div class="new-section">
            <h3>ğŸ†• æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—» (å…± {report_data['total_new_count']} æ¡)</h3>
        """

        for source_data in report_data["new_titles"]:
            escaped_source = html_escape(source_data["source_name"])
            html += f"<h4>{escaped_source} ({len(source_data['titles'])} æ¡)</h4><ul>"

            for title_data in source_data["titles"]:
                title_data_copy = title_data.copy()
                title_data_copy["is_new"] = False
                formatted_title = format_title_for_platform("html", title_data_copy)
                if "] " in formatted_title:
                    formatted_title = formatted_title.split("] ", 1)[1]
                html += f"<li>{formatted_title}</li>"

            html += "</ul>"

        html += "</div>"

    html += """
    </body>
    </html>
    """

    return html


def render_feishu_content(
    report_data: Dict, update_info: Optional[Dict] = None, mode: str = "daily"
) -> str:
    """æ¸²æŸ“é£ä¹¦å†…å®¹"""
    text_content = ""

    if report_data["stats"]:
        text_content += f"ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**\n\n"

    total_count = len(report_data["stats"])

    for i, stat in enumerate(report_data["stats"]):
        word = stat["word"]
        count = stat["count"]

        sequence_display = f"<font color='grey'>[{i + 1}/{total_count}]</font>"

        if count >= 10:
            text_content += f"ğŸ”¥ {sequence_display} **{word}** : <font color='red'>{count}</font> æ¡\n\n"
        elif count >= 5:
            text_content += f"ğŸ“ˆ {sequence_display} **{word}** : <font color='orange'>{count}</font> æ¡\n\n"
        else:
            text_content += f"ğŸ“Œ {sequence_display} **{word}** : {count} æ¡\n\n"

        for j, title_data in enumerate(stat["titles"], 1):
            formatted_title = format_title_for_platform(
                "feishu", title_data, show_source=True
            )
            text_content += f"  {j}. {formatted_title}\n"

            if j < len(stat["titles"]):
                text_content += "\n"

        if i < len(report_data["stats"]) - 1:
            text_content += f"\n{CONFIG['FEISHU_MESSAGE_SEPARATOR']}\n\n"

    if not text_content:
        if mode == "incremental":
            mode_text = "å¢é‡æ¨¡å¼ä¸‹æš‚æ— æ–°å¢åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡"
        elif mode == "current":
            mode_text = "å½“å‰æ¦œå•æ¨¡å¼ä¸‹æš‚æ— åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡"
        else:
            mode_text = "æš‚æ— åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡"
        text_content = f"ğŸ“­ {mode_text}\n\n"

    if report_data["new_titles"]:
        if text_content and "æš‚æ— åŒ¹é…" not in text_content:
            text_content += f"\n{CONFIG['FEISHU_MESSAGE_SEPARATOR']}\n\n"

        text_content += (
            f"ğŸ†• **æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—»** (å…± {report_data['total_new_count']} æ¡)\n\n"
        )

        for source_data in report_data["new_titles"]:
            text_content += (
                f"**{source_data['source_name']}** ({len(source_data['titles'])} æ¡):\n"
            )

            for j, title_data in enumerate(source_data["titles"], 1):
                title_data_copy = title_data.copy()
                title_data_copy["is_new"] = False
                formatted_title = format_title_for_platform(
                    "feishu", title_data_copy, show_source=False
                )
                text_content += f"  {j}. {formatted_title}\n"

            text_content += "\n"

    if report_data["failed_ids"]:
        if text_content and "æš‚æ— åŒ¹é…" not in text_content:
            text_content += f"\n{CONFIG['FEISHU_MESSAGE_SEPARATOR']}\n\n"

        text_content += "âš ï¸ **æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š**\n\n"
        for i, id_value in enumerate(report_data["failed_ids"], 1):
            text_content += f"  â€¢ <font color='red'>{id_value}</font>\n"

    now = get_beijing_time()
    text_content += (
        f"\n\n<font color='grey'>æ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}</font>"
    )

    if update_info:
        text_content += f"\n<font color='grey'>TrendRadar å‘ç°æ–°ç‰ˆæœ¬ {update_info['remote_version']}ï¼Œå½“å‰ {update_info['current_version']}</font>"

    return text_content


def render_dingtalk_content(
    report_data: Dict, update_info: Optional[Dict] = None, mode: str = "daily"
) -> str:
    """æ¸²æŸ“é’‰é’‰å†…å®¹"""
    text_content = ""

    total_titles = sum(
        len(stat["titles"]) for stat in report_data["stats"] if stat["count"] > 0
    )
    now = get_beijing_time()

    text_content += f"**æ€»æ–°é—»æ•°ï¼š** {total_titles}\n\n"
    text_content += f"**æ—¶é—´ï¼š** {now.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    text_content += f"**ç±»å‹ï¼š** çƒ­ç‚¹åˆ†ææŠ¥å‘Š\n\n"

    text_content += "---\n\n"

    if report_data["stats"]:
        text_content += f"ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**\n\n"

        total_count = len(report_data["stats"])

        for i, stat in enumerate(report_data["stats"]):
            word = stat["word"]
            count = stat["count"]

            sequence_display = f"[{i + 1}/{total_count}]"

            if count >= 10:
                text_content += f"ğŸ”¥ {sequence_display} **{word}** : **{count}** æ¡\n\n"
            elif count >= 5:
                text_content += f"ğŸ“ˆ {sequence_display} **{word}** : **{count}** æ¡\n\n"
            else:
                text_content += f"ğŸ“Œ {sequence_display} **{word}** : {count} æ¡\n\n"

            for j, title_data in enumerate(stat["titles"], 1):
                formatted_title = format_title_for_platform(
                    "dingtalk", title_data, show_source=True
                )
                text_content += f"  {j}. {formatted_title}\n"

                if j < len(stat["titles"]):
                    text_content += "\n"

            if i < len(report_data["stats"]) - 1:
                text_content += f"\n---\n\n"

    if not report_data["stats"]:
        if mode == "incremental":
            mode_text = "å¢é‡æ¨¡å¼ä¸‹æš‚æ— æ–°å¢åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡"
        elif mode == "current":
            mode_text = "å½“å‰æ¦œå•æ¨¡å¼ä¸‹æš‚æ— åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡"
        else:
            mode_text = "æš‚æ— åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡"
        text_content += f"ğŸ“­ {mode_text}\n\n"

    if report_data["new_titles"]:
        if text_content and "æš‚æ— åŒ¹é…" not in text_content:
            text_content += f"\n---\n\n"

        text_content += (
            f"ğŸ†• **æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—»** (å…± {report_data['total_new_count']} æ¡)\n\n"
        )

        for source_data in report_data["new_titles"]:
            text_content += f"**{source_data['source_name']}** ({len(source_data['titles'])} æ¡):\n\n"

            for j, title_data in enumerate(source_data["titles"], 1):
                title_data_copy = title_data.copy()
                title_data_copy["is_new"] = False
                formatted_title = format_title_for_platform(
                    "dingtalk", title_data_copy, show_source=False
                )
                text_content += f"  {j}. {formatted_title}\n"

            text_content += "\n"

    if report_data["failed_ids"]:
        if text_content and "æš‚æ— åŒ¹é…" not in text_content:
            text_content += f"\n---\n\n"

        text_content += "âš ï¸ **æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š**\n\n"
        for i, id_value in enumerate(report_data["failed_ids"], 1):
            text_content += f"  â€¢ **{id_value}**\n"

    text_content += f"\n\n> æ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}"

    if update_info:
        text_content += f"\n> TrendRadar å‘ç°æ–°ç‰ˆæœ¬ **{update_info['remote_version']}**ï¼Œå½“å‰ **{update_info['current_version']}**"

    return text_content


def split_content_into_batches(
    report_data: Dict,
    format_type: str,
    update_info: Optional[Dict] = None,
    max_bytes: int = CONFIG["MESSAGE_BATCH_SIZE"],
    mode: str = "daily",
) -> List[str]:
    """åˆ†æ‰¹å¤„ç†æ¶ˆæ¯å†…å®¹ï¼Œç¡®ä¿è¯ç»„æ ‡é¢˜+è‡³å°‘ç¬¬ä¸€æ¡æ–°é—»çš„å®Œæ•´æ€§"""
    batches = []

    total_titles = sum(
        len(stat["titles"]) for stat in report_data["stats"] if stat["count"] > 0
    )
    now = get_beijing_time()

    base_header = ""
    if format_type == "wework":
        base_header = f"**æ€»æ–°é—»æ•°ï¼š** {total_titles}\n\n\n\n"
    elif format_type == "telegram":
        base_header = f"æ€»æ–°é—»æ•°ï¼š {total_titles}\n\n"

    base_footer = ""
    if format_type == "wework":
        base_footer = f"\n\n\n> æ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}"
        if update_info:
            base_footer += f"\n> TrendRadar å‘ç°æ–°ç‰ˆæœ¬ **{update_info['remote_version']}**ï¼Œå½“å‰ **{update_info['current_version']}**"
    elif format_type == "telegram":
        base_footer = f"\n\næ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}"
        if update_info:
            base_footer += f"\nTrendRadar å‘ç°æ–°ç‰ˆæœ¬ {update_info['remote_version']}ï¼Œå½“å‰ {update_info['current_version']}"

    stats_header = ""
    if report_data["stats"]:
        if format_type == "wework":
            stats_header = f"ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**\n\n"
        elif format_type == "telegram":
            stats_header = f"ğŸ“Š çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡\n\n"

    current_batch = base_header
    current_batch_has_content = False

    if (
        not report_data["stats"]
        and not report_data["new_titles"]
        and not report_data["failed_ids"]
    ):
        if mode == "incremental":
            mode_text = "å¢é‡æ¨¡å¼ä¸‹æš‚æ— æ–°å¢åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡"
        elif mode == "current":
            mode_text = "å½“å‰æ¦œå•æ¨¡å¼ä¸‹æš‚æ— åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡"
        else:
            mode_text = "æš‚æ— åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡"
        simple_content = f"ğŸ“­ {mode_text}\n\n"
        final_content = base_header + simple_content + base_footer
        batches.append(final_content)
        return batches

    # å¤„ç†çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡
    if report_data["stats"]:
        total_count = len(report_data["stats"])

        # æ·»åŠ ç»Ÿè®¡æ ‡é¢˜
        test_content = current_batch + stats_header
        if (
            len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
            < max_bytes
        ):
            current_batch = test_content
            current_batch_has_content = True
        else:
            if current_batch_has_content:
                batches.append(current_batch + base_footer)
            current_batch = base_header + stats_header
            current_batch_has_content = True

        # é€ä¸ªå¤„ç†è¯ç»„ï¼ˆç¡®ä¿è¯ç»„æ ‡é¢˜+ç¬¬ä¸€æ¡æ–°é—»çš„åŸå­æ€§ï¼‰
        for i, stat in enumerate(report_data["stats"]):
            word = stat["word"]
            count = stat["count"]
            sequence_display = f"[{i + 1}/{total_count}]"

            # æ„å»ºè¯ç»„æ ‡é¢˜
            word_header = ""
            if format_type == "wework":
                if count >= 10:
                    word_header = (
                        f"ğŸ”¥ {sequence_display} **{word}** : **{count}** æ¡\n\n"
                    )
                elif count >= 5:
                    word_header = (
                        f"ğŸ“ˆ {sequence_display} **{word}** : **{count}** æ¡\n\n"
                    )
                else:
                    word_header = f"ğŸ“Œ {sequence_display} **{word}** : {count} æ¡\n\n"
            elif format_type == "telegram":
                if count >= 10:
                    word_header = f"ğŸ”¥ {sequence_display} {word} : {count} æ¡\n\n"
                elif count >= 5:
                    word_header = f"ğŸ“ˆ {sequence_display} {word} : {count} æ¡\n\n"
                else:
                    word_header = f"ğŸ“Œ {sequence_display} {word} : {count} æ¡\n\n"

            # æ„å»ºç¬¬ä¸€æ¡æ–°é—»
            first_news_line = ""
            if stat["titles"]:
                first_title_data = stat["titles"][0]
                if format_type == "wework":
                    formatted_title = format_title_for_platform(
                        "wework", first_title_data, show_source=True
                    )
                elif format_type == "telegram":
                    formatted_title = format_title_for_platform(
                        "telegram", first_title_data, show_source=True
                    )
                else:
                    formatted_title = f"{first_title_data['title']}"

                first_news_line = f"  1. {formatted_title}\n"
                if len(stat["titles"]) > 1:
                    first_news_line += "\n"

            # åŸå­æ€§æ£€æŸ¥ï¼šè¯ç»„æ ‡é¢˜+ç¬¬ä¸€æ¡æ–°é—»å¿…é¡»ä¸€èµ·å¤„ç†
            word_with_first_news = word_header + first_news_line
            test_content = current_batch + word_with_first_news

            if (
                len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                >= max_bytes
            ):
                # å½“å‰æ‰¹æ¬¡å®¹çº³ä¸ä¸‹ï¼Œå¼€å¯æ–°æ‰¹æ¬¡
                if current_batch_has_content:
                    batches.append(current_batch + base_footer)
                current_batch = base_header + stats_header + word_with_first_news
                current_batch_has_content = True
                start_index = 1
            else:
                current_batch = test_content
                current_batch_has_content = True
                start_index = 1

            # å¤„ç†å‰©ä½™æ–°é—»æ¡ç›®
            for j in range(start_index, len(stat["titles"])):
                title_data = stat["titles"][j]
                if format_type == "wework":
                    formatted_title = format_title_for_platform(
                        "wework", title_data, show_source=True
                    )
                elif format_type == "telegram":
                    formatted_title = format_title_for_platform(
                        "telegram", title_data, show_source=True
                    )
                else:
                    formatted_title = f"{title_data['title']}"

                news_line = f"  {j + 1}. {formatted_title}\n"
                if j < len(stat["titles"]) - 1:
                    news_line += "\n"

                test_content = current_batch + news_line
                if (
                    len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                    >= max_bytes
                ):
                    if current_batch_has_content:
                        batches.append(current_batch + base_footer)
                    current_batch = base_header + stats_header + word_header + news_line
                    current_batch_has_content = True
                else:
                    current_batch = test_content
                    current_batch_has_content = True

            # è¯ç»„é—´åˆ†éš”ç¬¦
            if i < len(report_data["stats"]) - 1:
                separator = ""
                if format_type == "wework":
                    separator = f"\n\n\n\n"
                elif format_type == "telegram":
                    separator = f"\n\n"

                test_content = current_batch + separator
                if (
                    len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                    < max_bytes
                ):
                    current_batch = test_content

    # å¤„ç†æ–°å¢æ–°é—»ï¼ˆåŒæ ·ç¡®ä¿æ¥æºæ ‡é¢˜+ç¬¬ä¸€æ¡æ–°é—»çš„åŸå­æ€§ï¼‰
    if report_data["new_titles"]:
        new_header = ""
        if format_type == "wework":
            new_header = f"\n\n\n\nğŸ†• **æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—»** (å…± {report_data['total_new_count']} æ¡)\n\n"
        elif format_type == "telegram":
            new_header = (
                f"\n\nğŸ†• æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—» (å…± {report_data['total_new_count']} æ¡)\n\n"
            )

        test_content = current_batch + new_header
        if (
            len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
            >= max_bytes
        ):
            if current_batch_has_content:
                batches.append(current_batch + base_footer)
            current_batch = base_header + new_header
            current_batch_has_content = True
        else:
            current_batch = test_content
            current_batch_has_content = True

        # é€ä¸ªå¤„ç†æ–°å¢æ–°é—»æ¥æº
        for source_data in report_data["new_titles"]:
            source_header = ""
            if format_type == "wework":
                source_header = f"**{source_data['source_name']}** ({len(source_data['titles'])} æ¡):\n\n"
            elif format_type == "telegram":
                source_header = f"{source_data['source_name']} ({len(source_data['titles'])} æ¡):\n\n"

            # æ„å»ºç¬¬ä¸€æ¡æ–°å¢æ–°é—»
            first_news_line = ""
            if source_data["titles"]:
                first_title_data = source_data["titles"][0]
                title_data_copy = first_title_data.copy()
                title_data_copy["is_new"] = False

                if format_type == "wework":
                    formatted_title = format_title_for_platform(
                        "wework", title_data_copy, show_source=False
                    )
                elif format_type == "telegram":
                    formatted_title = format_title_for_platform(
                        "telegram", title_data_copy, show_source=False
                    )
                else:
                    formatted_title = f"{title_data_copy['title']}"

                first_news_line = f"  1. {formatted_title}\n"

            # åŸå­æ€§æ£€æŸ¥ï¼šæ¥æºæ ‡é¢˜+ç¬¬ä¸€æ¡æ–°é—»
            source_with_first_news = source_header + first_news_line
            test_content = current_batch + source_with_first_news

            if (
                len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                >= max_bytes
            ):
                if current_batch_has_content:
                    batches.append(current_batch + base_footer)
                current_batch = base_header + new_header + source_with_first_news
                current_batch_has_content = True
                start_index = 1
            else:
                current_batch = test_content
                current_batch_has_content = True
                start_index = 1

            # å¤„ç†å‰©ä½™æ–°å¢æ–°é—»
            for j in range(start_index, len(source_data["titles"])):
                title_data = source_data["titles"][j]
                title_data_copy = title_data.copy()
                title_data_copy["is_new"] = False

                if format_type == "wework":
                    formatted_title = format_title_for_platform(
                        "wework", title_data_copy, show_source=False
                    )
                elif format_type == "telegram":
                    formatted_title = format_title_for_platform(
                        "telegram", title_data_copy, show_source=False
                    )
                else:
                    formatted_title = f"{title_data_copy['title']}"

                news_line = f"  {j + 1}. {formatted_title}\n"

                test_content = current_batch + news_line
                if (
                    len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                    >= max_bytes
                ):
                    if current_batch_has_content:
                        batches.append(current_batch + base_footer)
                    current_batch = base_header + new_header + source_header + news_line
                    current_batch_has_content = True
                else:
                    current_batch = test_content
                    current_batch_has_content = True

            current_batch += "\n"

    if report_data["failed_ids"]:
        failed_header = ""
        if format_type == "wework":
            failed_header = f"\n\n\n\nâš ï¸ **æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š**\n\n"
        elif format_type == "telegram":
            failed_header = f"\n\nâš ï¸ æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š\n\n"

        test_content = current_batch + failed_header
        if (
            len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
            >= max_bytes
        ):
            if current_batch_has_content:
                batches.append(current_batch + base_footer)
            current_batch = base_header + failed_header
            current_batch_has_content = True
        else:
            current_batch = test_content
            current_batch_has_content = True

        for i, id_value in enumerate(report_data["failed_ids"], 1):
            failed_line = f"  â€¢ {id_value}\n"
            test_content = current_batch + failed_line
            if (
                len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                >= max_bytes
            ):
                if current_batch_has_content:
                    batches.append(current_batch + base_footer)
                current_batch = base_header + failed_header + failed_line
                current_batch_has_content = True
            else:
                current_batch = test_content
                current_batch_has_content = True

    # å®Œæˆæœ€åæ‰¹æ¬¡
    if current_batch_has_content:
        batches.append(current_batch + base_footer)

    return batches


def send_to_webhooks(
    stats: List[Dict],
    failed_ids: Optional[List] = None,
    report_type: str = "å½“æ—¥æ±‡æ€»",
    new_titles: Optional[Dict] = None,
    id_to_name: Optional[Dict] = None,
    update_info: Optional[Dict] = None,
    proxy_url: Optional[str] = None,
    mode: str = "daily",
) -> Dict[str, bool]:
    """å‘é€æ•°æ®åˆ°å¤šä¸ªwebhookå¹³å°"""
    results = {}

    report_data = prepare_report_data(stats, failed_ids, new_titles, id_to_name, mode)

    feishu_url = CONFIG["FEISHU_WEBHOOK_URL"]
    dingtalk_url = CONFIG["DINGTALK_WEBHOOK_URL"]
    wework_url = CONFIG["WEWORK_WEBHOOK_URL"]
    telegram_token = CONFIG["TELEGRAM_BOT_TOKEN"]
    telegram_chat_id = CONFIG["TELEGRAM_CHAT_ID"]

    update_info_to_send = update_info if CONFIG["SHOW_VERSION_UPDATE"] else None

    # å‘é€åˆ°é£ä¹¦
    if feishu_url:
        results["feishu"] = send_to_feishu(
            feishu_url, report_data, report_type, update_info_to_send, proxy_url, mode
        )

    # å‘é€åˆ°é’‰é’‰
    if dingtalk_url:
        results["dingtalk"] = send_to_dingtalk(
            dingtalk_url, report_data, report_type, update_info_to_send, proxy_url, mode
        )

    # å‘é€åˆ°ä¼ä¸šå¾®ä¿¡
    if wework_url:
        results["wework"] = send_to_wework(
            wework_url, report_data, report_type, update_info_to_send, proxy_url, mode
        )

    # å‘é€åˆ° Telegram
    if telegram_token and telegram_chat_id:
        results["telegram"] = send_to_telegram(
            telegram_token,
            telegram_chat_id,
            report_data,
            report_type,
            update_info_to_send,
            proxy_url,
            mode,
        )

    if not results:
        print("æœªé…ç½®ä»»ä½•webhook URLï¼Œè·³è¿‡é€šçŸ¥å‘é€")

    return results


def send_to_feishu(
    webhook_url: str,
    report_data: Dict,
    report_type: str,
    update_info: Optional[Dict] = None,
    proxy_url: Optional[str] = None,
    mode: str = "daily",
) -> bool:
    """å‘é€åˆ°é£ä¹¦"""
    headers = {"Content-Type": "application/json"}

    text_content = render_feishu_content(report_data, update_info, mode)
    total_titles = sum(
        len(stat["titles"]) for stat in report_data["stats"] if stat["count"] > 0
    )

    now = get_beijing_time()
    payload = {
        "msg_type": "text",
        "content": {
            "total_titles": total_titles,
            "timestamp": now.strftime("%Y-%m-%d %H:%M:%S"),
            "report_type": report_type,
            "text": text_content,
        },
    }

    proxies = None
    if proxy_url:
        proxies = {"http": proxy_url, "https": proxy_url}

    try:
        response = requests.post(
            webhook_url, headers=headers, json=payload, proxies=proxies, timeout=30
        )
        if response.status_code == 200:
            print(f"é£ä¹¦é€šçŸ¥å‘é€æˆåŠŸ [{report_type}]")
            return True
        else:
            print(f"é£ä¹¦é€šçŸ¥å‘é€å¤±è´¥ [{report_type}]ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
            return False
    except Exception as e:
        print(f"é£ä¹¦é€šçŸ¥å‘é€å‡ºé”™ [{report_type}]ï¼š{e}")
        return False


def send_to_dingtalk(
    webhook_url: str,
    report_data: Dict,
    report_type: str,
    update_info: Optional[Dict] = None,
    proxy_url: Optional[str] = None,
    mode: str = "daily",
) -> bool:
    """å‘é€åˆ°é’‰é’‰"""
    headers = {"Content-Type": "application/json"}

    text_content = render_dingtalk_content(report_data, update_info, mode)

    payload = {
        "msgtype": "markdown",
        "markdown": {
            "title": f"TrendRadar çƒ­ç‚¹åˆ†ææŠ¥å‘Š - {report_type}",
            "text": text_content,
        },
    }

    proxies = None
    if proxy_url:
        proxies = {"http": proxy_url, "https": proxy_url}

    try:
        response = requests.post(
            webhook_url, headers=headers, json=payload, proxies=proxies, timeout=30
        )
        if response.status_code == 200:
            result = response.json()
            if result.get("errcode") == 0:
                print(f"é’‰é’‰é€šçŸ¥å‘é€æˆåŠŸ [{report_type}]")
                return True
            else:
                print(f"é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥ [{report_type}]ï¼Œé”™è¯¯ï¼š{result.get('errmsg')}")
                return False
        else:
            print(f"é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥ [{report_type}]ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
            return False
    except Exception as e:
        print(f"é’‰é’‰é€šçŸ¥å‘é€å‡ºé”™ [{report_type}]ï¼š{e}")
        return False


def send_to_wework(
    webhook_url: str,
    report_data: Dict,
    report_type: str,
    update_info: Optional[Dict] = None,
    proxy_url: Optional[str] = None,
    mode: str = "daily",
) -> bool:
    """å‘é€åˆ°ä¼ä¸šå¾®ä¿¡ï¼ˆæ”¯æŒåˆ†æ‰¹å‘é€ï¼‰"""
    headers = {"Content-Type": "application/json"}
    proxies = None
    if proxy_url:
        proxies = {"http": proxy_url, "https": proxy_url}

    # è·å–åˆ†æ‰¹å†…å®¹
    batches = split_content_into_batches(report_data, "wework", update_info, mode=mode)

    print(f"ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯åˆ†ä¸º {len(batches)} æ‰¹æ¬¡å‘é€ [{report_type}]")

    # é€æ‰¹å‘é€
    for i, batch_content in enumerate(batches, 1):
        batch_size = len(batch_content.encode("utf-8"))
        print(
            f"å‘é€ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡ï¼Œå¤§å°ï¼š{batch_size} å­—èŠ‚ [{report_type}]"
        )

        # æ·»åŠ æ‰¹æ¬¡æ ‡è¯†
        if len(batches) > 1:
            batch_header = f"**[ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡]**\n\n"
            batch_content = batch_header + batch_content

        payload = {"msgtype": "markdown", "markdown": {"content": batch_content}}

        try:
            response = requests.post(
                webhook_url, headers=headers, json=payload, proxies=proxies, timeout=30
            )
            if response.status_code == 200:
                result = response.json()
                if result.get("errcode") == 0:
                    print(f"ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€æˆåŠŸ [{report_type}]")
                    # æ‰¹æ¬¡é—´é—´éš”
                    if i < len(batches):
                        time.sleep(CONFIG["BATCH_SEND_INTERVAL"])
                else:
                    print(
                        f"ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼Œé”™è¯¯ï¼š{result.get('errmsg')}"
                    )
                    return False
            else:
                print(
                    f"ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}"
                )
                return False
        except Exception as e:
            print(f"ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å‡ºé”™ [{report_type}]ï¼š{e}")
            return False

    print(f"ä¼ä¸šå¾®ä¿¡æ‰€æœ‰ {len(batches)} æ‰¹æ¬¡å‘é€å®Œæˆ [{report_type}]")
    return True


def send_to_telegram(
    bot_token: str,
    chat_id: str,
    report_data: Dict,
    report_type: str,
    update_info: Optional[Dict] = None,
    proxy_url: Optional[str] = None,
    mode: str = "daily",
) -> bool:
    """å‘é€åˆ°Telegramï¼ˆæ”¯æŒåˆ†æ‰¹å‘é€ï¼‰"""
    headers = {"Content-Type": "application/json"}
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"

    proxies = None
    if proxy_url:
        proxies = {"http": proxy_url, "https": proxy_url}

    # è·å–åˆ†æ‰¹å†…å®¹
    batches = split_content_into_batches(
        report_data, "telegram", update_info, mode=mode
    )

    print(f"Telegramæ¶ˆæ¯åˆ†ä¸º {len(batches)} æ‰¹æ¬¡å‘é€ [{report_type}]")

    # é€æ‰¹å‘é€
    for i, batch_content in enumerate(batches, 1):
        batch_size = len(batch_content.encode("utf-8"))
        print(
            f"å‘é€Telegramç¬¬ {i}/{len(batches)} æ‰¹æ¬¡ï¼Œå¤§å°ï¼š{batch_size} å­—èŠ‚ [{report_type}]"
        )

        # æ·»åŠ æ‰¹æ¬¡æ ‡è¯†
        if len(batches) > 1:
            batch_header = f"<b>[ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡]</b>\n\n"
            batch_content = batch_header + batch_content

        payload = {
            "chat_id": chat_id,
            "text": batch_content,
            "parse_mode": "HTML",
            "disable_web_page_preview": True,
        }

        try:
            response = requests.post(
                url, headers=headers, json=payload, proxies=proxies, timeout=30
            )
            if response.status_code == 200:
                result = response.json()
                if result.get("ok"):
                    print(f"Telegramç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€æˆåŠŸ [{report_type}]")
                    # æ‰¹æ¬¡é—´é—´éš”
                    if i < len(batches):
                        time.sleep(CONFIG["BATCH_SEND_INTERVAL"])
                else:
                    print(
                        f"Telegramç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼Œé”™è¯¯ï¼š{result.get('description')}"
                    )
                    return False
            else:
                print(
                    f"Telegramç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}"
                )
                return False
        except Exception as e:
            print(f"Telegramç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å‡ºé”™ [{report_type}]ï¼š{e}")
            return False

    print(f"Telegramæ‰€æœ‰ {len(batches)} æ‰¹æ¬¡å‘é€å®Œæˆ [{report_type}]")
    return True


# === ä¸»åˆ†æå™¨ ===
class NewsAnalyzer:
    """æ–°é—»åˆ†æå™¨"""

    # æ¨¡å¼ç­–ç•¥å®šä¹‰
    MODE_STRATEGIES = {
        "incremental": {
            "mode_name": "å¢é‡æ¨¡å¼",
            "description": "å¢é‡æ¨¡å¼ï¼ˆåªå…³æ³¨æ–°å¢æ–°é—»ï¼Œæ— æ–°å¢æ—¶ä¸æ¨é€ï¼‰",
            "realtime_report_type": "å®æ—¶å¢é‡",
            "summary_report_type": "å½“æ—¥æ±‡æ€»",
            "should_send_realtime": True,
            "should_generate_summary": True,
            "summary_mode": "daily",
        },
        "current": {
            "mode_name": "å½“å‰æ¦œå•æ¨¡å¼",
            "description": "å½“å‰æ¦œå•æ¨¡å¼ï¼ˆå½“å‰æ¦œå•åŒ¹é…æ–°é—» + æ–°å¢æ–°é—»åŒºåŸŸ + æŒ‰æ—¶æ¨é€ï¼‰",
            "realtime_report_type": "å®æ—¶å½“å‰æ¦œå•",
            "summary_report_type": "å½“å‰æ¦œå•æ±‡æ€»",
            "should_send_realtime": True,
            "should_generate_summary": True,
            "summary_mode": "current",
        },
        "daily": {
            "mode_name": "å½“æ—¥æ±‡æ€»æ¨¡å¼",
            "description": "å½“æ—¥æ±‡æ€»æ¨¡å¼ï¼ˆæ‰€æœ‰åŒ¹é…æ–°é—» + æ–°å¢æ–°é—»åŒºåŸŸ + æŒ‰æ—¶æ¨é€ï¼‰",
            "realtime_report_type": "",
            "summary_report_type": "å½“æ—¥æ±‡æ€»",
            "should_send_realtime": False,
            "should_generate_summary": True,
            "summary_mode": "daily",
        },
    }

    def __init__(self):
        self.request_interval = CONFIG["REQUEST_INTERVAL"]
        self.report_mode = CONFIG["REPORT_MODE"]
        self.rank_threshold = CONFIG["RANK_THRESHOLD"]
        self.is_github_actions = os.environ.get("GITHUB_ACTIONS") == "true"
        self.is_docker_container = self._detect_docker_environment()
        self.update_info = None
        self.proxy_url = None
        self._setup_proxy()
        self.data_fetcher = DataFetcher(self.proxy_url)

        if self.is_github_actions:
            self._check_version_update()

    def _detect_docker_environment(self) -> bool:
        """æ£€æµ‹æ˜¯å¦è¿è¡Œåœ¨ Docker å®¹å™¨ä¸­"""
        try:
            if os.environ.get("DOCKER_CONTAINER") == "true":
                return True

            if os.path.exists("/.dockerenv"):
                return True

            return False
        except Exception:
            return False

    def _should_open_browser(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰“å¼€æµè§ˆå™¨"""
        return not self.is_github_actions and not self.is_docker_container

    def _setup_proxy(self) -> None:
        """è®¾ç½®ä»£ç†é…ç½®"""
        if not self.is_github_actions and CONFIG["USE_PROXY"]:
            self.proxy_url = CONFIG["DEFAULT_PROXY"]
            print("æœ¬åœ°ç¯å¢ƒï¼Œä½¿ç”¨ä»£ç†")
        elif not self.is_github_actions and not CONFIG["USE_PROXY"]:
            print("æœ¬åœ°ç¯å¢ƒï¼Œæœªå¯ç”¨ä»£ç†")
        else:
            print("GitHub Actionsç¯å¢ƒï¼Œä¸ä½¿ç”¨ä»£ç†")

    def _check_version_update(self) -> None:
        """æ£€æŸ¥ç‰ˆæœ¬æ›´æ–°"""
        try:
            need_update, remote_version = check_version_update(
                VERSION, CONFIG["VERSION_CHECK_URL"], self.proxy_url
            )

            if need_update and remote_version:
                self.update_info = {
                    "current_version": VERSION,
                    "remote_version": remote_version,
                }
                print(f"å‘ç°æ–°ç‰ˆæœ¬: {remote_version} (å½“å‰: {VERSION})")
            else:
                print("ç‰ˆæœ¬æ£€æŸ¥å®Œæˆï¼Œå½“å‰ä¸ºæœ€æ–°ç‰ˆæœ¬")
        except Exception as e:
            print(f"ç‰ˆæœ¬æ£€æŸ¥å‡ºé”™: {e}")

    def _get_mode_strategy(self) -> Dict:
        """è·å–å½“å‰æ¨¡å¼çš„ç­–ç•¥é…ç½®"""
        return self.MODE_STRATEGIES.get(self.report_mode, self.MODE_STRATEGIES["daily"])

    def _has_webhook_configured(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦é…ç½®äº†webhook"""
        return any(
            [
                CONFIG["FEISHU_WEBHOOK_URL"],
                CONFIG["DINGTALK_WEBHOOK_URL"],
                CONFIG["WEWORK_WEBHOOK_URL"],
                (CONFIG["TELEGRAM_BOT_TOKEN"] and CONFIG["TELEGRAM_CHAT_ID"]),
            ]
        )

    def _has_valid_content(
        self, stats: List[Dict], new_titles: Optional[Dict] = None
    ) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æ–°é—»å†…å®¹"""
        if self.report_mode in ["incremental", "current"]:
            # å¢é‡æ¨¡å¼å’Œcurrentæ¨¡å¼ä¸‹ï¼Œåªè¦statsæœ‰å†…å®¹å°±è¯´æ˜æœ‰åŒ¹é…çš„æ–°é—»
            return any(stat["count"] > 0 for stat in stats)
        else:
            # å½“æ—¥æ±‡æ€»æ¨¡å¼ä¸‹ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„é¢‘ç‡è¯æ–°é—»æˆ–æ–°å¢æ–°é—»
            has_matched_news = any(stat["count"] > 0 for stat in stats)
            has_new_news = bool(
                new_titles and any(len(titles) > 0 for titles in new_titles.values())
            )
            return has_matched_news or has_new_news

    def _load_analysis_data(
        self,
    ) -> Optional[Tuple[Dict, Dict, Dict, Dict, List, List]]:
        """ç»Ÿä¸€çš„æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼Œä½¿ç”¨å½“å‰ç›‘æ§å¹³å°åˆ—è¡¨è¿‡æ»¤å†å²æ•°æ®"""
        try:
            # è·å–å½“å‰é…ç½®çš„ç›‘æ§å¹³å°IDåˆ—è¡¨
            current_platform_ids = []
            for platform in CONFIG["PLATFORMS"]:
                current_platform_ids.append(platform["id"])

            print(f"å½“å‰ç›‘æ§å¹³å°: {current_platform_ids}")

            all_results, id_to_name, title_info = read_all_today_titles(
                current_platform_ids
            )

            if not all_results:
                print("æ²¡æœ‰æ‰¾åˆ°å½“å¤©çš„æ•°æ®")
                return None

            total_titles = sum(len(titles) for titles in all_results.values())
            print(f"è¯»å–åˆ° {total_titles} ä¸ªæ ‡é¢˜ï¼ˆå·²æŒ‰å½“å‰ç›‘æ§å¹³å°è¿‡æ»¤ï¼‰")

            new_titles = detect_latest_new_titles(current_platform_ids)
            word_groups, filter_words = load_frequency_words()

            return (
                all_results,
                id_to_name,
                title_info,
                new_titles,
                word_groups,
                filter_words,
            )
        except Exception as e:
            print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return None

    def _prepare_current_title_info(self, results: Dict, time_info: str) -> Dict:
        """ä»å½“å‰æŠ“å–ç»“æœæ„å»ºæ ‡é¢˜ä¿¡æ¯"""
        title_info = {}
        for source_id, titles_data in results.items():
            title_info[source_id] = {}
            for title, title_data in titles_data.items():
                ranks = title_data.get("ranks", [])
                url = title_data.get("url", "")
                mobile_url = title_data.get("mobileUrl", "")

                title_info[source_id][title] = {
                    "first_time": time_info,
                    "last_time": time_info,
                    "count": 1,
                    "ranks": ranks,
                    "url": url,
                    "mobileUrl": mobile_url,
                }
        return title_info

    def _run_analysis_pipeline(
        self,
        data_source: Dict,
        mode: str,
        title_info: Dict,
        new_titles: Dict,
        word_groups: List[Dict],
        filter_words: List[str],
        id_to_name: Dict,
        failed_ids: Optional[List] = None,
        is_daily_summary: bool = False,
    ) -> Tuple[List[Dict], str]:
        """ç»Ÿä¸€çš„åˆ†ææµæ°´çº¿ï¼šæ•°æ®å¤„ç† â†’ ç»Ÿè®¡è®¡ç®— â†’ HTMLç”Ÿæˆ"""

        # ç»Ÿè®¡è®¡ç®—
        stats, total_titles = count_word_frequency(
            data_source,
            word_groups,
            filter_words,
            id_to_name,
            title_info,
            self.rank_threshold,
            new_titles,
            mode=mode,
        )

        # HTMLç”Ÿæˆ
        html_file = generate_html_report(
            stats,
            total_titles,
            failed_ids=failed_ids,
            new_titles=new_titles,
            id_to_name=id_to_name,
            mode=mode,
            is_daily_summary=is_daily_summary,
        )

        return stats, html_file

    def _send_notification_if_needed(
        self,
        stats: List[Dict],
        report_type: str,
        mode: str,
        failed_ids: Optional[List] = None,
        new_titles: Optional[Dict] = None,
        id_to_name: Optional[Dict] = None,
    ) -> bool:
        """ç»Ÿä¸€çš„é€šçŸ¥å‘é€é€»è¾‘ï¼ŒåŒ…å«æ‰€æœ‰åˆ¤æ–­æ¡ä»¶"""
        has_webhook = self._has_webhook_configured()

        if (
            CONFIG["ENABLE_NOTIFICATION"]
            and has_webhook
            and self._has_valid_content(stats, new_titles)
        ):
            send_to_webhooks(
                stats,
                failed_ids or [],
                report_type,
                new_titles,
                id_to_name,
                self.update_info,
                self.proxy_url,
                mode=mode,
            )
            return True
        elif CONFIG["ENABLE_NOTIFICATION"] and not has_webhook:
            print("âš ï¸ è­¦å‘Šï¼šé€šçŸ¥åŠŸèƒ½å·²å¯ç”¨ä½†æœªé…ç½®webhook URLï¼Œå°†è·³è¿‡é€šçŸ¥å‘é€")
        elif not CONFIG["ENABLE_NOTIFICATION"]:
            print(f"è·³è¿‡{report_type}é€šçŸ¥ï¼šé€šçŸ¥åŠŸèƒ½å·²ç¦ç”¨")
        elif (
            CONFIG["ENABLE_NOTIFICATION"]
            and has_webhook
            and not self._has_valid_content(stats, new_titles)
        ):
            mode_strategy = self._get_mode_strategy()
            if "å®æ—¶" in report_type:
                print(
                    f"è·³è¿‡å®æ—¶æ¨é€é€šçŸ¥ï¼š{mode_strategy['mode_name']}ä¸‹æœªæ£€æµ‹åˆ°åŒ¹é…çš„æ–°é—»"
                )
            else:
                print(
                    f"è·³è¿‡{mode_strategy['summary_report_type']}é€šçŸ¥ï¼šæœªåŒ¹é…åˆ°æœ‰æ•ˆçš„æ–°é—»å†…å®¹"
                )

        return False

    def _generate_summary_report(self, mode_strategy: Dict) -> Optional[str]:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šï¼ˆå¸¦é€šçŸ¥ï¼‰"""
        summary_type = (
            "å½“å‰æ¦œå•æ±‡æ€»" if mode_strategy["summary_mode"] == "current" else "å½“æ—¥æ±‡æ€»"
        )
        print(f"ç”Ÿæˆ{summary_type}æŠ¥å‘Š...")

        # åŠ è½½åˆ†ææ•°æ®
        analysis_data = self._load_analysis_data()
        if not analysis_data:
            return None

        all_results, id_to_name, title_info, new_titles, word_groups, filter_words = (
            analysis_data
        )

        # è¿è¡Œåˆ†ææµæ°´çº¿
        stats, html_file = self._run_analysis_pipeline(
            all_results,
            mode_strategy["summary_mode"],
            title_info,
            new_titles,
            word_groups,
            filter_words,
            id_to_name,
            is_daily_summary=True,
        )

        print(f"{summary_type}æŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")

        # å‘é€é€šçŸ¥
        self._send_notification_if_needed(
            stats,
            mode_strategy["summary_report_type"],
            mode_strategy["summary_mode"],
            new_titles=new_titles,
            id_to_name=id_to_name,
        )

        return html_file

    def _generate_summary_html(self, mode: str = "daily") -> Optional[str]:
        """ç”Ÿæˆæ±‡æ€»HTML"""
        summary_type = "å½“å‰æ¦œå•æ±‡æ€»" if mode == "current" else "å½“æ—¥æ±‡æ€»"
        print(f"ç”Ÿæˆ{summary_type}HTML...")

        # åŠ è½½åˆ†ææ•°æ®
        analysis_data = self._load_analysis_data()
        if not analysis_data:
            return None

        all_results, id_to_name, title_info, new_titles, word_groups, filter_words = (
            analysis_data
        )

        # è¿è¡Œåˆ†ææµæ°´çº¿
        _, html_file = self._run_analysis_pipeline(
            all_results,
            mode,
            title_info,
            new_titles,
            word_groups,
            filter_words,
            id_to_name,
            is_daily_summary=True,
        )

        print(f"{summary_type}HTMLå·²ç”Ÿæˆ: {html_file}")
        return html_file

    def _initialize_and_check_config(self) -> None:
        """é€šç”¨åˆå§‹åŒ–å’Œé…ç½®æ£€æŸ¥"""
        now = get_beijing_time()
        print(f"å½“å‰åŒ—äº¬æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')}")

        if not CONFIG["ENABLE_CRAWLER"]:
            print("çˆ¬è™«åŠŸèƒ½å·²ç¦ç”¨ï¼ˆENABLE_CRAWLER=Falseï¼‰ï¼Œç¨‹åºé€€å‡º")
            return

        has_webhook = self._has_webhook_configured()
        if not CONFIG["ENABLE_NOTIFICATION"]:
            print("é€šçŸ¥åŠŸèƒ½å·²ç¦ç”¨ï¼ˆENABLE_NOTIFICATION=Falseï¼‰ï¼Œå°†åªè¿›è¡Œæ•°æ®æŠ“å–")
        elif not has_webhook:
            print("æœªé…ç½®ä»»ä½•webhook URLï¼Œå°†åªè¿›è¡Œæ•°æ®æŠ“å–ï¼Œä¸å‘é€é€šçŸ¥")
        else:
            print("é€šçŸ¥åŠŸèƒ½å·²å¯ç”¨ï¼Œå°†å‘é€webhooké€šçŸ¥")

        mode_strategy = self._get_mode_strategy()
        print(f"æŠ¥å‘Šæ¨¡å¼: {self.report_mode}")
        print(f"è¿è¡Œæ¨¡å¼: {mode_strategy['description']}")

    def _crawl_data(self) -> Tuple[Dict, Dict, List]:
        """æ‰§è¡Œæ•°æ®çˆ¬å–"""
        ids = []
        for platform in CONFIG["PLATFORMS"]:
            if "name" in platform:
                ids.append((platform["id"], platform["name"]))
            else:
                ids.append(platform["id"])

        print(
            f"é…ç½®çš„ç›‘æ§å¹³å°: {[p.get('name', p['id']) for p in CONFIG['PLATFORMS']]}"
        )
        print(f"å¼€å§‹çˆ¬å–æ•°æ®ï¼Œè¯·æ±‚é—´éš” {self.request_interval} æ¯«ç§’")
        ensure_directory_exists("output")

        results, id_to_name, failed_ids = self.data_fetcher.crawl_websites(
            ids, self.request_interval
        )

        title_file = save_titles_to_file(results, id_to_name, failed_ids)
        print(f"æ ‡é¢˜å·²ä¿å­˜åˆ°: {title_file}")

        return results, id_to_name, failed_ids

    def _execute_mode_strategy(
        self, mode_strategy: Dict, results: Dict, id_to_name: Dict, failed_ids: List
    ) -> Optional[str]:
        """æ‰§è¡Œæ¨¡å¼ç‰¹å®šé€»è¾‘"""
        # è·å–å½“å‰ç›‘æ§å¹³å°IDåˆ—è¡¨
        current_platform_ids = [platform["id"] for platform in CONFIG["PLATFORMS"]]

        new_titles = detect_latest_new_titles(current_platform_ids)
        time_info = Path(save_titles_to_file(results, id_to_name, failed_ids)).stem
        word_groups, filter_words = load_frequency_words()

        # currentæ¨¡å¼ä¸‹ï¼Œå®æ—¶æ¨é€éœ€è¦ä½¿ç”¨å®Œæ•´çš„å†å²æ•°æ®æ¥ä¿è¯ç»Ÿè®¡ä¿¡æ¯çš„å®Œæ•´æ€§
        if self.report_mode == "current":
            # åŠ è½½å®Œæ•´çš„å†å²æ•°æ®ï¼ˆå·²æŒ‰å½“å‰å¹³å°è¿‡æ»¤ï¼‰
            analysis_data = self._load_analysis_data()
            if analysis_data:
                (
                    all_results,
                    historical_id_to_name,
                    historical_title_info,
                    historical_new_titles,
                    _,
                    _,
                ) = analysis_data

                print(
                    f"currentæ¨¡å¼ï¼šä½¿ç”¨è¿‡æ»¤åçš„å†å²æ•°æ®ï¼ŒåŒ…å«å¹³å°ï¼š{list(all_results.keys())}"
                )

                stats, html_file = self._run_analysis_pipeline(
                    all_results,
                    self.report_mode,
                    historical_title_info,
                    historical_new_titles,
                    word_groups,
                    filter_words,
                    historical_id_to_name,
                    failed_ids=failed_ids,
                )

                combined_id_to_name = {**historical_id_to_name, **id_to_name}

                print(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")

                # å‘é€å®æ—¶é€šçŸ¥ï¼ˆä½¿ç”¨å®Œæ•´å†å²æ•°æ®çš„ç»Ÿè®¡ç»“æœï¼‰
                summary_html = None
                if mode_strategy["should_send_realtime"]:
                    self._send_notification_if_needed(
                        stats,
                        mode_strategy["realtime_report_type"],
                        self.report_mode,
                        failed_ids=failed_ids,
                        new_titles=historical_new_titles,
                        id_to_name=combined_id_to_name,
                    )
            else:
                print("âŒ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•è¯»å–åˆšä¿å­˜çš„æ•°æ®æ–‡ä»¶")
                raise RuntimeError("æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥ï¼šä¿å­˜åç«‹å³è¯»å–å¤±è´¥")
        else:
            title_info = self._prepare_current_title_info(results, time_info)
            stats, html_file = self._run_analysis_pipeline(
                results,
                self.report_mode,
                title_info,
                new_titles,
                word_groups,
                filter_words,
                id_to_name,
                failed_ids=failed_ids,
            )
            print(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")

            # å‘é€å®æ—¶é€šçŸ¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
            summary_html = None
            if mode_strategy["should_send_realtime"]:
                self._send_notification_if_needed(
                    stats,
                    mode_strategy["realtime_report_type"],
                    self.report_mode,
                    failed_ids=failed_ids,
                    new_titles=new_titles,
                    id_to_name=id_to_name,
                )

        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šï¼ˆå¦‚æœéœ€è¦ï¼‰
        summary_html = None
        if mode_strategy["should_generate_summary"]:
            if mode_strategy["should_send_realtime"]:
                # å¦‚æœå·²ç»å‘é€äº†å®æ—¶é€šçŸ¥ï¼Œæ±‡æ€»åªç”ŸæˆHTMLä¸å‘é€é€šçŸ¥
                summary_html = self._generate_summary_html(
                    mode_strategy["summary_mode"]
                )
            else:
                # dailyæ¨¡å¼ï¼šç›´æ¥ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šå¹¶å‘é€é€šçŸ¥
                summary_html = self._generate_summary_report(mode_strategy)

        # æ‰“å¼€æµè§ˆå™¨ï¼ˆä»…åœ¨éå®¹å™¨ç¯å¢ƒï¼‰
        if self._should_open_browser() and html_file:
            if summary_html:
                summary_url = "file://" + str(Path(summary_html).resolve())
                print(f"æ­£åœ¨æ‰“å¼€æ±‡æ€»æŠ¥å‘Š: {summary_url}")
                webbrowser.open(summary_url)
            else:
                file_url = "file://" + str(Path(html_file).resolve())
                print(f"æ­£åœ¨æ‰“å¼€HTMLæŠ¥å‘Š: {file_url}")
                webbrowser.open(file_url)
        elif self.is_docker_container and html_file:
            if summary_html:
                print(f"æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆï¼ˆDockerç¯å¢ƒï¼‰: {summary_html}")
            else:
                print(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆï¼ˆDockerç¯å¢ƒï¼‰: {html_file}")

        return summary_html

    def run(self) -> None:
        """æ‰§è¡Œåˆ†ææµç¨‹"""
        try:
            self._initialize_and_check_config()

            mode_strategy = self._get_mode_strategy()

            results, id_to_name, failed_ids = self._crawl_data()

            self._execute_mode_strategy(mode_strategy, results, id_to_name, failed_ids)

        except Exception as e:
            print(f"åˆ†ææµç¨‹æ‰§è¡Œå‡ºé”™: {e}")
            raise


def main():
    try:
        analyzer = NewsAnalyzer()
        analyzer.run()
    except FileNotFoundError as e:
        print(f"âŒ é…ç½®æ–‡ä»¶é”™è¯¯: {e}")
        print("\nè¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:")
        print("  â€¢ config/config.yaml")
        print("  â€¢ config/frequency_words.txt")
        print("\nå‚è€ƒé¡¹ç›®æ–‡æ¡£è¿›è¡Œæ­£ç¡®é…ç½®")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œé”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main()
