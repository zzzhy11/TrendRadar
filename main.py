# coding=utf-8

import json
import os
import time
import random
from datetime import datetime
import webbrowser
from typing import Dict, List, Tuple, Optional, Union

import requests
import pytz

# é…ç½®å¸¸é‡
CONFIG = {
    "FEISHU_SEPARATOR": "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",  # é£ä¹¦æ¶ˆæ¯ä¸­ï¼Œæ¯ä¸ªé¢‘ç‡è¯ä¹‹é—´çš„åˆ†å‰²çº¿ï¼Œæ³¨æ„ï¼Œå…¶å®ƒç±»å‹çš„åˆ†å‰²çº¿å¯èƒ½ä¼šè¢«é£ä¹¦è¿‡æ»¤è€Œä¸æ˜¾ç¤º
    "REQUEST_INTERVAL": 1000,  # æ¯«ç§’
    "FEISHU_REPORT_TYPE": "daily",  # å¯é€‰: "current", "daily", "both"
    "RANK_THRESHOLD": 5,  # æ’åé˜ˆå€¼ï¼Œå‰5åä½¿ç”¨çº¢è‰²åŠ ç²—æ˜¾ç¤º
    "USE_PROXY": True,  # æ˜¯å¦å¯ç”¨æœ¬åœ°ä»£ç†
    "DEFAULT_PROXY": "http://127.0.0.1:10086",
    "CONTINUE_WITHOUT_FEISHU": True,  # æ§åˆ¶æ˜¯å¦åœ¨æ²¡æœ‰é£ä¹¦webhook URLæ—¶ç»§ç»­æ‰§è¡Œçˆ¬è™«, å¦‚æœTrue ,ä¼šä¾ç„¶è¿›è¡Œçˆ¬è™«è¡Œä¸ºï¼Œä¼šåœ¨githubä¸ŠæŒç»­çš„ç”Ÿæˆçˆ¬å–çš„æ–°é—»æ•°æ®
    "FEISHU_WEBHOOK_URL": "",  # é£ä¹¦æœºå™¨äººçš„webhook URLï¼Œå¤§æ¦‚é•¿è¿™æ ·ï¼šhttps://www.feishu.cn/flow/api/trigger-webhook/xxxxï¼Œ é»˜è®¤ä¸ºç©ºï¼Œæ¨èé€šè¿‡GitHub Secretsè®¾ç½®
}


class TimeHelper:
    """æ—¶é—´ç›¸å…³çš„è¾…åŠ©åŠŸèƒ½"""

    @staticmethod
    def get_beijing_time() -> datetime:
        """è·å–åŒ—äº¬æ—¶é—´"""
        return datetime.now(pytz.timezone("Asia/Shanghai"))

    @staticmethod
    def format_date_folder() -> str:
        """è¿”å›æ—¥æœŸæ–‡ä»¶å¤¹åç§°æ ¼å¼"""
        return TimeHelper.get_beijing_time().strftime("%Yå¹´%mæœˆ%dæ—¥")

    @staticmethod
    def format_time_filename() -> str:
        """è¿”å›æ—¶é—´æ–‡ä»¶åæ ¼å¼"""
        return TimeHelper.get_beijing_time().strftime("%Hæ—¶%Måˆ†")


class FileHelper:
    """æ–‡ä»¶æ“ä½œç›¸å…³çš„è¾…åŠ©åŠŸèƒ½"""

    @staticmethod
    def ensure_directory_exists(directory: str) -> None:
        """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        if not os.path.exists(directory):
            os.makedirs(directory)

    @staticmethod
    def get_output_path(subfolder: str, filename: str) -> str:
        """è·å–è¾“å‡ºæ–‡ä»¶è·¯å¾„"""
        date_folder = TimeHelper.format_date_folder()
        output_dir = os.path.join("output", date_folder, subfolder)
        FileHelper.ensure_directory_exists(output_dir)
        return os.path.join(output_dir, filename)


class DataFetcher:
    """æ•°æ®è·å–ç›¸å…³åŠŸèƒ½"""

    def __init__(self, proxy_url: Optional[str] = None):
        self.proxy_url = proxy_url

    def fetch_data(
        self,
        id_info: Union[str, Tuple[str, str]],
        max_retries: int = 2,
        min_retry_wait: int = 3,
        max_retry_wait: int = 5,
    ) -> Tuple[Optional[str], str, str]:
        """
        åŒæ­¥è·å–æŒ‡å®šIDçš„æ•°æ®ï¼Œå¤±è´¥æ—¶è¿›è¡Œé‡è¯•
        æ¥å—'success'å’Œ'cache'ä¸¤ç§çŠ¶æ€ï¼Œå…¶ä»–çŠ¶æ€æ‰ä¼šè§¦å‘é‡è¯•

        Args:
            id_info: IDä¿¡æ¯ï¼Œå¯ä»¥æ˜¯IDå­—ç¬¦ä¸²æˆ–(ID, åˆ«å)å…ƒç»„
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            min_retry_wait: æœ€å°é‡è¯•ç­‰å¾…æ—¶é—´(ç§’)
            max_retry_wait: æœ€å¤§é‡è¯•ç­‰å¾…æ—¶é—´(ç§’)

        Returns:
            (å“åº”æ•°æ®, ID, åˆ«å)å…ƒç»„ï¼Œå¦‚æœè¯·æ±‚å¤±è´¥åˆ™å“åº”æ•°æ®ä¸ºNone
        """
        # å¤„ç†IDå’Œåˆ«å
        if isinstance(id_info, tuple):
            id_value, alias = id_info
        else:
            id_value = id_info
            alias = id_value

        url = f"https://newsnow.busiyi.world/api/s?id={id_value}&latest"

        # è®¾ç½®ä»£ç†
        proxies = None
        if self.proxy_url:
            proxies = {"http": self.proxy_url, "https": self.proxy_url}

        # æ·»åŠ éšæœºæ€§æ¨¡æ‹ŸçœŸå®ç”¨æˆ·
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
            "Cache-Control": "no-cache",
        }

        retries = 0
        while retries <= max_retries:
            try:
                print(
                    f"æ­£åœ¨è¯·æ±‚ {id_value} æ•°æ®... (å°è¯• {retries + 1}/{max_retries + 1})"
                )
                response = requests.get(
                    url, proxies=proxies, headers=headers, timeout=10
                )
                response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç 

                # è§£æJSONå¹¶æ£€æŸ¥å“åº”çŠ¶æ€
                data_text = response.text
                data_json = json.loads(data_text)

                # ä¿®æ”¹çŠ¶æ€æ£€æŸ¥é€»è¾‘ï¼šæ¥å—successå’Œcacheä¸¤ç§çŠ¶æ€
                status = data_json.get("status", "æœªçŸ¥")
                if status not in ["success", "cache"]:
                    raise ValueError(f"å“åº”çŠ¶æ€å¼‚å¸¸: {status}")

                # è®°å½•çŠ¶æ€ä¿¡æ¯
                status_info = "æœ€æ–°æ•°æ®" if status == "success" else "ç¼“å­˜æ•°æ®"
                print(f"æˆåŠŸè·å– {id_value} æ•°æ®ï¼ˆ{status_info}ï¼‰")
                return data_text, id_value, alias

            except Exception as e:
                retries += 1
                if retries <= max_retries:
                    # è®¡ç®—é‡è¯•ç­‰å¾…æ—¶é—´ï¼šåŸºç¡€3-5ç§’ï¼Œæ¯æ¬¡é‡è¯•å¢åŠ 1-2ç§’
                    base_wait = random.uniform(min_retry_wait, max_retry_wait)
                    additional_wait = (retries - 1) * random.uniform(1, 2)
                    wait_time = base_wait + additional_wait

                    print(
                        f"è¯·æ±‚ {id_value} å¤±è´¥: {e}. å°†åœ¨ {wait_time:.2f} ç§’åé‡è¯•..."
                    )
                    time.sleep(wait_time)
                else:
                    print(f"è¯·æ±‚ {id_value} å¤±è´¥: {e}. å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ã€‚")
                    return None, id_value, alias
        return None, id_value, alias

    def crawl_websites(
        self,
        ids_list: List[Union[str, Tuple[str, str]]],
        request_interval: int = CONFIG["REQUEST_INTERVAL"],
    ) -> Tuple[Dict, Dict, List]:
        """
        çˆ¬å–å¤šä¸ªç½‘ç«™çš„æ•°æ®ï¼Œä½¿ç”¨åŒæ­¥è¯·æ±‚

        Args:
            ids_list: IDåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å¯ä»¥æ˜¯IDå­—ç¬¦ä¸²æˆ–(ID, åˆ«å)å…ƒç»„
            request_interval: è¯·æ±‚é—´éš”(æ¯«ç§’)

        Returns:
            (results, id_to_alias, failed_ids)å…ƒç»„
        """
        results = {}
        id_to_alias = {}
        failed_ids = []

        for i, id_info in enumerate(ids_list):
            # å¤„ç†IDå’Œåˆ«å
            if isinstance(id_info, tuple):
                id_value, alias = id_info
            else:
                id_value = id_info
                alias = id_value

            # æ·»åŠ åˆ°ID-åˆ«åæ˜ å°„
            id_to_alias[id_value] = alias

            # å‘é€è¯·æ±‚
            response, _, _ = self.fetch_data(id_info)

            # å¤„ç†å“åº”
            if response:
                try:
                    data = json.loads(response)
                    # è·å–æ ‡é¢˜åˆ—è¡¨ï¼ŒåŒæ—¶è®°å½•æ’å
                    results[id_value] = {}
                    for index, item in enumerate(data.get("items", []), 1):
                        title = item["title"]
                        if title in results[id_value]:
                            results[id_value][title].append(index)
                        else:
                            results[id_value][title] = [index]
                except json.JSONDecodeError:
                    print(f"è§£æ {id_value} çš„å“åº”å¤±è´¥ï¼Œä¸æ˜¯æœ‰æ•ˆçš„JSON")
                    failed_ids.append(id_value)
                except Exception as e:
                    print(f"å¤„ç† {id_value} æ•°æ®æ—¶å‡ºé”™: {e}")
                    failed_ids.append(id_value)
            else:
                failed_ids.append(id_value)

            # æ·»åŠ é—´éš”æ—¶é—´ï¼Œé™¤éæ˜¯æœ€åä¸€ä¸ªè¯·æ±‚
            if i < len(ids_list) - 1:
                # æ·»åŠ ä¸€äº›éšæœºæ€§åˆ°é—´éš”æ—¶é—´
                actual_interval = request_interval + random.randint(-10, 20)
                actual_interval = max(50, actual_interval)  # ç¡®ä¿è‡³å°‘50æ¯«ç§’
                print(f"ç­‰å¾… {actual_interval} æ¯«ç§’åå‘é€ä¸‹ä¸€ä¸ªè¯·æ±‚...")
                time.sleep(actual_interval / 1000)

        print(f"\nè¯·æ±‚æ€»ç»“:")
        print(f"- æˆåŠŸè·å–æ•°æ®çš„ID: {list(results.keys())}")
        print(f"- è¯·æ±‚å¤±è´¥çš„ID: {failed_ids}")

        return results, id_to_alias, failed_ids


class DataProcessor:
    """æ•°æ®å¤„ç†ç›¸å…³åŠŸèƒ½"""

    @staticmethod
    def save_titles_to_file(results: Dict, id_to_alias: Dict, failed_ids: List) -> str:
        """å°†æ ‡é¢˜ä¿å­˜åˆ°æ–‡ä»¶ï¼ŒåŒ…æ‹¬å¤±è´¥çš„è¯·æ±‚ä¿¡æ¯"""
        file_path = FileHelper.get_output_path(
            "txt", f"{TimeHelper.format_time_filename()}.txt"
        )

        with open(file_path, "w", encoding="utf-8") as f:
            # å…ˆå†™å…¥æˆåŠŸè·å–çš„æ•°æ®
            for id_value, title_data in results.items():
                display_name = id_to_alias.get(id_value, id_value)
                f.write(f"{display_name}\n")
                for i, (title, ranks) in enumerate(title_data.items(), 1):
                    rank_str = ",".join(map(str, ranks))
                    f.write(f"{i}. {title} (æ’å:{rank_str})\n")
                f.write("\n")

            # å¦‚æœæœ‰å¤±è´¥çš„è¯·æ±‚ï¼Œå†™å…¥å¤±è´¥ä¿¡æ¯
            if failed_ids:
                f.write("==== ä»¥ä¸‹IDè¯·æ±‚å¤±è´¥ ====\n")
                for id_value in failed_ids:
                    display_name = id_to_alias.get(id_value, id_value)
                    f.write(f"{display_name} (ID: {id_value})\n")

        return file_path

    @staticmethod
    def load_frequency_words(
        frequency_file: str = "frequency_words.txt",
    ) -> Tuple[List[List[str]], List[str]]:
        """
        åŠ è½½é¢‘ç‡è¯å’Œè¿‡æ»¤è¯ï¼Œå¤„ç†å…³è”è¯

        Returns:
            (word_groups, filter_words)å…ƒç»„
        """
        if not os.path.exists(frequency_file):
            print(f"é¢‘ç‡è¯æ–‡ä»¶ {frequency_file} ä¸å­˜åœ¨")
            return [], []

        with open(frequency_file, "r", encoding="utf-8") as f:
            content = f.read()

        # æŒ‰åŒç©ºè¡Œåˆ†å‰²ä¸åŒçš„è¯ç»„
        word_groups = [
            group.strip() for group in content.split("\n\n") if group.strip()
        ]

        # å¤„ç†æ¯ä¸ªè¯ç»„
        processed_groups = []
        filter_words = []  # ç”¨äºå­˜å‚¨è¿‡æ»¤è¯

        for group in word_groups:
            words = [word.strip() for word in group.split("\n") if word.strip()]

            # åˆ†ç¦»é¢‘ç‡è¯å’Œè¿‡æ»¤è¯
            group_frequency_words = []

            for word in words:
                if word.startswith("!"):
                    # å»æ‰æ„Ÿå¹å·ï¼Œæ·»åŠ åˆ°è¿‡æ»¤è¯åˆ—è¡¨
                    filter_words.append(word[1:])
                else:
                    # æ­£å¸¸çš„é¢‘ç‡è¯
                    group_frequency_words.append(word)

            # åªæœ‰å½“è¯ç»„ä¸­åŒ…å«é¢‘ç‡è¯æ—¶æ‰æ·»åŠ åˆ°ç»“æœä¸­
            if group_frequency_words:
                processed_groups.append(group_frequency_words)

        return processed_groups, filter_words

    @staticmethod
    def read_all_today_titles() -> Tuple[Dict, Dict, Dict]:
        """
        è¯»å–å½“å¤©æ‰€æœ‰txtæ–‡ä»¶çš„æ ‡é¢˜ï¼Œå¹¶æŒ‰æ¥æºåˆå¹¶ï¼Œå»é™¤é‡å¤ï¼Œè®°å½•æ—¶é—´å’Œå‡ºç°æ¬¡æ•°

        Returns:
            (all_results, id_to_alias, title_info)å…ƒç»„
        """
        date_folder = TimeHelper.format_date_folder()
        txt_dir = os.path.join("output", date_folder, "txt")

        if not os.path.exists(txt_dir):
            print(f"ä»Šæ—¥æ–‡ä»¶å¤¹ {txt_dir} ä¸å­˜åœ¨")
            return {}, {}, {}

        all_results = {}  # æ‰€æœ‰æºçš„æ‰€æœ‰æ ‡é¢˜ {source_id: {title: [ranks]}}
        id_to_alias = {}  # IDåˆ°åˆ«åçš„æ˜ å°„
        title_info = (
            {}
        )  # æ ‡é¢˜ä¿¡æ¯ {source_id: {title: {"first_time": é¦–æ¬¡æ—¶é—´, "last_time": æœ€åæ—¶é—´, "count": å‡ºç°æ¬¡æ•°, "ranks": [æ’ååˆ—è¡¨]}}}

        # è¯»å–æ‰€æœ‰txtæ–‡ä»¶ï¼ŒæŒ‰æ—¶é—´æ’åºç¡®ä¿æ—©çš„æ—¶é—´ä¼˜å…ˆå¤„ç†
        files = sorted([f for f in os.listdir(txt_dir) if f.endswith(".txt")])

        for file in files:
            # ä»æ–‡ä»¶åæå–æ—¶é—´ä¿¡æ¯ (ä¾‹å¦‚ "12æ—¶34åˆ†.txt")
            time_info = file.replace(".txt", "")

            file_path = os.path.join(txt_dir, file)
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

                # è§£æå†…å®¹
                sections = content.split("\n\n")
                for section in sections:
                    if not section.strip() or "==== ä»¥ä¸‹IDè¯·æ±‚å¤±è´¥ ====" in section:
                        continue

                    lines = section.strip().split("\n")
                    if len(lines) < 2:
                        continue

                    # ç¬¬ä¸€è¡Œæ˜¯æ¥æºå
                    source_name = lines[0].strip()

                    # æå–æ ‡é¢˜å’Œæ’å
                    title_ranks = {}
                    for line in lines[1:]:
                        if line.strip():
                            try:
                                # æå–åºå·å’Œæ­£æ–‡éƒ¨åˆ†
                                match_num = None
                                title_part = line.strip()

                                # å¤„ç†æ ¼å¼ "æ•°å­—. æ ‡é¢˜"
                                if (
                                    ". " in title_part
                                    and title_part.split(". ")[0].isdigit()
                                ):
                                    parts = title_part.split(". ", 1)
                                    match_num = int(parts[0])  # åºå·å¯èƒ½æ˜¯æ’å
                                    title_part = parts[1]

                                # æå–æ’åä¿¡æ¯ "æ ‡é¢˜ (æ’å:1,2,3)"
                                ranks = []
                                if " (æ’å:" in title_part:
                                    title, rank_str = title_part.rsplit(" (æ’å:", 1)
                                    rank_str = rank_str.rstrip(")")
                                    ranks = [
                                        int(r)
                                        for r in rank_str.split(",")
                                        if r.strip() and r.isdigit()
                                    ]
                                else:
                                    title = title_part

                                # å¦‚æœæ²¡æ‰¾åˆ°æ’åä½†æœ‰åºå·ï¼Œåˆ™ä½¿ç”¨åºå·
                                if not ranks and match_num is not None:
                                    ranks = [match_num]

                                # ç¡®ä¿æ’ååˆ—è¡¨ä¸ä¸ºç©º
                                if not ranks:
                                    ranks = [99]  # é»˜è®¤æ’å

                                title_ranks[title] = ranks

                            except Exception as e:
                                print(f"è§£ææ ‡é¢˜è¡Œå‡ºé”™: {line}, é”™è¯¯: {e}")

                    # å¤„ç†æ¥æºæ•°æ®
                    DataProcessor._process_source_data(
                        source_name,
                        title_ranks,
                        time_info,
                        all_results,
                        title_info,
                        id_to_alias,
                    )

        # å°†ç»“æœä» {source_name: {title: [ranks]}} è½¬æ¢ä¸º {source_id: {title: [ranks]}}
        id_results = {}
        id_title_info = {}
        for name, titles in all_results.items():
            for id_value, alias in id_to_alias.items():
                if alias == name:
                    id_results[id_value] = titles
                    id_title_info[id_value] = title_info[name]
                    break

        return id_results, id_to_alias, id_title_info

    @staticmethod
    def _process_source_data(
        source_name: str,
        title_ranks: Dict,
        time_info: str,
        all_results: Dict,
        title_info: Dict,
        id_to_alias: Dict,
    ) -> None:
        """å¤„ç†æ¥æºæ•°æ®ï¼Œæ›´æ–°ç»“æœå’Œæ ‡é¢˜ä¿¡æ¯"""
        if source_name not in all_results:
            # é¦–æ¬¡é‡åˆ°æ­¤æ¥æº
            all_results[source_name] = title_ranks

            # åˆå§‹åŒ–æ ‡é¢˜ä¿¡æ¯
            if source_name not in title_info:
                title_info[source_name] = {}

            # è®°å½•æ¯ä¸ªæ ‡é¢˜çš„æ—¶é—´ã€æ¬¡æ•°å’Œæ’å
            for title, ranks in title_ranks.items():
                title_info[source_name][title] = {
                    "first_time": time_info,  # è®°å½•é¦–æ¬¡æ—¶é—´
                    "last_time": time_info,  # æœ€åæ—¶é—´åˆå§‹åŒé¦–æ¬¡æ—¶é—´
                    "count": 1,
                    "ranks": ranks,
                }

            # å°è¯•åå‘ç”ŸæˆID
            reversed_id = source_name.lower().replace(" ", "-")
            id_to_alias[reversed_id] = source_name
        else:
            # å·²æœ‰æ­¤æ¥æºï¼Œæ›´æ–°æ ‡é¢˜
            for title, ranks in title_ranks.items():
                if title not in all_results[source_name]:
                    all_results[source_name][title] = ranks
                    title_info[source_name][title] = {
                        "first_time": time_info,  # æ–°æ ‡é¢˜çš„é¦–æ¬¡å’Œæœ€åæ—¶é—´éƒ½è®¾ä¸ºå½“å‰
                        "last_time": time_info,
                        "count": 1,
                        "ranks": ranks,
                    }
                else:
                    # å·²å­˜åœ¨çš„æ ‡é¢˜ï¼Œæ›´æ–°æœ€åæ—¶é—´ï¼Œåˆå¹¶æ’åä¿¡æ¯å¹¶å¢åŠ è®¡æ•°
                    existing_ranks = title_info[source_name][title]["ranks"]
                    merged_ranks = existing_ranks.copy()
                    for rank in ranks:
                        if rank not in merged_ranks:
                            merged_ranks.append(rank)

                    title_info[source_name][title][
                        "last_time"
                    ] = time_info  # æ›´æ–°æœ€åæ—¶é—´
                    title_info[source_name][title]["ranks"] = merged_ranks
                    title_info[source_name][title]["count"] += 1


class StatisticsCalculator:
    """ç»Ÿè®¡è®¡ç®—ç›¸å…³åŠŸèƒ½"""

    @staticmethod
    def count_word_frequency(
        results: Dict,
        word_groups: List[List[str]],
        filter_words: List[str],
        id_to_alias: Dict,
        title_info: Optional[Dict] = None,
        rank_threshold: int = CONFIG["RANK_THRESHOLD"],
    ) -> Tuple[List[Dict], int]:
        """
        ç»Ÿè®¡è¯é¢‘ï¼Œå¤„ç†å…³è”è¯å’Œå¤§å°å†™ä¸æ•æ„Ÿï¼Œæ¯ä¸ªæ ‡é¢˜åªè®¡å…¥é¦–ä¸ªåŒ¹é…è¯ç»„ï¼Œå¹¶åº”ç”¨è¿‡æ»¤è¯

        Returns:
            (stats, total_titles)å…ƒç»„
        """
        word_stats = {}
        total_titles = 0
        processed_titles = {}  # ç”¨äºè·Ÿè¸ªå·²å¤„ç†æ ‡é¢˜ {source_id: {title: True}}

        # åˆå§‹åŒ–title_info
        if title_info is None:
            title_info = {}

        # ä¸ºæ¯ä¸ªè¯ç»„åˆ›å»ºç»Ÿè®¡å¯¹è±¡
        for group in word_groups:
            group_key = " ".join(group)
            word_stats[group_key] = {"count": 0, "titles": {}}

        # éå†æ‰€æœ‰æ ‡é¢˜å¹¶ç»Ÿè®¡
        for source_id, titles_data in results.items():
            total_titles += len(titles_data)

            # åˆå§‹åŒ–è¯¥æ¥æºçš„å¤„ç†è®°å½•
            if source_id not in processed_titles:
                processed_titles[source_id] = {}

            for title, source_ranks in titles_data.items():
                # è·³è¿‡å·²å¤„ç†çš„æ ‡é¢˜
                if title in processed_titles.get(source_id, {}):
                    continue

                title_lower = title.lower()  # è½¬æ¢ä¸ºå°å†™ä»¥å®ç°å¤§å°å†™ä¸æ•æ„Ÿ

                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•è¿‡æ»¤è¯
                contains_filter_word = any(
                    filter_word.lower() in title_lower for filter_word in filter_words
                )

                # å¦‚æœåŒ…å«è¿‡æ»¤è¯ï¼Œè·³è¿‡è¿™ä¸ªæ ‡é¢˜
                if contains_filter_word:
                    continue

                # æŒ‰é¡ºåºæ£€æŸ¥æ¯ä¸ªè¯ç»„
                for group in word_groups:
                    group_key = " ".join(group)

                    # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä¸€ä¸ªè¯åœ¨æ ‡é¢˜ä¸­
                    matched = any(word.lower() in title_lower for word in group)

                    # å¦‚æœåŒ¹é…ï¼Œå¢åŠ è®¡æ•°å¹¶æ·»åŠ æ ‡é¢˜ï¼Œç„¶åæ ‡è®°ä¸ºå·²å¤„ç†
                    if matched:
                        word_stats[group_key]["count"] += 1
                        if source_id not in word_stats[group_key]["titles"]:
                            word_stats[group_key]["titles"][source_id] = []

                        # è·å–æ ‡é¢˜ä¿¡æ¯
                        first_time = ""
                        last_time = ""
                        count_info = 1
                        ranks = source_ranks if source_ranks else []

                        if (
                            title_info
                            and source_id in title_info
                            and title in title_info[source_id]
                        ):
                            info = title_info[source_id][title]
                            first_time = info.get("first_time", "")
                            last_time = info.get("last_time", "")
                            count_info = info.get("count", 1)
                            if "ranks" in info and info["ranks"]:
                                ranks = info["ranks"]

                        # ç¡®ä¿æ’åæ˜¯æœ‰æ•ˆçš„
                        if not ranks:
                            ranks = [99]  # ä½¿ç”¨é»˜è®¤æ’å

                        # æ ¼å¼åŒ–æ—¶é—´ä¿¡æ¯
                        time_display = StatisticsCalculator._format_time_display(
                            first_time, last_time
                        )

                        # æ·»åŠ å¸¦å®Œæ•´ä¿¡æ¯çš„æ ‡é¢˜æ•°æ®ï¼Œä¿å­˜åŸå§‹æ•°æ®ç”¨äºåç»­æ ¼å¼åŒ–
                        source_alias = id_to_alias.get(source_id, source_id)
                        word_stats[group_key]["titles"][source_id].append(
                            {
                                "title": title,
                                "source_alias": source_alias,
                                "first_time": first_time,
                                "last_time": last_time,
                                "time_display": time_display,
                                "count": count_info,
                                "ranks": ranks,
                                "rank_threshold": rank_threshold,
                            }
                        )

                        # æ ‡è®°è¯¥æ ‡é¢˜å·²å¤„ç†ï¼Œä¸å†åŒ¹é…å…¶ä»–è¯ç»„
                        if source_id not in processed_titles:
                            processed_titles[source_id] = {}
                        processed_titles[source_id][title] = True
                        break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„è¯ç»„åé€€å‡ºå¾ªç¯

        # è½¬æ¢ç»Ÿè®¡ç»“æœ - è¿™é‡Œä¸å†è¿›è¡Œæ ¼å¼åŒ–ï¼Œä¿ç•™åŸå§‹æ•°æ®
        stats = []
        for group_key, data in word_stats.items():
            all_titles = []
            for source_id, title_list in data["titles"].items():
                all_titles.extend(title_list)

            stats.append(
                {
                    "word": group_key,
                    "count": data["count"],
                    "titles": all_titles,  # ä¿å­˜åŸå§‹æ ‡é¢˜æ•°æ®ï¼Œç”¨äºåç»­æ ¼å¼åŒ–
                    "percentage": (
                        round(data["count"] / total_titles * 100, 2)
                        if total_titles > 0
                        else 0
                    ),
                }
            )

        # æŒ‰å‡ºç°æ¬¡æ•°ä»é«˜åˆ°ä½æ’åº
        stats.sort(key=lambda x: x["count"], reverse=True)

        return stats, total_titles

    @staticmethod
    def _format_rank_for_html(ranks: List[int], rank_threshold: int = 5) -> str:
        """æ ¼å¼åŒ–æ’åæ˜¾ç¤ºç”¨äºHTMLï¼Œå‰5åä½¿ç”¨çº¢è‰²ç²—ä½“"""
        if not ranks:
            return ""

        # æ’åºæ’åå¹¶ç¡®ä¿ä¸é‡å¤
        unique_ranks = sorted(set(ranks))
        min_rank = unique_ranks[0]
        max_rank = unique_ranks[-1]

        # æ‰€æœ‰æ’åéƒ½ä½¿ç”¨[]ï¼Œåªæœ‰å‰5åæ˜¾ç¤ºçº¢è‰²ç²—ä½“
        if min_rank <= rank_threshold:
            if min_rank == max_rank:
                # å•ä¸€æ’åä¸”åœ¨å‰5
                return f"<font color='red'><strong>[{min_rank}]</strong></font>"
            else:
                return f"<font color='red'><strong>[{min_rank} - {max_rank}]</strong></font>"
        else:
            # æ’ååœ¨5åä¹‹åï¼Œä½¿ç”¨æ™®é€šæ˜¾ç¤º
            if min_rank == max_rank:
                return f"[{min_rank}]"
            else:
                return f"[{min_rank} - {max_rank}]"

    @staticmethod
    def _format_rank_for_feishu(ranks: List[int], rank_threshold: int = 5) -> str:
        """æ ¼å¼åŒ–æ’åæ˜¾ç¤ºç”¨äºé£ä¹¦ï¼Œå‰5åä½¿ç”¨çº¢è‰²ç²—ä½“markdownæ ¼å¼"""
        if not ranks:
            return ""

        # æ’åºæ’åå¹¶ç¡®ä¿ä¸é‡å¤
        unique_ranks = sorted(set(ranks))
        min_rank = unique_ranks[0]
        max_rank = unique_ranks[-1]

        # æ‰€æœ‰æ’åéƒ½ä½¿ç”¨[]ï¼Œåªæœ‰å‰5åæ˜¾ç¤ºçº¢è‰²
        if min_rank <= rank_threshold:
            if min_rank == max_rank:
                # å•ä¸€æ’åä¸”åœ¨å‰5
                return f"<font color='red'>**[{min_rank}]**</font>"
            else:
                return f"<font color='red'>**[{min_rank} - {max_rank}]**</font>"
        else:
            # æ’ååœ¨5åä¹‹åï¼Œä½¿ç”¨æ™®é€šæ˜¾ç¤º
            if min_rank == max_rank:
                return f"[{min_rank}]"
            else:
                return f"[{min_rank} - {max_rank}]"

    @staticmethod
    def _format_time_display(first_time: str, last_time: str) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤ºï¼Œå•æ¬¡æ˜¾ç¤ºæ—¶é—´ï¼Œå¤šæ¬¡æ˜¾ç¤ºæ—¶é—´èŒƒå›´"""
        if not first_time:
            return ""

        if first_time == last_time or not last_time:
            # åªæœ‰ä¸€ä¸ªæ—¶é—´ç‚¹ï¼Œç›´æ¥æ˜¾ç¤º
            return first_time
        else:
            # æœ‰ä¸¤ä¸ªæ—¶é—´ç‚¹ï¼Œæ˜¾ç¤ºèŒƒå›´
            return f"[{first_time} ~ {last_time}]"


class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆç›¸å…³åŠŸèƒ½"""

    @staticmethod
    def generate_html_report(
        stats: List[Dict],
        total_titles: int,
        failed_ids: Optional[List] = None,
        is_daily: bool = False,
    ) -> str:
        """
        ç”ŸæˆHTMLæŠ¥å‘Šï¼ŒåŒ…æ‹¬å¤±è´¥çš„è¯·æ±‚ä¿¡æ¯

        Returns:
            HTMLæ–‡ä»¶è·¯å¾„
        """
        # åˆ›å»ºæ–‡ä»¶è·¯å¾„
        if is_daily:
            filename = "å½“æ—¥ç»Ÿè®¡.html"
        else:
            filename = f"{TimeHelper.format_time_filename()}.html"

        file_path = FileHelper.get_output_path("html", filename)

        # HTMLæ¨¡æ¿å’Œå†…å®¹ç”Ÿæˆ
        html_content = ReportGenerator._create_html_content(
            stats, total_titles, failed_ids, is_daily
        )

        # å†™å…¥æ–‡ä»¶
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        # å¦‚æœæ˜¯å½“æ—¥ç»Ÿè®¡ï¼Œè¿˜éœ€è¦åœ¨æ ¹ç›®å½•ä¸‹ç”Ÿæˆindex.html
        if is_daily:
            root_file_path = "index.html"
            with open(root_file_path, "w", encoding="utf-8") as f:
                f.write(html_content)
            print(
                f"å½“æ—¥ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°æ ¹ç›®å½•çš„index.html: {os.path.abspath(root_file_path)}"
            )

        return file_path

    @staticmethod
    def _create_html_content(
        stats: List[Dict],
        total_titles: int,
        failed_ids: Optional[List] = None,
        is_daily: bool = False,
    ) -> str:
        """åˆ›å»ºHTMLå†…å®¹"""
        # HTMLå¤´éƒ¨
        html = """
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>é¢‘ç‡è¯ç»Ÿè®¡æŠ¥å‘Š</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                h1, h2 { color: #333; }
                table { border-collapse: collapse; width: 100%; margin-top: 20px; }
                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                th { background-color: #f2f2f2; }
                tr:nth-child(even) { background-color: #f9f9f9; }
                .word { font-weight: bold; }
                .count { text-align: center; }
                .percentage { text-align: center; }
                .titles { max-width: 500px; }
                .source { color: #666; font-style: italic; }
                .error { color: #d9534f; }
            </style>
        </head>
        <body>
            <h1>é¢‘ç‡è¯ç»Ÿè®¡æŠ¥å‘Š</h1>
        """

        # æŠ¥å‘Šç±»å‹
        if is_daily:
            html += "<p>æŠ¥å‘Šç±»å‹: å½“æ—¥æ±‡æ€»</p>"

        # åŸºæœ¬ä¿¡æ¯
        now = TimeHelper.get_beijing_time()
        html += f"<p>æ€»æ ‡é¢˜æ•°: {total_titles}</p>"
        html += f"<p>ç”Ÿæˆæ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')}</p>"

        # å¤±è´¥çš„è¯·æ±‚ä¿¡æ¯
        if failed_ids and len(failed_ids) > 0:
            html += """
            <div class="error">
                <h2>è¯·æ±‚å¤±è´¥çš„å¹³å°</h2>
                <ul>
            """
            for id_value in failed_ids:
                html += f"<li>{id_value}</li>"
            html += """
                </ul>
            </div>
            """

        # è¡¨æ ¼å¤´éƒ¨
        html += """
            <table>
                <tr>
                    <th>æ’å</th>
                    <th>é¢‘ç‡è¯</th>
                    <th>å‡ºç°æ¬¡æ•°</th>
                    <th>å æ¯”</th>
                    <th>ç›¸å…³æ ‡é¢˜</th>
                </tr>
        """

        # è¡¨æ ¼å†…å®¹
        for i, stat in enumerate(stats, 1):
            # æ ¼å¼åŒ–æ ‡é¢˜åˆ—è¡¨ç”¨äºHTMLæ˜¾ç¤º
            formatted_titles = []
            for title_data in stat["titles"]:
                title = title_data["title"]
                source_alias = title_data["source_alias"]
                time_display = title_data["time_display"]
                count_info = title_data["count"]
                ranks = title_data["ranks"]
                rank_threshold = title_data["rank_threshold"]

                # ä½¿ç”¨HTMLæ ¼å¼åŒ–æ’å
                rank_display = StatisticsCalculator._format_rank_for_html(
                    ranks, rank_threshold
                )

                # æ ¼å¼åŒ–æ ‡é¢˜ä¿¡æ¯
                formatted_title = f"[{source_alias}] {title}"
                if rank_display:
                    formatted_title += f" {rank_display}"
                if time_display:
                    formatted_title += f" <font color='grey'>- {time_display}</font>"
                if count_info > 1:
                    formatted_title += f" <font color='green'>({count_info}æ¬¡)</font>"

                formatted_titles.append(formatted_title)

            html += f"""
                <tr>
                    <td>{i}</td>
                    <td class="word">{stat['word']}</td>
                    <td class="count">{stat['count']}</td>
                    <td class="percentage">{stat['percentage']}%</td>
                    <td class="titles">{"<br>".join(formatted_titles)}</td>
                </tr>
            """

        # è¡¨æ ¼ç»“å°¾
        html += """
            </table>
        </body>
        </html>
        """

        return html

    @staticmethod
    def send_to_feishu(
        stats: List[Dict],
        failed_ids: Optional[List] = None,
        report_type: str = "å•æ¬¡çˆ¬å–",
    ) -> bool:
        """
        å°†é¢‘ç‡è¯ç»Ÿè®¡ç»“æœå‘é€åˆ°é£ä¹¦

        Returns:
            æˆåŠŸå‘é€è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        # è·å–webhook URLï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œå…¶æ¬¡ä½¿ç”¨é…ç½®ä¸­çš„URL
        webhook_url = os.environ.get("FEISHU_WEBHOOK_URL", CONFIG["FEISHU_WEBHOOK_URL"])

        # æ£€æŸ¥webhook URLæ˜¯å¦æœ‰æ•ˆ
        if not webhook_url:
            print(f"è­¦å‘Š: FEISHU_WEBHOOK_URLæœªè®¾ç½®æˆ–æ— æ•ˆï¼Œè·³è¿‡å‘é€é£ä¹¦é€šçŸ¥")
            return False

        headers = {"Content-Type": "application/json"}

        # è·å–æ€»æ ‡é¢˜æ•°
        total_titles = sum(len(stat["titles"]) for stat in stats if stat["count"] > 0)

        # æ„å»ºæ–‡æœ¬å†…å®¹
        text_content = ReportGenerator._build_feishu_content(stats, failed_ids)

        # æ„é€ æ¶ˆæ¯ä½“
        now = TimeHelper.get_beijing_time()
        payload = {
            "msg_type": "text",
            "content": {
                "total_titles": total_titles,
                "timestamp": now.strftime("%Y-%m-%d %H:%M:%S"),
                "report_type": report_type,
                "text": text_content,
            },
        }

        # å‘é€è¯·æ±‚
        try:
            response = requests.post(webhook_url, headers=headers, json=payload)
            if response.status_code == 200:
                print(f"æ•°æ®å‘é€åˆ°é£ä¹¦æˆåŠŸ [{report_type}]")
                return True
            else:
                print(
                    f"å‘é€åˆ°é£ä¹¦å¤±è´¥ [{report_type}]ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}ï¼Œå“åº”ï¼š{response.text}"
                )
                return False
        except Exception as e:
            print(f"å‘é€åˆ°é£ä¹¦æ—¶å‡ºé”™ [{report_type}]ï¼š{e}")
            return False

    @staticmethod
    def _build_feishu_content(
        stats: List[Dict], failed_ids: Optional[List] = None
    ) -> str:
        """æ„å»ºé£ä¹¦æ¶ˆæ¯å†…å®¹ï¼Œä½¿ç”¨å¯Œæ–‡æœ¬æ ¼å¼"""
        text_content = ""

        # æ·»åŠ é¢‘ç‡è¯ç»Ÿè®¡ä¿¡æ¯
        filtered_stats = [stat for stat in stats if stat["count"] > 0]

        # å¦‚æœæœ‰ç»Ÿè®¡æ•°æ®ï¼Œæ·»åŠ æ ‡é¢˜
        if filtered_stats:
            text_content += "ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**\n\n"

        # è·å–æ€»æ•°ç”¨äºåºå·æ˜¾ç¤º
        total_count = len(filtered_stats)

        for i, stat in enumerate(filtered_stats):
            word = stat["word"]
            count = stat["count"]

            # æ„å»ºåºå·æ˜¾ç¤ºï¼Œæ ¼å¼ä¸º [å½“å‰åºå·/æ€»æ•°]ï¼Œä½¿ç”¨ç°è‰²ä¸”ä¸åŠ ç²—
            sequence_display = f"<font color='grey'>[{i + 1}/{total_count}]</font>"

            # å…³é”®è¯åŠ ç²—ï¼Œè®¡æ•°å’Œç™¾åˆ†æ¯”ä½¿ç”¨ä¸åŒé¢œè‰²ï¼Œåºå·å•ç‹¬æ˜¾ç¤ºä¸ºç°è‰²
            if count >= 10:
                # é«˜é¢‘è¯ä½¿ç”¨çº¢è‰²
                text_content += f"ğŸ”¥ {sequence_display} **{word}** : <font color='red'>{count}</font> æ¡\n\n"
            elif count >= 5:
                # ä¸­é¢‘è¯ä½¿ç”¨æ©™è‰²
                text_content += f"ğŸ“ˆ {sequence_display} **{word}** : <font color='orange'>{count}</font> æ¡\n\n"
            else:
                # ä½é¢‘è¯ä½¿ç”¨é»˜è®¤é¢œè‰²
                text_content += f"ğŸ“Œ {sequence_display} **{word}** : {count} æ¡\n\n"

            # æ ¼å¼åŒ–æ ‡é¢˜åˆ—è¡¨ç”¨äºé£ä¹¦æ˜¾ç¤º
            for j, title_data in enumerate(stat["titles"], 1):
                title = title_data["title"]
                source_alias = title_data["source_alias"]
                time_display = title_data["time_display"]
                count_info = title_data["count"]
                ranks = title_data["ranks"]
                rank_threshold = title_data["rank_threshold"]

                # ä½¿ç”¨é£ä¹¦æ ¼å¼åŒ–æ’å
                rank_display = StatisticsCalculator._format_rank_for_feishu(
                    ranks, rank_threshold
                )

                # æ ¼å¼åŒ–æ ‡é¢˜ä¿¡æ¯
                formatted_title = f"[{source_alias}] {title}"
                if rank_display:
                    formatted_title += f" {rank_display}"
                if time_display:
                    formatted_title += f" <font color='grey'>- {time_display}</font>"
                if count_info > 1:
                    formatted_title += f" <font color='green'>({count_info}æ¬¡)</font>"

                # ä½¿ç”¨ç°è‰²æ˜¾ç¤ºæ¥æº
                text_content += (
                    f"  {j}. <font color='grey'>[{source_alias}]</font> {title}"
                )
                if rank_display:
                    text_content += f" {rank_display}"
                if time_display:
                    text_content += f" <font color='grey'>- {time_display}</font>"
                if count_info > 1:
                    text_content += f" <font color='green'>({count_info}æ¬¡)</font>"
                text_content += "\n"

                # åœ¨æ¯æ¡æ–°é—»åæ·»åŠ é¢å¤–é—´éš”ï¼ˆé™¤äº†æœ€åä¸€æ¡ï¼‰
                if j < len(stat["titles"]):
                    text_content += "\n"

            # æ·»åŠ åˆ†å‰²çº¿ï¼Œä½¿ç”¨æ›´ä¼˜é›…çš„æ ·å¼
            if i < len(filtered_stats) - 1:
                text_content += f"\n{CONFIG['FEISHU_SEPARATOR']}\n\n"

        if not text_content:
            text_content = "ğŸ“­ æš‚æ— åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡\n\n"

        # æ·»åŠ å¤±è´¥å¹³å°ä¿¡æ¯
        if failed_ids and len(failed_ids) > 0:
            if text_content and "æš‚æ— åŒ¹é…" not in text_content:
                text_content += f"\n{CONFIG['FEISHU_SEPARATOR']}\n\n"

            text_content += "âš ï¸ **æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š**\n\n"
            for i, id_value in enumerate(failed_ids, 1):
                text_content += f"  â€¢ <font color='red'>{id_value}</font>\n"

        # æ·»åŠ åº•éƒ¨æ—¶é—´æˆ³
        now = TimeHelper.get_beijing_time()
        text_content += f"\n\n<font color='grey'>æ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}</font>"

        return text_content


class NewsAnalyzer:
    """æ–°é—»åˆ†æä¸»ç±»"""

    def __init__(
        self,
        request_interval: int = CONFIG["REQUEST_INTERVAL"],
        feishu_report_type: str = CONFIG["FEISHU_REPORT_TYPE"],
        rank_threshold: int = CONFIG["RANK_THRESHOLD"],
    ):
        """
        åˆå§‹åŒ–æ–°é—»åˆ†æå™¨

        Args:
            request_interval: è¯·æ±‚é—´éš”(æ¯«ç§’)
            feishu_report_type: é£ä¹¦æŠ¥å‘Šç±»å‹ï¼Œå¯é€‰å€¼: "current"(å½“å‰çˆ¬å–), "daily"(å½“æ—¥æ±‡æ€»), "both"(ä¸¤è€…éƒ½å‘é€)
            rank_threshold: æ’åæ˜¾ç¤ºé˜ˆå€¼
        """
        self.request_interval = request_interval
        self.feishu_report_type = feishu_report_type
        self.rank_threshold = rank_threshold

        # åˆ¤æ–­æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒä¸­
        self.is_github_actions = os.environ.get("GITHUB_ACTIONS") == "true"

        # è®¾ç½®ä»£ç†
        self.proxy_url = None
        if not self.is_github_actions and CONFIG["USE_PROXY"]:
            # æœ¬åœ°ç¯å¢ƒä¸”å¯ç”¨ä»£ç†æ—¶ä½¿ç”¨ä»£ç†
            self.proxy_url = CONFIG["DEFAULT_PROXY"]
            print("æœ¬åœ°ç¯å¢ƒï¼Œä½¿ç”¨ä»£ç†")
        elif not self.is_github_actions and not CONFIG["USE_PROXY"]:
            print("æœ¬åœ°ç¯å¢ƒï¼Œæœªå¯ç”¨ä»£ç†")
        else:
            print("GitHub Actionsç¯å¢ƒï¼Œä¸ä½¿ç”¨ä»£ç†")

        # åˆå§‹åŒ–æ•°æ®è·å–å™¨
        self.data_fetcher = DataFetcher(self.proxy_url)

    def generate_daily_summary(self) -> Optional[str]:
        """
        ç”Ÿæˆå½“æ—¥ç»Ÿè®¡æŠ¥å‘Š

        Returns:
            HTMLæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœç”Ÿæˆå¤±è´¥åˆ™è¿”å›None
        """
        print("å¼€å§‹ç”Ÿæˆå½“æ—¥ç»Ÿè®¡æŠ¥å‘Š...")

        # è¯»å–å½“å¤©æ‰€æœ‰æ ‡é¢˜
        all_results, id_to_alias, title_info = DataProcessor.read_all_today_titles()

        if not all_results:
            print("æ²¡æœ‰æ‰¾åˆ°å½“å¤©çš„æ•°æ®")
            return None

        # è®¡ç®—æ ‡é¢˜æ€»æ•°
        total_titles = sum(len(titles) for titles in all_results.values())
        print(f"è¯»å–åˆ° {total_titles} ä¸ªæ ‡é¢˜")

        # åŠ è½½é¢‘ç‡è¯å’Œè¿‡æ»¤è¯
        word_groups, filter_words = DataProcessor.load_frequency_words()

        # ç»Ÿè®¡è¯é¢‘
        stats, total_titles = StatisticsCalculator.count_word_frequency(
            all_results,
            word_groups,
            filter_words,
            id_to_alias,
            title_info,
            self.rank_threshold,
        )

        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_file = ReportGenerator.generate_html_report(
            stats, total_titles, is_daily=True
        )
        print(f"å½“æ—¥HTMLç»Ÿè®¡æŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")

        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å‘é€å½“æ—¥æ±‡æ€»åˆ°é£ä¹¦
        if self.feishu_report_type in ["daily", "both"]:
            ReportGenerator.send_to_feishu(stats, [], "å½“æ—¥æ±‡æ€»")

        return html_file

    def run(self) -> None:
        """æ‰§è¡Œæ–°é—»åˆ†ææµç¨‹"""
        # è¾“å‡ºå½“å‰æ—¶é—´ä¿¡æ¯
        now = TimeHelper.get_beijing_time()
        print(f"å½“å‰åŒ—äº¬æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')}")

        # æ£€æŸ¥FEISHU_WEBHOOK_URLæ˜¯å¦å­˜åœ¨
        webhook_url = os.environ.get("FEISHU_WEBHOOK_URL", CONFIG["FEISHU_WEBHOOK_URL"])
        if not webhook_url and not CONFIG["CONTINUE_WITHOUT_FEISHU"]:
            print(
                "é”™è¯¯: FEISHU_WEBHOOK_URLæœªè®¾ç½®æˆ–æ— æ•ˆï¼Œä¸”CONTINUE_WITHOUT_FEISHUä¸ºFalseï¼Œç¨‹åºé€€å‡º"
            )
            return

        if not webhook_url:
            print(
                "è­¦å‘Š: FEISHU_WEBHOOK_URLæœªè®¾ç½®æˆ–æ— æ•ˆï¼Œå°†ç»§ç»­æ‰§è¡Œçˆ¬è™«ä½†ä¸å‘é€é£ä¹¦é€šçŸ¥"
            )

        print(f"é£ä¹¦æŠ¥å‘Šç±»å‹: {self.feishu_report_type}")
        print(f"æ’åé˜ˆå€¼: {self.rank_threshold}")

        # è¦çˆ¬å–çš„ç½‘ç«™IDåˆ—è¡¨
        ids = [
            ("toutiao", "ä»Šæ—¥å¤´æ¡"),
            ("baidu", "ç™¾åº¦çƒ­æœ"),
            ("wallstreetcn-hot", "åå°”è¡—è§é—»"),
            ("thepaper", "æ¾æ¹ƒæ–°é—»"),
            ("bilibili-hot-search", "bilibili çƒ­æœ"),
            ("cls-hot", "è´¢è”ç¤¾çƒ­é—¨"),
            ("ifeng", "å‡¤å‡°ç½‘"),
            "tieba",
            "weibo",
            "douyin",
            "zhihu",
        ]

        print(f"å¼€å§‹çˆ¬å–æ•°æ®ï¼Œè¯·æ±‚é—´éš”è®¾ç½®ä¸º {self.request_interval} æ¯«ç§’")

        # ç¡®ä¿outputç›®å½•å­˜åœ¨
        FileHelper.ensure_directory_exists("output")

        # çˆ¬å–æ•°æ®
        results, id_to_alias, failed_ids = self.data_fetcher.crawl_websites(
            ids, self.request_interval
        )

        # ä¿å­˜æ ‡é¢˜åˆ°æ–‡ä»¶
        title_file = DataProcessor.save_titles_to_file(results, id_to_alias, failed_ids)
        print(f"æ ‡é¢˜å·²ä¿å­˜åˆ°: {title_file}")

        # ä»æ–‡ä»¶åä¸­æå–æ—¶é—´ä¿¡æ¯
        time_info = os.path.basename(title_file).replace(".txt", "")

        # åˆ›å»ºæ ‡é¢˜ä¿¡æ¯å­—å…¸
        title_info = {}
        for source_id, titles_data in results.items():
            title_info[source_id] = {}
            for title, ranks in titles_data.items():
                title_info[source_id][title] = {
                    "first_time": time_info,
                    "last_time": time_info,
                    "count": 1,
                    "ranks": ranks,
                }

        # åŠ è½½é¢‘ç‡è¯å’Œè¿‡æ»¤è¯
        word_groups, filter_words = DataProcessor.load_frequency_words()

        # ç»Ÿè®¡è¯é¢‘
        stats, total_titles = StatisticsCalculator.count_word_frequency(
            results,
            word_groups,
            filter_words,
            id_to_alias,
            title_info,
            self.rank_threshold,
        )

        # æ ¹æ®é…ç½®å†³å®šå‘é€å“ªç§æŠ¥å‘Š
        if self.feishu_report_type in ["current", "both"]:
            # å‘é€å½“å‰çˆ¬å–æ•°æ®åˆ°é£ä¹¦
            ReportGenerator.send_to_feishu(stats, failed_ids, "å•æ¬¡çˆ¬å–")

        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_file = ReportGenerator.generate_html_report(
            stats, total_titles, failed_ids
        )
        print(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")

        # ç”Ÿæˆå½“æ—¥ç»Ÿè®¡æŠ¥å‘Š
        daily_html = self.generate_daily_summary()

        # åœ¨æœ¬åœ°ç¯å¢ƒä¸­è‡ªåŠ¨æ‰“å¼€HTMLæ–‡ä»¶
        if not self.is_github_actions and html_file:
            file_url = "file://" + os.path.abspath(html_file)
            print(f"æ­£åœ¨æ‰“å¼€HTMLæŠ¥å‘Š: {file_url}")
            webbrowser.open(file_url)

            if daily_html:
                daily_url = "file://" + os.path.abspath(daily_html)
                print(f"æ­£åœ¨æ‰“å¼€å½“æ—¥ç»Ÿè®¡æŠ¥å‘Š: {daily_url}")
                webbrowser.open(daily_url)


def main():
    """ç¨‹åºå…¥å£ç‚¹"""
    # åˆå§‹åŒ–å¹¶è¿è¡Œæ–°é—»åˆ†æå™¨
    analyzer = NewsAnalyzer(
        request_interval=CONFIG["REQUEST_INTERVAL"],
        feishu_report_type=CONFIG["FEISHU_REPORT_TYPE"],
        rank_threshold=CONFIG["RANK_THRESHOLD"],
    )
    analyzer.run()


if __name__ == "__main__":
    main()
