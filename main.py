# coding=utf-8

import json
import time
import random
from datetime import datetime
import webbrowser
from typing import Dict, List, Tuple, Optional, Union
from pathlib import Path
import os

import requests
import pytz

CONFIG = {
    "VERSION": "1.3.0",
    "VERSION_CHECK_URL": "https://raw.githubusercontent.com/sansan0/TrendRadar/refs/heads/master/version",
    "SHOW_VERSION_UPDATE": True,  # æ§åˆ¶æ˜¾ç¤ºç‰ˆæœ¬æ›´æ–°æç¤ºï¼Œæ”¹æˆ False å°†ä¸æ¥å—æ–°ç‰ˆæœ¬æç¤º
    "FEISHU_MESSAGE_SEPARATOR": "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",  # feishuæ¶ˆæ¯åˆ†å‰²çº¿
    "REQUEST_INTERVAL": 1000,  # è¯·æ±‚é—´éš”(æ¯«ç§’)
    "REPORT_TYPE": "daily",  # æŠ¥å‘Šç±»å‹: "current"|"daily"|"both"
    "RANK_THRESHOLD": 5,  # æ’åé«˜äº®é˜ˆå€¼
    "USE_PROXY": True,  # æ˜¯å¦å¯ç”¨ä»£ç†
    "DEFAULT_PROXY": "http://127.0.0.1:10086",
    "ENABLE_CRAWLER": True,  # æ˜¯å¦å¯ç”¨çˆ¬å–æ–°é—»åŠŸèƒ½ï¼ŒFalseæ—¶ç›´æ¥åœæ­¢ç¨‹åº
    "ENABLE_NOTIFICATION": True,  # æ˜¯å¦å¯ç”¨é€šçŸ¥åŠŸèƒ½ï¼ŒFalseæ—¶ä¸å‘é€æ‰‹æœºé€šçŸ¥
    "MESSAGE_BATCH_SIZE": 4000,  # æ¶ˆæ¯åˆ†æ‰¹å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    "BATCH_SEND_INTERVAL": 1,  # æ‰¹æ¬¡å‘é€é—´éš”ï¼ˆç§’ï¼‰
    # é£ä¹¦æœºå™¨äººçš„ webhook URL
    "FEISHU_WEBHOOK_URL": "",
    # é’‰é’‰æœºå™¨äººçš„ webhook URL
    "DINGTALK_WEBHOOK_URL": "",
    # ä¼ä¸šå¾®ä¿¡æœºå™¨äººçš„ webhook URL
    "WEWORK_WEBHOOK_URL": "",
    # Telegram è¦å¡«ä¸¤ä¸ª
    "TELEGRAM_BOT_TOKEN": "",
    "TELEGRAM_CHAT_ID": "",
    # ç”¨äºè®©å…³æ³¨åº¦æ›´é«˜çš„æ–°é—»åœ¨æ›´å‰é¢æ˜¾ç¤ºï¼Œè¿™é‡Œæ˜¯æƒé‡æ’åºé…ç½®ï¼Œåˆèµ·æ¥æ˜¯ 1 å°±è¡Œ
    "WEIGHT_CONFIG": {
        "RANK_WEIGHT": 0.6,  # æ’å
        "FREQUENCY_WEIGHT": 0.3,  # é¢‘æ¬¡
        "HOTNESS_WEIGHT": 0.1,  # çƒ­åº¦
    },
}


class TimeHelper:
    """æ—¶é—´å¤„ç†å·¥å…·"""

    @staticmethod
    def get_beijing_time() -> datetime:
        return datetime.now(pytz.timezone("Asia/Shanghai"))

    @staticmethod
    def format_date_folder() -> str:
        return TimeHelper.get_beijing_time().strftime("%Yå¹´%mæœˆ%dæ—¥")

    @staticmethod
    def format_time_filename() -> str:
        return TimeHelper.get_beijing_time().strftime("%Hæ—¶%Måˆ†")


class VersionChecker:
    """ç‰ˆæœ¬æ£€æŸ¥å·¥å…·"""

    @staticmethod
    def parse_version(version_str: str) -> Tuple[int, int, int]:
        """è§£æç‰ˆæœ¬å·å­—ç¬¦ä¸²ä¸ºå…ƒç»„"""
        try:
            parts = version_str.strip().split(".")
            if len(parts) != 3:
                raise ValueError("ç‰ˆæœ¬å·æ ¼å¼ä¸æ­£ç¡®")
            return tuple(int(part) for part in parts)
        except (ValueError, AttributeError):
            print(f"æ— æ³•è§£æç‰ˆæœ¬å·: {version_str}")
            return (0, 0, 0)

    @staticmethod
    def compare_versions(current: str, remote: str) -> int:
        """æ¯”è¾ƒç‰ˆæœ¬å·"""
        current_tuple = VersionChecker.parse_version(current)
        remote_tuple = VersionChecker.parse_version(remote)

        if current_tuple < remote_tuple:
            return -1  # éœ€è¦æ›´æ–°
        elif current_tuple > remote_tuple:
            return 1  # å½“å‰ç‰ˆæœ¬æ›´æ–°
        else:
            return 0  # ç‰ˆæœ¬ç›¸åŒ

    @staticmethod
    def check_for_updates(
        current_version: str,
        version_url: str,
        proxy_url: Optional[str] = None,
        timeout: int = 10,
    ) -> Tuple[bool, Optional[str]]:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ–°ç‰ˆæœ¬"""
        try:
            proxies = None
            if proxy_url:
                proxies = {"http": proxy_url, "https": proxy_url}

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "text/plain, */*",
                "Cache-Control": "no-cache",
            }

            response = requests.get(
                version_url, proxies=proxies, headers=headers, timeout=timeout
            )
            response.raise_for_status()

            remote_version = response.text.strip()
            print(f"å½“å‰ç‰ˆæœ¬: {current_version}, è¿œç¨‹ç‰ˆæœ¬: {remote_version}")

            comparison = VersionChecker.compare_versions(
                current_version, remote_version
            )
            need_update = comparison == -1

            return need_update, remote_version if need_update else None

        except Exception as e:
            print(f"ç‰ˆæœ¬æ£€æŸ¥å¤±è´¥: {e}")
            return False, None


class FileHelper:
    """æ–‡ä»¶æ“ä½œå·¥å…·"""

    @staticmethod
    def ensure_directory_exists(directory: str) -> None:
        Path(directory).mkdir(parents=True, exist_ok=True)

    @staticmethod
    def get_output_path(subfolder: str, filename: str) -> str:
        date_folder = TimeHelper.format_date_folder()
        output_dir = Path("output") / date_folder / subfolder
        FileHelper.ensure_directory_exists(str(output_dir))
        return str(output_dir / filename)


class DataFetcher:
    """æ•°æ®è·å–å™¨"""

    def __init__(self, proxy_url: Optional[str] = None):
        self.proxy_url = proxy_url

    def fetch_data(
        self,
        id_info: Union[str, Tuple[str, str]],
        max_retries: int = 2,
        min_retry_wait: int = 3,
        max_retry_wait: int = 5,
    ) -> Tuple[Optional[str], str, str]:
        """è·å–æŒ‡å®šIDæ•°æ®ï¼Œæ”¯æŒé‡è¯•"""
        if isinstance(id_info, tuple):
            id_value, alias = id_info
        else:
            id_value = id_info
            alias = id_value

        url = f"https://newsnow.busiyi.world/api/s?id={id_value}&latest"

        proxies = None
        if self.proxy_url:
            proxies = {"http": self.proxy_url, "https": self.proxy_url}

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
            "Cache-Control": "no-cache",
        }

        retries = 0
        while retries <= max_retries:
            try:
                response = requests.get(
                    url, proxies=proxies, headers=headers, timeout=10
                )
                response.raise_for_status()

                data_text = response.text
                data_json = json.loads(data_text)

                status = data_json.get("status", "æœªçŸ¥")
                if status not in ["success", "cache"]:
                    raise ValueError(f"å“åº”çŠ¶æ€å¼‚å¸¸: {status}")

                status_info = "æœ€æ–°æ•°æ®" if status == "success" else "ç¼“å­˜æ•°æ®"
                print(f"è·å– {id_value} æˆåŠŸï¼ˆ{status_info}ï¼‰")
                return data_text, id_value, alias

            except Exception as e:
                retries += 1
                if retries <= max_retries:
                    base_wait = random.uniform(min_retry_wait, max_retry_wait)
                    additional_wait = (retries - 1) * random.uniform(1, 2)
                    wait_time = base_wait + additional_wait
                    print(f"è¯·æ±‚ {id_value} å¤±è´¥: {e}. {wait_time:.2f}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"è¯·æ±‚ {id_value} å¤±è´¥: {e}")
                    return None, id_value, alias
        return None, id_value, alias

    def crawl_websites(
        self,
        ids_list: List[Union[str, Tuple[str, str]]],
        request_interval: int = CONFIG["REQUEST_INTERVAL"],
    ) -> Tuple[Dict, Dict, List]:
        """çˆ¬å–å¤šä¸ªç½‘ç«™æ•°æ®"""
        results = {}
        id_to_alias = {}
        failed_ids = []

        for i, id_info in enumerate(ids_list):
            if isinstance(id_info, tuple):
                id_value, alias = id_info
            else:
                id_value = id_info
                alias = id_value

            id_to_alias[id_value] = alias
            response, _, _ = self.fetch_data(id_info)

            if response:
                try:
                    data = json.loads(response)
                    results[id_value] = {}
                    for index, item in enumerate(data.get("items", []), 1):
                        title = item["title"]
                        url = item.get("url", "")
                        mobile_url = item.get("mobileUrl", "")

                        if title in results[id_value]:
                            results[id_value][title]["ranks"].append(index)
                        else:
                            results[id_value][title] = {
                                "ranks": [index],
                                "url": url,
                                "mobileUrl": mobile_url,
                            }
                except json.JSONDecodeError:
                    print(f"è§£æ {id_value} å“åº”å¤±è´¥")
                    failed_ids.append(id_value)
                except Exception as e:
                    print(f"å¤„ç† {id_value} æ•°æ®å‡ºé”™: {e}")
                    failed_ids.append(id_value)
            else:
                failed_ids.append(id_value)

            if i < len(ids_list) - 1:
                actual_interval = request_interval + random.randint(-10, 20)
                actual_interval = max(50, actual_interval)
                time.sleep(actual_interval / 1000)

        print(f"æˆåŠŸ: {list(results.keys())}, å¤±è´¥: {failed_ids}")
        return results, id_to_alias, failed_ids


class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""

    @staticmethod
    def detect_latest_new_titles(id_to_alias: Dict) -> Dict:
        """æ£€æµ‹å½“æ—¥æœ€æ–°æ‰¹æ¬¡çš„æ–°å¢æ ‡é¢˜"""
        date_folder = TimeHelper.format_date_folder()
        txt_dir = Path("output") / date_folder / "txt"

        if not txt_dir.exists():
            return {}

        files = sorted([f for f in txt_dir.iterdir() if f.suffix == ".txt"])
        if len(files) < 2:
            # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼ˆç¬¬ä¸€æ¬¡çˆ¬å–ï¼‰ï¼Œæ²¡æœ‰"æ–°å¢"çš„æ¦‚å¿µï¼Œè¿”å›ç©ºå­—å…¸
            return {}

        latest_file = files[-1]
        latest_titles = DataProcessor._parse_file_titles(latest_file)

        # æ±‡æ€»å†å²æ ‡é¢˜
        historical_titles = {}
        for file_path in files[:-1]:
            historical_data = DataProcessor._parse_file_titles(file_path)
            for source_name, titles_data in historical_data.items():
                if source_name not in historical_titles:
                    historical_titles[source_name] = set()
                for title in titles_data.keys():
                    historical_titles[source_name].add(title)

        # æ‰¾å‡ºæ–°å¢æ ‡é¢˜
        new_titles = {}
        for source_name, latest_source_titles in latest_titles.items():
            historical_set = historical_titles.get(source_name, set())
            source_new_titles = {}

            for title, title_data in latest_source_titles.items():
                if title not in historical_set:
                    source_new_titles[title] = title_data

            if source_new_titles:
                source_id = None
                for id_val, alias in id_to_alias.items():
                    if alias == source_name:
                        source_id = id_val
                        break

                if source_id:
                    new_titles[source_id] = source_new_titles

        return new_titles

    @staticmethod
    def _parse_file_titles(file_path: Path) -> Dict:
        """è§£æå•ä¸ªtxtæ–‡ä»¶çš„æ ‡é¢˜æ•°æ®"""
        titles_by_source = {}

        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
            sections = content.split("\n\n")

            for section in sections:
                if not section.strip() or "==== ä»¥ä¸‹IDè¯·æ±‚å¤±è´¥ ====" in section:
                    continue

                lines = section.strip().split("\n")
                if len(lines) < 2:
                    continue

                source_name = lines[0].strip()
                titles_by_source[source_name] = {}

                for line in lines[1:]:
                    if line.strip():
                        try:
                            title_part = line.strip()
                            rank = None

                            # æå–æ’å
                            if (
                                ". " in title_part
                                and title_part.split(". ")[0].isdigit()
                            ):
                                rank_str, title_part = title_part.split(". ", 1)
                                rank = int(rank_str)

                            # æå–MOBILE URL
                            mobile_url = ""
                            if " [MOBILE:" in title_part:
                                title_part, mobile_part = title_part.rsplit(
                                    " [MOBILE:", 1
                                )
                                if mobile_part.endswith("]"):
                                    mobile_url = mobile_part[:-1]

                            # æå–URL
                            url = ""
                            if " [URL:" in title_part:
                                title_part, url_part = title_part.rsplit(" [URL:", 1)
                                if url_part.endswith("]"):
                                    url = url_part[:-1]

                            title = title_part.strip()
                            ranks = [rank] if rank is not None else [1]

                            titles_by_source[source_name][title] = {
                                "ranks": ranks,
                                "url": url,
                                "mobileUrl": mobile_url,
                            }

                        except Exception as e:
                            print(f"è§£ææ ‡é¢˜è¡Œå‡ºé”™: {line}, é”™è¯¯: {e}")

        return titles_by_source

    @staticmethod
    def save_titles_to_file(results: Dict, id_to_alias: Dict, failed_ids: List) -> str:
        """ä¿å­˜æ ‡é¢˜åˆ°æ–‡ä»¶"""
        file_path = FileHelper.get_output_path(
            "txt", f"{TimeHelper.format_time_filename()}.txt"
        )

        with open(file_path, "w", encoding="utf-8") as f:
            for id_value, title_data in results.items():
                display_name = id_to_alias.get(id_value, id_value)
                f.write(f"{display_name}\n")

                # æŒ‰æ’åæ’åºæ ‡é¢˜
                sorted_titles = []
                for title, info in title_data.items():
                    if isinstance(info, dict):
                        ranks = info.get("ranks", [])
                        url = info.get("url", "")
                        mobile_url = info.get("mobileUrl", "")
                    else:
                        ranks = info if isinstance(info, list) else []
                        url = ""
                        mobile_url = ""

                    rank = ranks[0] if ranks else 1
                    sorted_titles.append((rank, title, url, mobile_url))

                sorted_titles.sort(key=lambda x: x[0])

                for rank, title, url, mobile_url in sorted_titles:
                    line = f"{rank}. {title}"

                    if url:
                        line += f" [URL:{url}]"
                    if mobile_url:
                        line += f" [MOBILE:{mobile_url}]"
                    f.write(line + "\n")

                f.write("\n")

            if failed_ids:
                f.write("==== ä»¥ä¸‹IDè¯·æ±‚å¤±è´¥ ====\n")
                for id_value in failed_ids:
                    display_name = id_to_alias.get(id_value, id_value)
                    f.write(f"{display_name} (ID: {id_value})\n")

        return file_path

    @staticmethod
    def load_frequency_words(
        frequency_file: str = "frequency_words.txt",
    ) -> Tuple[List[Dict], List[str]]:
        """åŠ è½½é¢‘ç‡è¯é…ç½®"""
        frequency_path = Path(frequency_file)
        if not frequency_path.exists():
            print(f"é¢‘ç‡è¯æ–‡ä»¶ {frequency_file} ä¸å­˜åœ¨")
            return [], []

        with open(frequency_path, "r", encoding="utf-8") as f:
            content = f.read()

        word_groups = [
            group.strip() for group in content.split("\n\n") if group.strip()
        ]

        processed_groups = []
        filter_words = []

        for group in word_groups:
            words = [word.strip() for word in group.split("\n") if word.strip()]

            group_required_words = []
            group_normal_words = []
            group_filter_words = []

            for word in words:
                if word.startswith("!"):
                    filter_words.append(word[1:])
                    group_filter_words.append(word[1:])
                elif word.startswith("+"):
                    group_required_words.append(word[1:])
                else:
                    group_normal_words.append(word)

            if group_required_words or group_normal_words:
                if group_normal_words:
                    group_key = " ".join(group_normal_words)
                else:
                    group_key = " ".join(group_required_words)

                processed_groups.append(
                    {
                        "required": group_required_words,
                        "normal": group_normal_words,
                        "group_key": group_key,
                    }
                )

        return processed_groups, filter_words

    @staticmethod
    def read_all_today_titles() -> Tuple[Dict, Dict, Dict]:
        """è¯»å–å½“å¤©æ‰€æœ‰æ ‡é¢˜æ–‡ä»¶"""
        date_folder = TimeHelper.format_date_folder()
        txt_dir = Path("output") / date_folder / "txt"

        if not txt_dir.exists():
            return {}, {}, {}

        all_results = {}
        id_to_alias = {}
        title_info = {}

        files = sorted([f for f in txt_dir.iterdir() if f.suffix == ".txt"])

        for file_path in files:
            time_info = file_path.stem

            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

                sections = content.split("\n\n")
                for section in sections:
                    if not section.strip() or "==== ä»¥ä¸‹IDè¯·æ±‚å¤±è´¥ ====" in section:
                        continue

                    lines = section.strip().split("\n")
                    if len(lines) < 2:
                        continue

                    source_name = lines[0].strip()
                    title_data = {}

                    for line in lines[1:]:
                        if line.strip():
                            try:
                                rank = None
                                title_part = line.strip()

                                # æå–è¡Œé¦–çš„æ’åæ•°å­—
                                if (
                                    ". " in title_part
                                    and title_part.split(". ")[0].isdigit()
                                ):
                                    parts = title_part.split(". ", 1)
                                    rank = int(parts[0])
                                    title_part = parts[1]

                                # æå– MOBILE URL
                                mobile_url = ""
                                if " [MOBILE:" in title_part:
                                    title_part, mobile_part = title_part.rsplit(
                                        " [MOBILE:", 1
                                    )
                                    if mobile_part.endswith("]"):
                                        mobile_url = mobile_part[:-1]

                                # æå– URL
                                url = ""
                                if " [URL:" in title_part:
                                    title_part, url_part = title_part.rsplit(
                                        " [URL:", 1
                                    )
                                    if url_part.endswith("]"):
                                        url = url_part[:-1]

                                title = title_part.strip()
                                ranks = [rank] if rank is not None else [1]

                                title_data[title] = {
                                    "ranks": ranks,
                                    "url": url,
                                    "mobileUrl": mobile_url,
                                }

                            except Exception as e:
                                print(f"è§£ææ ‡é¢˜è¡Œå‡ºé”™: {line}, é”™è¯¯: {e}")

                    DataProcessor._process_source_data(
                        source_name,
                        title_data,
                        time_info,
                        all_results,
                        title_info,
                        id_to_alias,
                    )

        # è½¬æ¢ä¸ºIDæ ¼å¼
        id_results = {}
        id_title_info = {}
        for name, titles in all_results.items():
            for id_value, alias in id_to_alias.items():
                if alias == name:
                    id_results[id_value] = titles
                    id_title_info[id_value] = title_info[name]
                    break

        return id_results, id_to_alias, id_title_info

    @staticmethod
    def _process_source_data(
        source_name: str,
        title_data: Dict,
        time_info: str,
        all_results: Dict,
        title_info: Dict,
        id_to_alias: Dict,
    ) -> None:
        """å¤„ç†æ¥æºæ•°æ®ï¼Œåˆå¹¶é‡å¤æ ‡é¢˜"""
        if source_name not in all_results:
            all_results[source_name] = title_data

            if source_name not in title_info:
                title_info[source_name] = {}

            for title, data in title_data.items():
                ranks = data.get("ranks", [])
                url = data.get("url", "")
                mobile_url = data.get("mobileUrl", "")

                title_info[source_name][title] = {
                    "first_time": time_info,
                    "last_time": time_info,
                    "count": 1,
                    "ranks": ranks,
                    "url": url,
                    "mobileUrl": mobile_url,
                }

            reversed_id = source_name.lower().replace(" ", "-")
            id_to_alias[reversed_id] = source_name
        else:
            for title, data in title_data.items():
                ranks = data.get("ranks", [])
                url = data.get("url", "")
                mobile_url = data.get("mobileUrl", "")

                if title not in all_results[source_name]:
                    all_results[source_name][title] = {
                        "ranks": ranks,
                        "url": url,
                        "mobileUrl": mobile_url,
                    }
                    title_info[source_name][title] = {
                        "first_time": time_info,
                        "last_time": time_info,
                        "count": 1,
                        "ranks": ranks,
                        "url": url,
                        "mobileUrl": mobile_url,
                    }
                else:
                    existing_data = all_results[source_name][title]
                    existing_ranks = existing_data.get("ranks", [])
                    existing_url = existing_data.get("url", "")
                    existing_mobile_url = existing_data.get("mobileUrl", "")

                    merged_ranks = existing_ranks.copy()
                    for rank in ranks:
                        if rank not in merged_ranks:
                            merged_ranks.append(rank)

                    all_results[source_name][title] = {
                        "ranks": merged_ranks,
                        "url": existing_url or url,
                        "mobileUrl": existing_mobile_url or mobile_url,
                    }

                    title_info[source_name][title]["last_time"] = time_info
                    title_info[source_name][title]["ranks"] = merged_ranks
                    title_info[source_name][title]["count"] += 1
                    if not title_info[source_name][title].get("url"):
                        title_info[source_name][title]["url"] = url
                    if not title_info[source_name][title].get("mobileUrl"):
                        title_info[source_name][title]["mobileUrl"] = mobile_url


class StatisticsCalculator:
    """ç»Ÿè®¡è®¡ç®—å™¨"""

    @staticmethod
    def calculate_news_weight(
        title_data: Dict, rank_threshold: int = CONFIG["RANK_THRESHOLD"]
    ) -> float:
        """è®¡ç®—æ–°é—»æƒé‡ï¼Œç”¨äºæ’åº"""
        ranks = title_data.get("ranks", [])
        if not ranks:
            return 0.0

        count = title_data.get("count", len(ranks))
        weight_config = CONFIG["WEIGHT_CONFIG"]

        # æ’åæƒé‡ï¼šÎ£(11 - min(rank, 10)) / å‡ºç°æ¬¡æ•°
        rank_scores = []
        for rank in ranks:
            score = 11 - min(rank, 10)
            rank_scores.append(score)

        rank_weight = sum(rank_scores) / len(ranks) if ranks else 0

        # é¢‘æ¬¡æƒé‡ï¼šmin(å‡ºç°æ¬¡æ•°, 10) Ã— 10
        frequency_weight = min(count, 10) * 10

        # çƒ­åº¦åŠ æˆï¼šé«˜æ’åæ¬¡æ•° / æ€»å‡ºç°æ¬¡æ•° Ã— 100
        high_rank_count = sum(1 for rank in ranks if rank <= rank_threshold)
        hotness_ratio = high_rank_count / len(ranks) if ranks else 0
        hotness_weight = hotness_ratio * 100

        # ç»¼åˆæƒé‡è®¡ç®—
        total_weight = (
            rank_weight * weight_config["RANK_WEIGHT"]
            + frequency_weight * weight_config["FREQUENCY_WEIGHT"]
            + hotness_weight * weight_config["HOTNESS_WEIGHT"]
        )

        return total_weight

    @staticmethod
    def sort_titles_by_weight(
        titles_list: List[Dict], rank_threshold: int = CONFIG["RANK_THRESHOLD"]
    ) -> List[Dict]:
        """æŒ‰æƒé‡å¯¹æ–°é—»æ ‡é¢˜åˆ—è¡¨è¿›è¡Œæ’åº"""

        def get_sort_key(title_data):
            weight = StatisticsCalculator.calculate_news_weight(
                title_data, rank_threshold
            )
            ranks = title_data.get("ranks", [])
            count = title_data.get("count", 1)

            # ä¸»è¦æŒ‰æƒé‡æ’åºï¼Œæƒé‡ç›¸åŒæ—¶æŒ‰æœ€é«˜æ’åæ’åºï¼Œå†ç›¸åŒæ—¶æŒ‰å‡ºç°æ¬¡æ•°æ’åº
            min_rank = min(ranks) if ranks else 999
            return (-weight, min_rank, -count)

        return sorted(titles_list, key=get_sort_key)

    @staticmethod
    def _matches_word_groups(
        title: str, word_groups: List[Dict], filter_words: List[str]
    ) -> bool:
        """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ¹é…è¯ç»„è§„åˆ™"""
        title_lower = title.lower()

        # è¿‡æ»¤è¯æ£€æŸ¥
        if any(filter_word.lower() in title_lower for filter_word in filter_words):
            return False

        # è¯ç»„åŒ¹é…æ£€æŸ¥
        for group in word_groups:
            required_words = group["required"]
            normal_words = group["normal"]

            # å¿…é¡»è¯æ£€æŸ¥
            if required_words:
                all_required_present = all(
                    req_word.lower() in title_lower for req_word in required_words
                )
                if not all_required_present:
                    continue

            # æ™®é€šè¯æ£€æŸ¥
            if normal_words:
                any_normal_present = any(
                    normal_word.lower() in title_lower for normal_word in normal_words
                )
                if not any_normal_present:
                    continue

            return True

        return False

    @staticmethod
    def count_word_frequency(
        results: Dict,
        word_groups: List[Dict],
        filter_words: List[str],
        id_to_alias: Dict,
        title_info: Optional[Dict] = None,
        rank_threshold: int = CONFIG["RANK_THRESHOLD"],
        new_titles: Optional[Dict] = None,
    ) -> Tuple[List[Dict], int]:
        """ç»Ÿè®¡è¯é¢‘ï¼Œæ”¯æŒå¿…é¡»è¯ã€é¢‘ç‡è¯ã€è¿‡æ»¤è¯ï¼Œå¹¶æ ‡è®°æ–°å¢æ ‡é¢˜"""
        word_stats = {}
        total_titles = 0
        processed_titles = {}

        if title_info is None:
            title_info = {}
        if new_titles is None:
            new_titles = {}

        for group in word_groups:
            group_key = group["group_key"]
            word_stats[group_key] = {"count": 0, "titles": {}}

        for source_id, titles_data in results.items():
            total_titles += len(titles_data)

            if source_id not in processed_titles:
                processed_titles[source_id] = {}

            for title, title_data in titles_data.items():
                if title in processed_titles.get(source_id, {}):
                    continue

                # ä½¿ç”¨ç»Ÿä¸€çš„åŒ¹é…é€»è¾‘
                if not StatisticsCalculator._matches_word_groups(
                    title, word_groups, filter_words
                ):
                    continue

                source_ranks = title_data.get("ranks", [])
                source_url = title_data.get("url", "")
                source_mobile_url = title_data.get("mobileUrl", "")

                # æ‰¾åˆ°åŒ¹é…çš„è¯ç»„
                title_lower = title.lower()
                for group in word_groups:
                    required_words = group["required"]
                    normal_words = group["normal"]

                    # å†æ¬¡æ£€æŸ¥åŒ¹é…
                    if required_words:
                        all_required_present = all(
                            req_word.lower() in title_lower
                            for req_word in required_words
                        )
                        if not all_required_present:
                            continue

                    if normal_words:
                        any_normal_present = any(
                            normal_word.lower() in title_lower
                            for normal_word in normal_words
                        )
                        if not any_normal_present:
                            continue

                    group_key = group["group_key"]
                    word_stats[group_key]["count"] += 1
                    if source_id not in word_stats[group_key]["titles"]:
                        word_stats[group_key]["titles"][source_id] = []

                    first_time = ""
                    last_time = ""
                    count_info = 1
                    ranks = source_ranks if source_ranks else []
                    url = source_url
                    mobile_url = source_mobile_url

                    if (
                        title_info
                        and source_id in title_info
                        and title in title_info[source_id]
                    ):
                        info = title_info[source_id][title]
                        first_time = info.get("first_time", "")
                        last_time = info.get("last_time", "")
                        count_info = info.get("count", 1)
                        if "ranks" in info and info["ranks"]:
                            ranks = info["ranks"]
                        url = info.get("url", source_url)
                        mobile_url = info.get("mobileUrl", source_mobile_url)

                    if not ranks:
                        ranks = [99]

                    time_display = StatisticsCalculator._format_time_display(
                        first_time, last_time
                    )

                    source_alias = id_to_alias.get(source_id, source_id)

                    # ä¿®å¤is_newåˆ¤æ–­é€»è¾‘ï¼Œæ·»åŠ å®¹é”™å¤„ç†
                    is_new = False
                    if new_titles and source_id in new_titles:
                        new_titles_for_source = new_titles[source_id]
                        if title in new_titles_for_source:
                            is_new = True
                        else:
                            # å¦‚æœç›´æ¥åŒ¹é…å¤±è´¥ï¼Œå°è¯•å»é™¤é¦–å°¾ç©ºæ ¼ååŒ¹é…
                            title_stripped = title.strip()
                            for new_title in new_titles_for_source.keys():
                                if title_stripped == new_title.strip():
                                    is_new = True
                                    break

                    word_stats[group_key]["titles"][source_id].append(
                        {
                            "title": title,
                            "source_alias": source_alias,
                            "first_time": first_time,
                            "last_time": last_time,
                            "time_display": time_display,
                            "count": count_info,
                            "ranks": ranks,
                            "rank_threshold": rank_threshold,
                            "url": url,
                            "mobileUrl": mobile_url,
                            "is_new": is_new,
                        }
                    )

                    if source_id not in processed_titles:
                        processed_titles[source_id] = {}
                    processed_titles[source_id][title] = True
                    break

        stats = []
        for group_key, data in word_stats.items():
            all_titles = []
            for source_id, title_list in data["titles"].items():
                all_titles.extend(title_list)

            # æŒ‰æƒé‡æ’åºæ ‡é¢˜
            sorted_titles = StatisticsCalculator.sort_titles_by_weight(
                all_titles, rank_threshold
            )

            stats.append(
                {
                    "word": group_key,
                    "count": data["count"],
                    "titles": sorted_titles,
                    "percentage": (
                        round(data["count"] / total_titles * 100, 2)
                        if total_titles > 0
                        else 0
                    ),
                }
            )

        stats.sort(key=lambda x: x["count"], reverse=True)
        return stats, total_titles

    @staticmethod
    def _format_rank_base(
        ranks: List[int], rank_threshold: int = 5, format_type: str = "html"
    ) -> str:
        """åŸºç¡€æ’åæ ¼å¼åŒ–æ–¹æ³•"""
        if not ranks:
            return ""

        unique_ranks = sorted(set(ranks))
        min_rank = unique_ranks[0]
        max_rank = unique_ranks[-1]

        # æ ¹æ®æ ¼å¼ç±»å‹é€‰æ‹©ä¸åŒçš„æ ‡è®°æ–¹å¼
        if format_type == "html":
            highlight_start = "<font color='red'><strong>"
            highlight_end = "</strong></font>"
        elif format_type == "feishu":
            highlight_start = "<font color='red'>**"
            highlight_end = "**</font>"
        elif format_type == "dingtalk":
            highlight_start = "**"
            highlight_end = "**"
        elif format_type == "wework":
            highlight_start = "**"
            highlight_end = "**"
        elif format_type == "telegram":
            highlight_start = "<b>"
            highlight_end = "</b>"
        else:
            highlight_start = "**"
            highlight_end = "**"

        # æ ¼å¼åŒ–æ’åæ˜¾ç¤º
        if min_rank <= rank_threshold:
            if min_rank == max_rank:
                return f"{highlight_start}[{min_rank}]{highlight_end}"
            else:
                return f"{highlight_start}[{min_rank} - {max_rank}]{highlight_end}"
        else:
            if min_rank == max_rank:
                return f"[{min_rank}]"
            else:
                return f"[{min_rank} - {max_rank}]"

    @staticmethod
    def _format_rank_for_html(ranks: List[int], rank_threshold: int = 5) -> str:
        """æ ¼å¼åŒ–HTMLæ’åæ˜¾ç¤º"""
        return StatisticsCalculator._format_rank_base(ranks, rank_threshold, "html")

    @staticmethod
    def _format_rank_for_feishu(ranks: List[int], rank_threshold: int = 5) -> str:
        """æ ¼å¼åŒ–é£ä¹¦æ’åæ˜¾ç¤º"""
        return StatisticsCalculator._format_rank_base(ranks, rank_threshold, "feishu")

    @staticmethod
    def _format_rank_for_dingtalk(ranks: List[int], rank_threshold: int = 5) -> str:
        """æ ¼å¼åŒ–é’‰é’‰æ’åæ˜¾ç¤º"""
        return StatisticsCalculator._format_rank_base(ranks, rank_threshold, "dingtalk")

    @staticmethod
    def _format_rank_for_wework(ranks: List[int], rank_threshold: int = 5) -> str:
        """æ ¼å¼åŒ–ä¼ä¸šå¾®ä¿¡æ’åæ˜¾ç¤º"""
        return StatisticsCalculator._format_rank_base(ranks, rank_threshold, "wework")

    @staticmethod
    def _format_rank_for_telegram(ranks: List[int], rank_threshold: int = 5) -> str:
        """æ ¼å¼åŒ–Telegramæ’åæ˜¾ç¤º"""
        return StatisticsCalculator._format_rank_base(ranks, rank_threshold, "telegram")

    @staticmethod
    def _format_time_display(first_time: str, last_time: str) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
        if not first_time:
            return ""
        if first_time == last_time or not last_time:
            return first_time
        else:
            return f"[{first_time} ~ {last_time}]"


class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨"""

    @staticmethod
    def generate_html_report(
        stats: List[Dict],
        total_titles: int,
        failed_ids: Optional[List] = None,
        is_daily: bool = False,
        new_titles: Optional[Dict] = None,
        id_to_alias: Optional[Dict] = None,
    ) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        if is_daily:
            filename = "å½“æ—¥ç»Ÿè®¡.html"
        else:
            filename = f"{TimeHelper.format_time_filename()}.html"

        file_path = FileHelper.get_output_path("html", filename)

        # æ•°æ®å¤„ç†å±‚
        report_data = ReportGenerator._prepare_report_data(
            stats, failed_ids, new_titles, id_to_alias
        )

        # æ¸²æŸ“å±‚
        html_content = ReportGenerator._render_html_content(
            report_data, total_titles, is_daily
        )

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        if is_daily:
            root_file_path = Path("index.html")
            with open(root_file_path, "w", encoding="utf-8") as f:
                f.write(html_content)

        return file_path

    @staticmethod
    def _prepare_report_data(
        stats: List[Dict],
        failed_ids: Optional[List] = None,
        new_titles: Optional[Dict] = None,
        id_to_alias: Optional[Dict] = None,
    ) -> Dict:
        """å‡†å¤‡æŠ¥å‘Šæ•°æ®"""
        filtered_new_titles = {}
        if new_titles and id_to_alias:
            word_groups, filter_words = DataProcessor.load_frequency_words()
            for source_id, titles_data in new_titles.items():
                filtered_titles = ReportGenerator._apply_frequency_filter(
                    titles_data, word_groups, filter_words
                )
                if filtered_titles:
                    filtered_new_titles[source_id] = filtered_titles

        processed_stats = []
        for stat in stats:
            if stat["count"] <= 0:
                continue

            processed_titles = []
            for title_data in stat["titles"]:
                processed_title = {
                    "title": title_data["title"],
                    "source_alias": title_data["source_alias"],
                    "time_display": title_data["time_display"],
                    "count": title_data["count"],
                    "ranks": title_data["ranks"],
                    "rank_threshold": title_data["rank_threshold"],
                    "url": title_data.get("url", ""),
                    "mobile_url": title_data.get("mobileUrl", ""),
                    "is_new": title_data.get("is_new", False),
                }
                processed_titles.append(processed_title)

            processed_stats.append(
                {
                    "word": stat["word"],
                    "count": stat["count"],
                    "percentage": stat.get("percentage", 0),
                    "titles": processed_titles,
                }
            )

        processed_new_titles = []
        if filtered_new_titles and id_to_alias:
            for source_id, titles_data in filtered_new_titles.items():
                source_alias = id_to_alias.get(source_id, source_id)
                source_titles = []

                for title, title_data in titles_data.items():
                    url, mobile_url, ranks = ReportGenerator._extract_title_data_fields(
                        title_data
                    )

                    processed_title = {
                        "title": title,
                        "source_alias": source_alias,
                        "time_display": "",
                        "count": 1,
                        "ranks": ranks,
                        "rank_threshold": CONFIG["RANK_THRESHOLD"],
                        "url": url,
                        "mobile_url": mobile_url,
                        "is_new": True,
                    }
                    source_titles.append(processed_title)

                if source_titles:
                    processed_new_titles.append(
                        {
                            "source_id": source_id,
                            "source_alias": source_alias,
                            "titles": source_titles,
                        }
                    )

        return {
            "stats": processed_stats,
            "new_titles": processed_new_titles,
            "failed_ids": failed_ids or [],
            "total_new_count": sum(
                len(source["titles"]) for source in processed_new_titles
            ),
        }

    @staticmethod
    def _extract_title_data_fields(title_data) -> Tuple[str, str, List[int]]:
        """æå–æ ‡é¢˜æ•°æ®çš„é€šç”¨å­—æ®µ"""
        url = title_data.get("url", "")
        mobile_url = title_data.get("mobileUrl", "")
        ranks = title_data.get("ranks", [])

        return url, mobile_url, ranks

    @staticmethod
    def _apply_frequency_filter(
        titles_data: Dict, word_groups: List[Dict], filter_words: List[str]
    ) -> Dict:
        """åº”ç”¨é¢‘ç‡è¯è¿‡æ»¤é€»è¾‘"""
        filtered_titles = {}

        for title, title_data in titles_data.items():
            if StatisticsCalculator._matches_word_groups(
                title, word_groups, filter_words
            ):
                filtered_titles[title] = title_data

        return filtered_titles

    @staticmethod
    def _html_escape(text: str) -> str:
        """HTMLè½¬ä¹‰"""
        if not isinstance(text, str):
            text = str(text)

        return (
            text.replace("&", "&amp;")
            .replace("<", "&lt;")
            .replace(">", "&gt;")
            .replace('"', "&quot;")
            .replace("'", "&#x27;")
        )

    @staticmethod
    def _format_title_html(title_data: Dict) -> str:
        """æ ¼å¼åŒ–HTMLæ ‡é¢˜æ˜¾ç¤º"""
        rank_display = StatisticsCalculator._format_rank_for_html(
            title_data["ranks"], title_data["rank_threshold"]
        )

        link_url = title_data["mobile_url"] or title_data["url"]
        escaped_title = ReportGenerator._html_escape(title_data["title"])
        escaped_source_alias = ReportGenerator._html_escape(title_data["source_alias"])

        if link_url:
            escaped_url = ReportGenerator._html_escape(link_url)
            formatted_title = f'[{escaped_source_alias}] <a href="{escaped_url}" target="_blank" class="news-link">{escaped_title}</a>'
        else:
            formatted_title = (
                f'[{escaped_source_alias}] <span class="no-link">{escaped_title}</span>'
            )

        if rank_display:
            formatted_title += f" {rank_display}"
        if title_data["time_display"]:
            escaped_time = ReportGenerator._html_escape(title_data["time_display"])
            formatted_title += f" <font color='grey'>- {escaped_time}</font>"
        if title_data["count"] > 1:
            formatted_title += f" <font color='green'>({title_data['count']}æ¬¡)</font>"

        if title_data["is_new"]:
            formatted_title = f"<div class='new-title'>ğŸ†• {formatted_title}</div>"

        return formatted_title

    @staticmethod
    def _render_html_content(
        report_data: Dict, total_titles: int, is_daily: bool = False
    ) -> str:
        """æ¸²æŸ“HTMLå†…å®¹"""
        html = """
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>é¢‘ç‡è¯ç»Ÿè®¡æŠ¥å‘Š</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                h1, h2 { color: #333; }
                table { border-collapse: collapse; width: 100%; margin-top: 20px; }
                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                th { background-color: #f2f2f2; }
                tr:nth-child(even) { background-color: #f9f9f9; }
                .word { font-weight: bold; }
                .count { text-align: center; }
                .percentage { text-align: center; }
                .titles { max-width: 500px; }
                .source { color: #666; font-style: italic; }
                .error { color: #d9534f; }
                .news-link { 
                    color: #007bff; 
                    text-decoration: none; 
                    border-bottom: 1px dotted #007bff;
                }
                .news-link:hover { 
                    color: #0056b3; 
                    text-decoration: underline; 
                }
                .news-link:visited { 
                    color: #6f42c1; 
                }
                .no-link { 
                    color: #333; 
                }
                .new-title {
                    background-color: #fff3cd;
                    border: 1px solid #ffc107;
                    border-radius: 3px;
                    padding: 2px 6px;
                    margin: 2px 0;
                }
                .new-section {
                    background-color: #d1ecf1;
                    border: 1px solid #bee5eb;
                    border-radius: 5px;
                    padding: 10px;
                    margin-top: 10px;
                }
                .new-section h3 {
                    color: #0c5460;
                    margin-top: 0;
                }
            </style>
        </head>
        <body>
            <h1>é¢‘ç‡è¯ç»Ÿè®¡æŠ¥å‘Š</h1>
        """

        if is_daily:
            html += "<p>æŠ¥å‘Šç±»å‹: å½“æ—¥æ±‡æ€»</p>"

        now = TimeHelper.get_beijing_time()
        html += f"<p>æ€»æ ‡é¢˜æ•°: {total_titles}</p>"
        html += f"<p>ç”Ÿæˆæ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')}</p>"

        # æ¸²æŸ“å¤±è´¥å¹³å°
        if report_data["failed_ids"]:
            html += """
            <div class="error">
                <h2>è¯·æ±‚å¤±è´¥çš„å¹³å°</h2>
                <ul>
            """
            for id_value in report_data["failed_ids"]:
                html += f"<li>{ReportGenerator._html_escape(id_value)}</li>"
            html += """
                </ul>
            </div>
            """

        # æ¸²æŸ“ç»Ÿè®¡è¡¨æ ¼
        html += """
            <table>
                <tr>
                    <th>æ’å</th>
                    <th>é¢‘ç‡è¯</th>
                    <th>å‡ºç°æ¬¡æ•°</th>
                    <th>å æ¯”</th>
                    <th>ç›¸å…³æ ‡é¢˜</th>
                </tr>
        """

        for i, stat in enumerate(report_data["stats"], 1):
            formatted_titles = []

            for title_data in stat["titles"]:
                formatted_title = ReportGenerator._format_title_html(title_data)
                formatted_titles.append(formatted_title)

            escaped_word = ReportGenerator._html_escape(stat["word"])
            html += f"""
                <tr>
                    <td>{i}</td>
                    <td class="word">{escaped_word}</td>
                    <td class="count">{stat['count']}</td>
                    <td class="percentage">{stat.get('percentage', 0)}%</td>
                    <td class="titles">{"<br>".join(formatted_titles)}</td>
                </tr>
            """

        html += """
            </table>
        """

        # æ¸²æŸ“æ–°å¢æ–°é—»éƒ¨åˆ†
        if report_data["new_titles"]:
            html += f"""
            <div class="new-section">
                <h3>ğŸ†• æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—» (å…± {report_data['total_new_count']} æ¡)</h3>
            """

            for source_data in report_data["new_titles"]:
                escaped_source = ReportGenerator._html_escape(
                    source_data["source_alias"]
                )
                html += (
                    f"<h4>{escaped_source} ({len(source_data['titles'])} æ¡)</h4><ul>"
                )

                for title_data in source_data["titles"]:
                    title_data_copy = title_data.copy()
                    title_data_copy["is_new"] = False
                    formatted_title = ReportGenerator._format_title_html(
                        title_data_copy
                    )
                    # ç§»é™¤æ¥æºæ ‡ç­¾
                    if "] " in formatted_title:
                        formatted_title = formatted_title.split("] ", 1)[1]
                    html += f"<li>{formatted_title}</li>"

                html += "</ul>"

            html += "</div>"

        html += """
        </body>
        </html>
        """

        return html

    @staticmethod
    def _format_title_feishu(title_data: Dict, show_source: bool = True) -> str:
        """æ ¼å¼åŒ–é£ä¹¦æ ‡é¢˜æ˜¾ç¤º"""
        rank_display = StatisticsCalculator._format_rank_for_feishu(
            title_data["ranks"], title_data["rank_threshold"]
        )

        link_url = title_data["mobile_url"] or title_data["url"]
        if link_url:
            formatted_title = f"[{title_data['title']}]({link_url})"
        else:
            formatted_title = title_data["title"]

        title_prefix = "ğŸ†• " if title_data["is_new"] else ""

        if show_source:
            result = f"<font color='grey'>[{title_data['source_alias']}]</font> {title_prefix}{formatted_title}"
        else:
            result = f"{title_prefix}{formatted_title}"

        if rank_display:
            result += f" {rank_display}"
        if title_data["time_display"]:
            result += f" <font color='grey'>- {title_data['time_display']}</font>"
        if title_data["count"] > 1:
            result += f" <font color='green'>({title_data['count']}æ¬¡)</font>"

        return result

    @staticmethod
    def _format_title_dingtalk(title_data: Dict, show_source: bool = True) -> str:
        """æ ¼å¼åŒ–é’‰é’‰æ ‡é¢˜æ˜¾ç¤º"""
        rank_display = StatisticsCalculator._format_rank_for_dingtalk(
            title_data["ranks"], title_data["rank_threshold"]
        )

        link_url = title_data["mobile_url"] or title_data["url"]
        if link_url:
            formatted_title = f"[{title_data['title']}]({link_url})"
        else:
            formatted_title = title_data["title"]

        title_prefix = "ğŸ†• " if title_data["is_new"] else ""

        if show_source:
            result = f"[{title_data['source_alias']}] {title_prefix}{formatted_title}"
        else:
            result = f"{title_prefix}{formatted_title}"

        if rank_display:
            result += f" {rank_display}"
        if title_data["time_display"]:
            result += f" - {title_data['time_display']}"
        if title_data["count"] > 1:
            result += f" ({title_data['count']}æ¬¡)"

        return result

    @staticmethod
    def _format_title_wework(title_data: Dict, show_source: bool = True) -> str:
        """æ ¼å¼åŒ–ä¼ä¸šå¾®ä¿¡æ ‡é¢˜æ˜¾ç¤º"""
        rank_display = StatisticsCalculator._format_rank_for_wework(
            title_data["ranks"], title_data["rank_threshold"]
        )

        link_url = title_data["mobile_url"] or title_data["url"]
        if link_url:
            formatted_title = f"[{title_data['title']}]({link_url})"
        else:
            formatted_title = title_data["title"]

        title_prefix = "ğŸ†• " if title_data["is_new"] else ""

        if show_source:
            result = f"[{title_data['source_alias']}] {title_prefix}{formatted_title}"
        else:
            result = f"{title_prefix}{formatted_title}"

        if rank_display:
            result += f" {rank_display}"
        if title_data["time_display"]:
            result += f" - {title_data['time_display']}"
        if title_data["count"] > 1:
            result += f" ({title_data['count']}æ¬¡)"

        return result

    @staticmethod
    def _format_title_telegram(title_data: Dict, show_source: bool = True) -> str:
        """æ ¼å¼åŒ–Telegramæ ‡é¢˜æ˜¾ç¤º"""
        rank_display = StatisticsCalculator._format_rank_for_telegram(
            title_data["ranks"], title_data["rank_threshold"]
        )

        link_url = title_data["mobile_url"] or title_data["url"]
        if link_url:
            formatted_title = f'<a href="{link_url}">{ReportGenerator._html_escape(title_data["title"])}</a>'
        else:
            formatted_title = title_data["title"]

        title_prefix = "ğŸ†• " if title_data["is_new"] else ""

        if show_source:
            result = f"[{title_data['source_alias']}] {title_prefix}{formatted_title}"
        else:
            result = f"{title_prefix}{formatted_title}"

        if rank_display:
            result += f" {rank_display}"
        if title_data["time_display"]:
            result += f" <code>- {title_data['time_display']}</code>"
        if title_data["count"] > 1:
            result += f" <code>({title_data['count']}æ¬¡)</code>"

        return result

    @staticmethod
    def _render_feishu_content(
        report_data: Dict, update_info: Optional[Dict] = None
    ) -> str:
        """æ¸²æŸ“é£ä¹¦å†…å®¹"""
        text_content = ""

        # æ¸²æŸ“çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡
        if report_data["stats"]:
            text_content += "ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**\n\n"

        total_count = len(report_data["stats"])

        for i, stat in enumerate(report_data["stats"]):
            word = stat["word"]
            count = stat["count"]

            sequence_display = f"<font color='grey'>[{i + 1}/{total_count}]</font>"

            if count >= 10:
                text_content += f"ğŸ”¥ {sequence_display} **{word}** : <font color='red'>{count}</font> æ¡\n\n"
            elif count >= 5:
                text_content += f"ğŸ“ˆ {sequence_display} **{word}** : <font color='orange'>{count}</font> æ¡\n\n"
            else:
                text_content += f"ğŸ“Œ {sequence_display} **{word}** : {count} æ¡\n\n"

            for j, title_data in enumerate(stat["titles"], 1):
                formatted_title = ReportGenerator._format_title_feishu(
                    title_data, show_source=True
                )
                text_content += f"  {j}. {formatted_title}\n"

                if j < len(stat["titles"]):
                    text_content += "\n"

            if i < len(report_data["stats"]) - 1:
                text_content += f"\n{CONFIG['FEISHU_MESSAGE_SEPARATOR']}\n\n"

        if not text_content:
            text_content = "ğŸ“­ æš‚æ— åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡\n\n"

        # æ¸²æŸ“æ–°å¢æ–°é—»éƒ¨åˆ†
        if report_data["new_titles"]:
            if text_content and "æš‚æ— åŒ¹é…" not in text_content:
                text_content += f"\n{CONFIG['FEISHU_MESSAGE_SEPARATOR']}\n\n"

            text_content += (
                f"ğŸ†• **æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—»** (å…± {report_data['total_new_count']} æ¡)\n\n"
            )

            for source_data in report_data["new_titles"]:
                text_content += f"**{source_data['source_alias']}** ({len(source_data['titles'])} æ¡):\n"

                for j, title_data in enumerate(source_data["titles"], 1):
                    title_data_copy = title_data.copy()
                    title_data_copy["is_new"] = False
                    formatted_title = ReportGenerator._format_title_feishu(
                        title_data_copy, show_source=False
                    )
                    text_content += f"  {j}. {formatted_title}\n"

                text_content += "\n"

        # æ¸²æŸ“å¤±è´¥å¹³å°
        if report_data["failed_ids"]:
            if text_content and "æš‚æ— åŒ¹é…" not in text_content:
                text_content += f"\n{CONFIG['FEISHU_MESSAGE_SEPARATOR']}\n\n"

            text_content += "âš ï¸ **æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š**\n\n"
            for i, id_value in enumerate(report_data["failed_ids"], 1):
                text_content += f"  â€¢ <font color='red'>{id_value}</font>\n"

        # æ·»åŠ æ—¶é—´æˆ³
        now = TimeHelper.get_beijing_time()
        text_content += f"\n\n<font color='grey'>æ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}</font>"

        # ç‰ˆæœ¬æ›´æ–°æç¤º
        if update_info:
            text_content += f"\n<font color='grey'>TrendRadar å‘ç°æ–°ç‰ˆæœ¬ {update_info['remote_version']}ï¼Œå½“å‰ {update_info['current_version']}</font>"

        return text_content

    @staticmethod
    def _render_dingtalk_content(
        report_data: Dict, update_info: Optional[Dict] = None
    ) -> str:
        """æ¸²æŸ“é’‰é’‰å†…å®¹"""
        text_content = ""

        # è®¡ç®—æ€»æ ‡é¢˜æ•°
        total_titles = sum(
            len(stat["titles"]) for stat in report_data["stats"] if stat["count"] > 0
        )
        now = TimeHelper.get_beijing_time()

        # é¡¶éƒ¨ç»Ÿè®¡ä¿¡æ¯
        text_content += f"**æ€»æ–°é—»æ•°ï¼š** {total_titles}\n\n"
        text_content += f"**æ—¶é—´ï¼š** {now.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        text_content += f"**ç±»å‹ï¼š** çƒ­ç‚¹åˆ†ææŠ¥å‘Š\n\n"

        text_content += "---\n\n"

        # æ¸²æŸ“çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡
        if report_data["stats"]:
            text_content += "ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**\n\n"

            total_count = len(report_data["stats"])

            for i, stat in enumerate(report_data["stats"]):
                word = stat["word"]
                count = stat["count"]

                sequence_display = f"[{i + 1}/{total_count}]"

                if count >= 10:
                    text_content += (
                        f"ğŸ”¥ {sequence_display} **{word}** : **{count}** æ¡\n\n"
                    )
                elif count >= 5:
                    text_content += (
                        f"ğŸ“ˆ {sequence_display} **{word}** : **{count}** æ¡\n\n"
                    )
                else:
                    text_content += f"ğŸ“Œ {sequence_display} **{word}** : {count} æ¡\n\n"

                for j, title_data in enumerate(stat["titles"], 1):
                    formatted_title = ReportGenerator._format_title_dingtalk(
                        title_data, show_source=True
                    )
                    text_content += f"  {j}. {formatted_title}\n"

                    if j < len(stat["titles"]):
                        text_content += "\n"

                if i < len(report_data["stats"]) - 1:
                    text_content += f"\n---\n\n"

        if not report_data["stats"]:
            text_content += "ğŸ“­ æš‚æ— åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡\n\n"

        # æ¸²æŸ“æ–°å¢æ–°é—»éƒ¨åˆ†
        if report_data["new_titles"]:
            if text_content and "æš‚æ— åŒ¹é…" not in text_content:
                text_content += f"\n---\n\n"

            text_content += (
                f"ğŸ†• **æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—»** (å…± {report_data['total_new_count']} æ¡)\n\n"
            )

            for source_data in report_data["new_titles"]:
                text_content += f"**{source_data['source_alias']}** ({len(source_data['titles'])} æ¡):\n\n"

                for j, title_data in enumerate(source_data["titles"], 1):
                    title_data_copy = title_data.copy()
                    title_data_copy["is_new"] = False
                    formatted_title = ReportGenerator._format_title_dingtalk(
                        title_data_copy, show_source=False
                    )
                    text_content += f"  {j}. {formatted_title}\n"

                text_content += "\n"

        # æ¸²æŸ“å¤±è´¥å¹³å°
        if report_data["failed_ids"]:
            if text_content and "æš‚æ— åŒ¹é…" not in text_content:
                text_content += f"\n---\n\n"

            text_content += "âš ï¸ **æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š**\n\n"
            for i, id_value in enumerate(report_data["failed_ids"], 1):
                text_content += f"  â€¢ **{id_value}**\n"

        # æ·»åŠ æ—¶é—´æˆ³
        text_content += f"\n\n> æ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}"

        # ç‰ˆæœ¬æ›´æ–°æç¤º
        if update_info:
            text_content += f"\n> TrendRadar å‘ç°æ–°ç‰ˆæœ¬ **{update_info['remote_version']}**ï¼Œå½“å‰ **{update_info['current_version']}**"

        return text_content

    @staticmethod
    def _split_content_into_batches(
        report_data: Dict,
        format_type: str,
        update_info: Optional[Dict] = None,
        max_bytes: int = CONFIG["MESSAGE_BATCH_SIZE"],
    ) -> List[str]:
        """åˆ†æ‰¹å¤„ç†æ¶ˆæ¯å†…å®¹ï¼Œç¡®ä¿è¯ç»„æ ‡é¢˜+è‡³å°‘ç¬¬ä¸€æ¡æ–°é—»çš„å®Œæ•´æ€§"""
        batches = []

        # åŸºç¡€ä¿¡æ¯æ„å»º
        total_titles = sum(
            len(stat["titles"]) for stat in report_data["stats"] if stat["count"] > 0
        )
        now = TimeHelper.get_beijing_time()

        base_header = ""
        if format_type == "wework":
            base_header = f"**æ€»æ–°é—»æ•°ï¼š** {total_titles}\n\n\n\n"
        elif format_type == "telegram":
            base_header = f"æ€»æ–°é—»æ•°ï¼š {total_titles}\n\n"

        base_footer = ""
        if format_type == "wework":
            base_footer = f"\n\n\n> æ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}"
            if update_info:
                base_footer += f"\n> TrendRadar å‘ç°æ–°ç‰ˆæœ¬ **{update_info['remote_version']}**ï¼Œå½“å‰ **{update_info['current_version']}**"
        elif format_type == "telegram":
            base_footer = f"\n\næ›´æ–°æ—¶é—´ï¼š{now.strftime('%Y-%m-%d %H:%M:%S')}"
            if update_info:
                base_footer += f"\nTrendRadar å‘ç°æ–°ç‰ˆæœ¬ {update_info['remote_version']}ï¼Œå½“å‰ {update_info['current_version']}"

        stats_header = ""
        if report_data["stats"]:
            if format_type == "wework":
                stats_header = "ğŸ“Š **çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡**\n\n"
            elif format_type == "telegram":
                stats_header = "ğŸ“Š çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡\n\n"

        current_batch = base_header
        current_batch_has_content = False

        # ç©ºå†…å®¹å¤„ç†
        if (
            not report_data["stats"]
            and not report_data["new_titles"]
            and not report_data["failed_ids"]
        ):
            simple_content = "ğŸ“­ æš‚æ— åŒ¹é…çš„çƒ­ç‚¹è¯æ±‡\n\n"
            final_content = base_header + simple_content + base_footer
            batches.append(final_content)
            return batches

        # å¤„ç†çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡
        if report_data["stats"]:
            total_count = len(report_data["stats"])

            # æ·»åŠ ç»Ÿè®¡æ ‡é¢˜
            test_content = current_batch + stats_header
            if (
                len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                < max_bytes
            ):
                current_batch = test_content
                current_batch_has_content = True
            else:
                if current_batch_has_content:
                    batches.append(current_batch + base_footer)
                current_batch = base_header + stats_header
                current_batch_has_content = True

            # é€ä¸ªå¤„ç†è¯ç»„ï¼ˆç¡®ä¿è¯ç»„æ ‡é¢˜+ç¬¬ä¸€æ¡æ–°é—»çš„åŸå­æ€§ï¼‰
            for i, stat in enumerate(report_data["stats"]):
                word = stat["word"]
                count = stat["count"]
                sequence_display = f"[{i + 1}/{total_count}]"

                # æ„å»ºè¯ç»„æ ‡é¢˜
                word_header = ""
                if format_type == "wework":
                    if count >= 10:
                        word_header = (
                            f"ğŸ”¥ {sequence_display} **{word}** : **{count}** æ¡\n\n"
                        )
                    elif count >= 5:
                        word_header = (
                            f"ğŸ“ˆ {sequence_display} **{word}** : **{count}** æ¡\n\n"
                        )
                    else:
                        word_header = (
                            f"ğŸ“Œ {sequence_display} **{word}** : {count} æ¡\n\n"
                        )
                elif format_type == "telegram":
                    if count >= 10:
                        word_header = f"ğŸ”¥ {sequence_display} {word} : {count} æ¡\n\n"
                    elif count >= 5:
                        word_header = f"ğŸ“ˆ {sequence_display} {word} : {count} æ¡\n\n"
                    else:
                        word_header = f"ğŸ“Œ {sequence_display} {word} : {count} æ¡\n\n"

                # æ„å»ºç¬¬ä¸€æ¡æ–°é—»
                first_news_line = ""
                if stat["titles"]:
                    first_title_data = stat["titles"][0]
                    if format_type == "wework":
                        formatted_title = ReportGenerator._format_title_wework(
                            first_title_data, show_source=True
                        )
                    elif format_type == "telegram":
                        formatted_title = ReportGenerator._format_title_telegram(
                            first_title_data, show_source=True
                        )
                    else:
                        formatted_title = f"{first_title_data['title']}"

                    first_news_line = f"  1. {formatted_title}\n"
                    if len(stat["titles"]) > 1:
                        first_news_line += "\n"

                # åŸå­æ€§æ£€æŸ¥ï¼šè¯ç»„æ ‡é¢˜+ç¬¬ä¸€æ¡æ–°é—»å¿…é¡»ä¸€èµ·å¤„ç†
                word_with_first_news = word_header + first_news_line
                test_content = current_batch + word_with_first_news

                if (
                    len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                    >= max_bytes
                ):
                    # å½“å‰æ‰¹æ¬¡å®¹çº³ä¸ä¸‹ï¼Œå¼€å¯æ–°æ‰¹æ¬¡
                    if current_batch_has_content:
                        batches.append(current_batch + base_footer)
                    current_batch = base_header + stats_header + word_with_first_news
                    current_batch_has_content = True
                    start_index = 1
                else:
                    current_batch = test_content
                    current_batch_has_content = True
                    start_index = 1

                # å¤„ç†å‰©ä½™æ–°é—»æ¡ç›®
                for j in range(start_index, len(stat["titles"])):
                    title_data = stat["titles"][j]
                    if format_type == "wework":
                        formatted_title = ReportGenerator._format_title_wework(
                            title_data, show_source=True
                        )
                    elif format_type == "telegram":
                        formatted_title = ReportGenerator._format_title_telegram(
                            title_data, show_source=True
                        )
                    else:
                        formatted_title = f"{title_data['title']}"

                    news_line = f"  {j + 1}. {formatted_title}\n"
                    if j < len(stat["titles"]) - 1:
                        news_line += "\n"

                    test_content = current_batch + news_line
                    if (
                        len(test_content.encode("utf-8"))
                        + len(base_footer.encode("utf-8"))
                        >= max_bytes
                    ):
                        if current_batch_has_content:
                            batches.append(current_batch + base_footer)
                        current_batch = (
                            base_header + stats_header + word_header + news_line
                        )
                        current_batch_has_content = True
                    else:
                        current_batch = test_content
                        current_batch_has_content = True

                # è¯ç»„é—´åˆ†éš”ç¬¦
                if i < len(report_data["stats"]) - 1:
                    separator = ""
                    if format_type == "wework":
                        separator = f"\n\n\n\n"
                    elif format_type == "telegram":
                        separator = f"\n\n"

                    test_content = current_batch + separator
                    if (
                        len(test_content.encode("utf-8"))
                        + len(base_footer.encode("utf-8"))
                        < max_bytes
                    ):
                        current_batch = test_content

        # å¤„ç†æ–°å¢æ–°é—»ï¼ˆåŒæ ·ç¡®ä¿æ¥æºæ ‡é¢˜+ç¬¬ä¸€æ¡æ–°é—»çš„åŸå­æ€§ï¼‰
        if report_data["new_titles"]:
            new_header = ""
            if format_type == "wework":
                new_header = f"\n\n\n\nğŸ†• **æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—»** (å…± {report_data['total_new_count']} æ¡)\n\n"
            elif format_type == "telegram":
                new_header = f"\n\nğŸ†• æœ¬æ¬¡æ–°å¢çƒ­ç‚¹æ–°é—» (å…± {report_data['total_new_count']} æ¡)\n\n"

            test_content = current_batch + new_header
            if (
                len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                >= max_bytes
            ):
                if current_batch_has_content:
                    batches.append(current_batch + base_footer)
                current_batch = base_header + new_header
                current_batch_has_content = True
            else:
                current_batch = test_content
                current_batch_has_content = True

            # é€ä¸ªå¤„ç†æ–°å¢æ–°é—»æ¥æº
            for source_data in report_data["new_titles"]:
                source_header = ""
                if format_type == "wework":
                    source_header = f"**{source_data['source_alias']}** ({len(source_data['titles'])} æ¡):\n\n"
                elif format_type == "telegram":
                    source_header = f"{source_data['source_alias']} ({len(source_data['titles'])} æ¡):\n\n"

                # æ„å»ºç¬¬ä¸€æ¡æ–°å¢æ–°é—»
                first_news_line = ""
                if source_data["titles"]:
                    first_title_data = source_data["titles"][0]
                    title_data_copy = first_title_data.copy()
                    title_data_copy["is_new"] = False

                    if format_type == "wework":
                        formatted_title = ReportGenerator._format_title_wework(
                            title_data_copy, show_source=False
                        )
                    elif format_type == "telegram":
                        formatted_title = ReportGenerator._format_title_telegram(
                            title_data_copy, show_source=False
                        )
                    else:
                        formatted_title = f"{title_data_copy['title']}"

                    first_news_line = f"  1. {formatted_title}\n"

                # åŸå­æ€§æ£€æŸ¥ï¼šæ¥æºæ ‡é¢˜+ç¬¬ä¸€æ¡æ–°é—»
                source_with_first_news = source_header + first_news_line
                test_content = current_batch + source_with_first_news

                if (
                    len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                    >= max_bytes
                ):
                    if current_batch_has_content:
                        batches.append(current_batch + base_footer)
                    current_batch = base_header + new_header + source_with_first_news
                    current_batch_has_content = True
                    start_index = 1
                else:
                    current_batch = test_content
                    current_batch_has_content = True
                    start_index = 1

                # å¤„ç†å‰©ä½™æ–°å¢æ–°é—»
                for j in range(start_index, len(source_data["titles"])):
                    title_data = source_data["titles"][j]
                    title_data_copy = title_data.copy()
                    title_data_copy["is_new"] = False

                    if format_type == "wework":
                        formatted_title = ReportGenerator._format_title_wework(
                            title_data_copy, show_source=False
                        )
                    elif format_type == "telegram":
                        formatted_title = ReportGenerator._format_title_telegram(
                            title_data_copy, show_source=False
                        )
                    else:
                        formatted_title = f"{title_data_copy['title']}"

                    news_line = f"  {j + 1}. {formatted_title}\n"

                    test_content = current_batch + news_line
                    if (
                        len(test_content.encode("utf-8"))
                        + len(base_footer.encode("utf-8"))
                        >= max_bytes
                    ):
                        if current_batch_has_content:
                            batches.append(current_batch + base_footer)
                        current_batch = (
                            base_header + new_header + source_header + news_line
                        )
                        current_batch_has_content = True
                    else:
                        current_batch = test_content
                        current_batch_has_content = True

                current_batch += "\n"

        # å¤„ç†å¤±è´¥å¹³å°
        if report_data["failed_ids"]:
            failed_header = ""
            if format_type == "wework":
                failed_header = f"\n\n\n\nâš ï¸ **æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š**\n\n"
            elif format_type == "telegram":
                failed_header = f"\n\nâš ï¸ æ•°æ®è·å–å¤±è´¥çš„å¹³å°ï¼š\n\n"

            test_content = current_batch + failed_header
            if (
                len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                >= max_bytes
            ):
                if current_batch_has_content:
                    batches.append(current_batch + base_footer)
                current_batch = base_header + failed_header
                current_batch_has_content = True
            else:
                current_batch = test_content
                current_batch_has_content = True

            for i, id_value in enumerate(report_data["failed_ids"], 1):
                failed_line = f"  â€¢ {id_value}\n"
                test_content = current_batch + failed_line
                if (
                    len(test_content.encode("utf-8")) + len(base_footer.encode("utf-8"))
                    >= max_bytes
                ):
                    if current_batch_has_content:
                        batches.append(current_batch + base_footer)
                    current_batch = base_header + failed_header + failed_line
                    current_batch_has_content = True
                else:
                    current_batch = test_content
                    current_batch_has_content = True

        # å®Œæˆæœ€åæ‰¹æ¬¡
        if current_batch_has_content:
            batches.append(current_batch + base_footer)

        return batches

    @staticmethod
    def send_to_webhooks(
        stats: List[Dict],
        failed_ids: Optional[List] = None,
        report_type: str = "å•æ¬¡çˆ¬å–",
        new_titles: Optional[Dict] = None,
        id_to_alias: Optional[Dict] = None,
        update_info: Optional[Dict] = None,
        proxy_url: Optional[str] = None,
    ) -> Dict[str, bool]:
        """å‘é€æ•°æ®åˆ°å¤šä¸ªwebhookå¹³å°"""
        results = {}

        # æ•°æ®å¤„ç†å±‚
        report_data = ReportGenerator._prepare_report_data(
            stats, failed_ids, new_titles, id_to_alias
        )

        # è·å–ç¯å¢ƒå˜é‡ä¸­çš„webhooké…ç½®
        feishu_url = os.environ.get("FEISHU_WEBHOOK_URL", CONFIG["FEISHU_WEBHOOK_URL"])
        dingtalk_url = os.environ.get(
            "DINGTALK_WEBHOOK_URL", CONFIG["DINGTALK_WEBHOOK_URL"]
        )
        wework_url = os.environ.get("WEWORK_WEBHOOK_URL", CONFIG["WEWORK_WEBHOOK_URL"])
        telegram_token = os.environ.get(
            "TELEGRAM_BOT_TOKEN", CONFIG["TELEGRAM_BOT_TOKEN"]
        )
        telegram_chat_id = os.environ.get(
            "TELEGRAM_CHAT_ID", CONFIG["TELEGRAM_CHAT_ID"]
        )

        update_info_to_send = update_info if CONFIG["SHOW_VERSION_UPDATE"] else None

        # å‘é€åˆ°é£ä¹¦
        if feishu_url:
            results["feishu"] = ReportGenerator._send_to_feishu(
                feishu_url, report_data, report_type, update_info_to_send, proxy_url
            )

        # å‘é€åˆ°é’‰é’‰
        if dingtalk_url:
            results["dingtalk"] = ReportGenerator._send_to_dingtalk(
                dingtalk_url, report_data, report_type, update_info_to_send, proxy_url
            )

        # å‘é€åˆ°ä¼ä¸šå¾®ä¿¡
        if wework_url:
            results["wework"] = ReportGenerator._send_to_wework(
                wework_url, report_data, report_type, update_info_to_send, proxy_url
            )

        # å‘é€åˆ°Telegram
        if telegram_token and telegram_chat_id:
            results["telegram"] = ReportGenerator._send_to_telegram(
                telegram_token,
                telegram_chat_id,
                report_data,
                report_type,
                update_info_to_send,
                proxy_url,
            )

        if not results:
            print("æœªé…ç½®ä»»ä½•webhook URLï¼Œè·³è¿‡é€šçŸ¥å‘é€")

        return results

    @staticmethod
    def _send_to_feishu(
        webhook_url: str,
        report_data: Dict,
        report_type: str,
        update_info: Optional[Dict] = None,
        proxy_url: Optional[str] = None,
    ) -> bool:
        """å‘é€åˆ°é£ä¹¦"""
        headers = {"Content-Type": "application/json"}

        text_content = ReportGenerator._render_feishu_content(report_data, update_info)
        total_titles = sum(
            len(stat["titles"]) for stat in report_data["stats"] if stat["count"] > 0
        )

        now = TimeHelper.get_beijing_time()
        payload = {
            "msg_type": "text",
            "content": {
                "total_titles": total_titles,
                "timestamp": now.strftime("%Y-%m-%d %H:%M:%S"),
                "report_type": report_type,
                "text": text_content,
            },
        }

        proxies = None
        if proxy_url:
            proxies = {"http": proxy_url, "https": proxy_url}

        try:
            response = requests.post(
                webhook_url, headers=headers, json=payload, proxies=proxies, timeout=30
            )
            if response.status_code == 200:
                print(f"é£ä¹¦é€šçŸ¥å‘é€æˆåŠŸ [{report_type}]")
                return True
            else:
                print(
                    f"é£ä¹¦é€šçŸ¥å‘é€å¤±è´¥ [{report_type}]ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}"
                )
                return False
        except Exception as e:
            print(f"é£ä¹¦é€šçŸ¥å‘é€å‡ºé”™ [{report_type}]ï¼š{e}")
            return False

    @staticmethod
    def _send_to_dingtalk(
        webhook_url: str,
        report_data: Dict,
        report_type: str,
        update_info: Optional[Dict] = None,
        proxy_url: Optional[str] = None,
    ) -> bool:
        """å‘é€åˆ°é’‰é’‰"""
        headers = {"Content-Type": "application/json"}

        text_content = ReportGenerator._render_dingtalk_content(
            report_data, update_info
        )

        payload = {
            "msgtype": "markdown",
            "markdown": {
                "title": f"TrendRadar çƒ­ç‚¹åˆ†ææŠ¥å‘Š - {report_type}",
                "text": text_content,
            },
        }

        proxies = None
        if proxy_url:
            proxies = {"http": proxy_url, "https": proxy_url}

        try:
            response = requests.post(
                webhook_url, headers=headers, json=payload, proxies=proxies, timeout=30
            )
            if response.status_code == 200:
                result = response.json()
                if result.get("errcode") == 0:
                    print(f"é’‰é’‰é€šçŸ¥å‘é€æˆåŠŸ [{report_type}]")
                    return True
                else:
                    print(
                        f"é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥ [{report_type}]ï¼Œé”™è¯¯ï¼š{result.get('errmsg')}"
                    )
                    return False
            else:
                print(
                    f"é’‰é’‰é€šçŸ¥å‘é€å¤±è´¥ [{report_type}]ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}"
                )
                return False
        except Exception as e:
            print(f"é’‰é’‰é€šçŸ¥å‘é€å‡ºé”™ [{report_type}]ï¼š{e}")
            return False

    @staticmethod
    def _send_to_wework(
        webhook_url: str,
        report_data: Dict,
        report_type: str,
        update_info: Optional[Dict] = None,
        proxy_url: Optional[str] = None,
    ) -> bool:
        """å‘é€åˆ°ä¼ä¸šå¾®ä¿¡ï¼ˆæ”¯æŒåˆ†æ‰¹å‘é€ï¼‰"""
        headers = {"Content-Type": "application/json"}
        proxies = None
        if proxy_url:
            proxies = {"http": proxy_url, "https": proxy_url}

        # è·å–åˆ†æ‰¹å†…å®¹
        batches = ReportGenerator._split_content_into_batches(
            report_data, "wework", update_info
        )

        print(f"ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯åˆ†ä¸º {len(batches)} æ‰¹æ¬¡å‘é€ [{report_type}]")

        # é€æ‰¹å‘é€
        for i, batch_content in enumerate(batches, 1):
            batch_size = len(batch_content.encode("utf-8"))
            print(
                f"å‘é€ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡ï¼Œå¤§å°ï¼š{batch_size} å­—èŠ‚ [{report_type}]"
            )

            # æ·»åŠ æ‰¹æ¬¡æ ‡è¯†
            if len(batches) > 1:
                batch_header = f"**[ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡]**\n\n"
                batch_content = batch_header + batch_content

            payload = {"msgtype": "markdown", "markdown": {"content": batch_content}}

            try:
                response = requests.post(
                    webhook_url,
                    headers=headers,
                    json=payload,
                    proxies=proxies,
                    timeout=30,
                )
                if response.status_code == 200:
                    result = response.json()
                    if result.get("errcode") == 0:
                        print(
                            f"ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€æˆåŠŸ [{report_type}]"
                        )
                        # æ‰¹æ¬¡é—´é—´éš”
                        if i < len(batches):
                            time.sleep(CONFIG["BATCH_SEND_INTERVAL"])
                    else:
                        print(
                            f"ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼Œé”™è¯¯ï¼š{result.get('errmsg')}"
                        )
                        return False
                else:
                    print(
                        f"ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}"
                    )
                    return False
            except Exception as e:
                print(
                    f"ä¼ä¸šå¾®ä¿¡ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å‡ºé”™ [{report_type}]ï¼š{e}"
                )
                return False

        print(f"ä¼ä¸šå¾®ä¿¡æ‰€æœ‰ {len(batches)} æ‰¹æ¬¡å‘é€å®Œæˆ [{report_type}]")
        return True

    @staticmethod
    def _send_to_telegram(
        bot_token: str,
        chat_id: str,
        report_data: Dict,
        report_type: str,
        update_info: Optional[Dict] = None,
        proxy_url: Optional[str] = None,
    ) -> bool:
        """å‘é€åˆ°Telegramï¼ˆæ”¯æŒåˆ†æ‰¹å‘é€ï¼‰"""
        headers = {"Content-Type": "application/json"}
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"

        proxies = None
        if proxy_url:
            proxies = {"http": proxy_url, "https": proxy_url}

        # è·å–åˆ†æ‰¹å†…å®¹
        batches = ReportGenerator._split_content_into_batches(
            report_data, "telegram", update_info
        )

        print(f"Telegramæ¶ˆæ¯åˆ†ä¸º {len(batches)} æ‰¹æ¬¡å‘é€ [{report_type}]")

        # é€æ‰¹å‘é€
        for i, batch_content in enumerate(batches, 1):
            batch_size = len(batch_content.encode("utf-8"))
            print(
                f"å‘é€Telegramç¬¬ {i}/{len(batches)} æ‰¹æ¬¡ï¼Œå¤§å°ï¼š{batch_size} å­—èŠ‚ [{report_type}]"
            )

            # æ·»åŠ æ‰¹æ¬¡æ ‡è¯†
            if len(batches) > 1:
                batch_header = f"<b>[ç¬¬ {i}/{len(batches)} æ‰¹æ¬¡]</b>\n\n"
                batch_content = batch_header + batch_content

            payload = {
                "chat_id": chat_id,
                "text": batch_content,
                "parse_mode": "HTML",
                "disable_web_page_preview": True,
            }

            try:
                response = requests.post(
                    url, headers=headers, json=payload, proxies=proxies, timeout=30
                )
                if response.status_code == 200:
                    result = response.json()
                    if result.get("ok"):
                        print(
                            f"Telegramç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€æˆåŠŸ [{report_type}]"
                        )
                        # æ‰¹æ¬¡é—´é—´éš”
                        if i < len(batches):
                            time.sleep(CONFIG["BATCH_SEND_INTERVAL"])
                    else:
                        print(
                            f"Telegramç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼Œé”™è¯¯ï¼š{result.get('description')}"
                        )
                        return False
                else:
                    print(
                        f"Telegramç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å¤±è´¥ [{report_type}]ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}"
                    )
                    return False
            except Exception as e:
                print(
                    f"Telegramç¬¬ {i}/{len(batches)} æ‰¹æ¬¡å‘é€å‡ºé”™ [{report_type}]ï¼š{e}"
                )
                return False

        print(f"Telegramæ‰€æœ‰ {len(batches)} æ‰¹æ¬¡å‘é€å®Œæˆ [{report_type}]")
        return True


class NewsAnalyzer:
    """æ–°é—»åˆ†æå™¨"""

    def __init__(
        self,
        request_interval: int = CONFIG["REQUEST_INTERVAL"],
        report_type: str = CONFIG["REPORT_TYPE"],
        rank_threshold: int = CONFIG["RANK_THRESHOLD"],
    ):
        self.request_interval = request_interval
        self.report_type = report_type
        self.rank_threshold = rank_threshold
        self.is_github_actions = os.environ.get("GITHUB_ACTIONS") == "true"
        self.update_info = None
        self.proxy_url = None
        if not self.is_github_actions and CONFIG["USE_PROXY"]:
            self.proxy_url = CONFIG["DEFAULT_PROXY"]
            print("æœ¬åœ°ç¯å¢ƒï¼Œä½¿ç”¨ä»£ç†")
        elif not self.is_github_actions and not CONFIG["USE_PROXY"]:
            print("æœ¬åœ°ç¯å¢ƒï¼Œæœªå¯ç”¨ä»£ç†")
        else:
            print("GitHub Actionsç¯å¢ƒï¼Œä¸ä½¿ç”¨ä»£ç†")

        self.data_fetcher = DataFetcher(self.proxy_url)

        if self.is_github_actions:
            self._check_version_update()

    def _check_version_update(self) -> None:
        """æ£€æŸ¥ç‰ˆæœ¬æ›´æ–°"""
        try:
            need_update, remote_version = VersionChecker.check_for_updates(
                CONFIG["VERSION"], CONFIG["VERSION_CHECK_URL"], self.proxy_url
            )

            if need_update and remote_version:
                self.update_info = {
                    "current_version": CONFIG["VERSION"],
                    "remote_version": remote_version,
                }
                print(f"å‘ç°æ–°ç‰ˆæœ¬: {remote_version} (å½“å‰: {CONFIG['VERSION']})")
            else:
                print("ç‰ˆæœ¬æ£€æŸ¥å®Œæˆï¼Œå½“å‰ä¸ºæœ€æ–°ç‰ˆæœ¬")
        except Exception as e:
            print(f"ç‰ˆæœ¬æ£€æŸ¥å‡ºé”™: {e}")

    def generate_daily_summary(self) -> Optional[str]:
        """ç”Ÿæˆå½“æ—¥ç»Ÿè®¡æŠ¥å‘Š"""
        print("ç”Ÿæˆå½“æ—¥ç»Ÿè®¡æŠ¥å‘Š...")

        all_results, id_to_alias, title_info = DataProcessor.read_all_today_titles()

        if not all_results:
            print("æ²¡æœ‰æ‰¾åˆ°å½“å¤©çš„æ•°æ®")
            return None

        total_titles = sum(len(titles) for titles in all_results.values())
        print(f"è¯»å–åˆ° {total_titles} ä¸ªæ ‡é¢˜")

        latest_new_titles = DataProcessor.detect_latest_new_titles(id_to_alias)
        if latest_new_titles:
            total_new_count = sum(len(titles) for titles in latest_new_titles.values())
            print(f"æ£€æµ‹åˆ° {total_new_count} æ¡æœ€æ–°æ–°å¢æ–°é—»")

        word_groups, filter_words = DataProcessor.load_frequency_words()

        stats, total_titles = StatisticsCalculator.count_word_frequency(
            all_results,
            word_groups,
            filter_words,
            id_to_alias,
            title_info,
            self.rank_threshold,
            latest_new_titles,
        )

        html_file = ReportGenerator.generate_html_report(
            stats,
            total_titles,
            is_daily=True,
            new_titles=latest_new_titles,
            id_to_alias=id_to_alias,
        )
        print(f"å½“æ—¥HTMLç»Ÿè®¡æŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")

        # æ£€æŸ¥é€šçŸ¥é…ç½®
        has_webhook = any(
            [
                os.environ.get("FEISHU_WEBHOOK_URL", CONFIG["FEISHU_WEBHOOK_URL"]),
                os.environ.get("DINGTALK_WEBHOOK_URL", CONFIG["DINGTALK_WEBHOOK_URL"]),
                os.environ.get("WEWORK_WEBHOOK_URL", CONFIG["WEWORK_WEBHOOK_URL"]),
                (
                    os.environ.get("TELEGRAM_BOT_TOKEN", CONFIG["TELEGRAM_BOT_TOKEN"])
                    and os.environ.get("TELEGRAM_CHAT_ID", CONFIG["TELEGRAM_CHAT_ID"])
                ),
            ]
        )

        if (
            CONFIG["ENABLE_NOTIFICATION"]
            and has_webhook
            and self.report_type in ["daily", "both"]
        ):
            ReportGenerator.send_to_webhooks(
                stats,
                [],
                "å½“æ—¥æ±‡æ€»",
                latest_new_titles,
                id_to_alias,
                self.update_info,
                self.proxy_url,
            )
        elif CONFIG["ENABLE_NOTIFICATION"] and not has_webhook:
            print("âš ï¸ è­¦å‘Šï¼šé€šçŸ¥åŠŸèƒ½å·²å¯ç”¨ä½†æœªé…ç½®webhook URLï¼Œå°†è·³è¿‡é€šçŸ¥å‘é€")
        elif not CONFIG["ENABLE_NOTIFICATION"]:
            print("è·³è¿‡å½“æ—¥æ±‡æ€»é€šçŸ¥ï¼šé€šçŸ¥åŠŸèƒ½å·²ç¦ç”¨")

        return html_file

    def run(self) -> None:
        """æ‰§è¡Œåˆ†ææµç¨‹"""
        now = TimeHelper.get_beijing_time()
        print(f"å½“å‰åŒ—äº¬æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')}")

        if not CONFIG["ENABLE_CRAWLER"]:
            print("çˆ¬è™«åŠŸèƒ½å·²ç¦ç”¨ï¼ˆENABLE_CRAWLER=Falseï¼‰ï¼Œç¨‹åºé€€å‡º")
            return

        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†ä»»ä½•webhook URL
        has_webhook = any(
            [
                os.environ.get("FEISHU_WEBHOOK_URL", CONFIG["FEISHU_WEBHOOK_URL"]),
                os.environ.get("DINGTALK_WEBHOOK_URL", CONFIG["DINGTALK_WEBHOOK_URL"]),
                os.environ.get("WEWORK_WEBHOOK_URL", CONFIG["WEWORK_WEBHOOK_URL"]),
                (
                    os.environ.get("TELEGRAM_BOT_TOKEN", CONFIG["TELEGRAM_BOT_TOKEN"])
                    and os.environ.get("TELEGRAM_CHAT_ID", CONFIG["TELEGRAM_CHAT_ID"])
                ),
            ]
        )

        # é€šçŸ¥åŠŸèƒ½çŠ¶æ€æ£€æŸ¥å’Œæç¤º
        if not CONFIG["ENABLE_NOTIFICATION"]:
            print("é€šçŸ¥åŠŸèƒ½å·²ç¦ç”¨ï¼ˆENABLE_NOTIFICATION=Falseï¼‰ï¼Œå°†åªè¿›è¡Œæ•°æ®æŠ“å–")
        elif not has_webhook:
            print("æœªé…ç½®ä»»ä½•webhook URLï¼Œå°†åªè¿›è¡Œæ•°æ®æŠ“å–ï¼Œä¸å‘é€é€šçŸ¥")
        else:
            print("é€šçŸ¥åŠŸèƒ½å·²å¯ç”¨ï¼Œå°†å‘é€webhooké€šçŸ¥")

        print(f"æŠ¥å‘Šç±»å‹: {self.report_type}")

        ids = [
            ("toutiao", "ä»Šæ—¥å¤´æ¡"),
            ("baidu", "ç™¾åº¦çƒ­æœ"),
            ("wallstreetcn-hot", "åå°”è¡—è§é—»"),
            ("thepaper", "æ¾æ¹ƒæ–°é—»"),
            ("bilibili-hot-search", "bilibili çƒ­æœ"),
            ("cls-hot", "è´¢è”ç¤¾çƒ­é—¨"),
            ("ifeng", "å‡¤å‡°ç½‘"),
            "tieba",
            "weibo",
            "douyin",
            "zhihu",
        ]

        print(f"å¼€å§‹çˆ¬å–æ•°æ®ï¼Œè¯·æ±‚é—´éš” {self.request_interval} æ¯«ç§’")
        FileHelper.ensure_directory_exists("output")

        results, id_to_alias, failed_ids = self.data_fetcher.crawl_websites(
            ids, self.request_interval
        )

        title_file = DataProcessor.save_titles_to_file(results, id_to_alias, failed_ids)
        print(f"æ ‡é¢˜å·²ä¿å­˜åˆ°: {title_file}")

        new_titles = DataProcessor.detect_latest_new_titles(id_to_alias)

        # æ„å»ºæ ‡é¢˜ä¿¡æ¯
        time_info = Path(title_file).stem
        title_info = {}
        for source_id, titles_data in results.items():
            title_info[source_id] = {}
            for title, title_data in titles_data.items():
                ranks = title_data.get("ranks", [])
                url = title_data.get("url", "")
                mobile_url = title_data.get("mobileUrl", "")

                title_info[source_id][title] = {
                    "first_time": time_info,
                    "last_time": time_info,
                    "count": 1,
                    "ranks": ranks,
                    "url": url,
                    "mobileUrl": mobile_url,
                }

        word_groups, filter_words = DataProcessor.load_frequency_words()

        stats, total_titles = StatisticsCalculator.count_word_frequency(
            results,
            word_groups,
            filter_words,
            id_to_alias,
            title_info,
            self.rank_threshold,
            new_titles,
        )

        # åªæœ‰å¯ç”¨é€šçŸ¥ä¸”é…ç½®äº†webhookæ—¶æ‰å‘é€é€šçŸ¥
        if (
            CONFIG["ENABLE_NOTIFICATION"]
            and has_webhook
            and self.report_type in ["current", "both"]
        ):
            ReportGenerator.send_to_webhooks(
                stats,
                failed_ids,
                "å•æ¬¡çˆ¬å–",
                new_titles,
                id_to_alias,
                self.update_info,
                self.proxy_url,
            )
        elif CONFIG["ENABLE_NOTIFICATION"] and not has_webhook:
            print("âš ï¸ è­¦å‘Šï¼šé€šçŸ¥åŠŸèƒ½å·²å¯ç”¨ä½†æœªé…ç½®webhook URLï¼Œå°†è·³è¿‡é€šçŸ¥å‘é€")
        elif not CONFIG["ENABLE_NOTIFICATION"]:
            print("è·³è¿‡å•æ¬¡çˆ¬å–é€šçŸ¥ï¼šé€šçŸ¥åŠŸèƒ½å·²ç¦ç”¨")

        html_file = ReportGenerator.generate_html_report(
            stats, total_titles, failed_ids, False, new_titles, id_to_alias
        )
        print(f"HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}")

        daily_html = self.generate_daily_summary()

        if not self.is_github_actions and html_file:
            file_url = "file://" + str(Path(html_file).resolve())
            print(f"æ­£åœ¨æ‰“å¼€HTMLæŠ¥å‘Š: {file_url}")
            webbrowser.open(file_url)

            if daily_html:
                daily_url = "file://" + str(Path(daily_html).resolve())
                print(f"æ­£åœ¨æ‰“å¼€å½“æ—¥ç»Ÿè®¡æŠ¥å‘Š: {daily_url}")
                webbrowser.open(daily_url)


def main():
    analyzer = NewsAnalyzer(
        request_interval=CONFIG["REQUEST_INTERVAL"],
        report_type=CONFIG["REPORT_TYPE"],
        rank_threshold=CONFIG["RANK_THRESHOLD"],
    )
    analyzer.run()


if __name__ == "__main__":
    main()
